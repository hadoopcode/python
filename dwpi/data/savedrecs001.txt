FN Clarivate Analytics Web of Science
VR 1.0
PT P
PN CN108122234-A
TI Method for training convolutional neural network, involves adjusting network parameters of convolutional neural network according to first difference and second difference.
AU LUO F
   DAN J
AE BEIJING SENSETIME TECHNOLOGY DEV CO LTD (BEIJ-Non-standard)
GA 201847086B
AB    NOVELTY - The method involves detecting (S100) a raw sample image containing target object label information and a scrambled sample image corresponding to the original sample image based on a convolutional neural network to obtain the original image for the original image. A first prediction information of the target object in the sample image and second prediction information for the target object in the scrambled sample image. A first difference between the first prediction information and the annotation information, and a second difference between the first prediction information and the second prediction information are determined (S200). The network parameters of the convolutional neural network is adjusted (S300) according to the first difference and the second difference.
   USE - Method for training convolutional neural network.
   ADVANTAGE - The method effectively reduces the inter-frame jitter of the video frame images in the detection process.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a video processing method;
   (2) a convolution neural network training device;
   (3) a video processing device; and
   (4) an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the steps of a convolutional neural network training method. (Drawing includes non-English language text)
   Step for detecting raw sample image (S100)
   Step for determining first difference and second difference (S200)
   Step for adjusting network parameters of convolutional neural network (S300)
DC T01 (Digital Computers); W03 (TV and Broadcast Radio Receivers)
MC T01-J04B2; T01-J10B2; T01-J10B3A; T01-N01B3; W03-A11D; W03-A16C5A
IP G06T-007/11; G06T-007/194; H04N-021/44
PD CN108122234-A   05 Jun 2018   G06T-007/11   201842   Pages: 22   Chinese
AD CN108122234-A    CN11073607    29 Nov 2016
PI CN11073607    29 Nov 2016
UT DIIDW:201847086B
ER

PT P
PN CN108108858-A; WO2019141040-A1
TI Short-term power load prediction method of power system, involves predicting actual power consumption of measured power load and obtaining predicted power consumption of measured power load according to determined network prediction model.
AU WANG R
   ZHANG X
   HE M
   WANG J
   WU G
   RONG H
   XIONG Y
AE UNIV FOSHAN (UYFS-C)
   FOSHAN YOUYIJIA TECHNOLOGY CO LTD (FOSH-Non-standard)
   UNIV FOSHAN (UYFS-C)
   FOSHAN YOUYIJIA TECHNOLOGY CO LTD (FOSH-Non-standard)
GA 201845409F
AB    NOVELTY - The method involves establishing a deep belief network prediction model of a four-layer network structure. The actual power consumption of the measured power load is combined to train the deep belief network prediction model using an unsupervised greedy algorithm. The parameter values of each layer of the deep belief network prediction model are obtained and the activation function of the deep belief network prediction model is set. The mapping relationship between the input and output of the deep belief network prediction model is obtained through training and learning. The actual power consumption of the measured power load is predicted and the predicted power consumption of the measured power load is obtained according to the determined depth belief network prediction model.
   USE - Short-term power load prediction method of power system.
   ADVANTAGE - The depth belief network prediction network is introduced into the power load power prediction, and the internal relationship between the input and the output is learned to realize a prediction of the load power consumption for the period of time through the deep structure of the network. The short-term power load prediction process largely predicts the accuracy and speed of prediction and is created to predict the short-term power load conditions.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the short-term power load prediction process. (Drawing includes non-English language text)
DC T01 (Digital Computers)
MC T01-N01A2; T01-N01B3
IP G06N-003/04; G06N-003/08; G06Q-010/04; G06Q-050/06
PD CN108108858-A   01 Jun 2018   G06Q-010/04   201841   Pages: 7   Chinese
   WO2019141040-A1   25 Jul 2019   G06Q-010/04   201956      Chinese
AD CN108108858-A    CN10059484    22 Jan 2018
   WO2019141040-A1    WOCN122401    20 Dec 2018
PI CN10059484    22 Jan 2018
DS WO2019141040-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DJ; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IR; IS; JO; JP; KE; KG; KH; KN; KP; KR; KW; KZ; LA; LC; LK; LR; LS; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SA; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
      (Regional): BW; GH; GM; KE; LR; LS; MW; MZ; NA; RW; SD; SL; ST; SZ; TZ; UG; ZM; ZW; EA; AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; OA
UT DIIDW:201845409F
ER

PT P
PN CN108134979-A
TI Small base station switch control method based on deep neural network, involves calculating real value of predicted coordinate in sample error by using actual coordinate and predicted coordinate.
AU PAN Z
   DU P
   YOU X
   LIU N
AE UNIV SOUTHEAST (UYSE-C)
GA 201846315R
AB    NOVELTY - The method involves sampling user information collected in a base station. The user number, access time, and user location information of accessing the base station are recorded. The user data collected by the base station is collated and merged into path data that can be trained by the model, and data samples of all users are combined to obtain a sample set that is ultimately used for training. A neural network model is constructed to select a fully connected neural network as the training model. The average square error is used to calculate the training error. The commonly used forward propagation method is used to obtain a training result during training. The reverse propagation method is used to update the parameters in the neural network. The real value of the predicted coordinate in the sample error is calculated by using the actual coordinate and the predicted coordinate. The data to be predicted is collected and the position of the user is predicted at the next time.
   USE - Small base station switch control method based on deep neural network.
   ADVANTAGE - The accuracy of prediction and the practicality of system can be improved.
DC T01 (Digital Computers); W01 (Telephone and Data Transmission Systems); W02 (Broadcasting, Radio and Line Transmission Systems)
MC T01-E01C; T01-J04A; T01-N01B3; W01-A06C4; W01-A06D; W01-A06E; W02-C03C1B
IP G06N-003/08; H04W-004/02; H04W-004/029; H04W-052/02
PD CN108134979-A   08 Jun 2018   H04W-004/02   201842   Pages: 7   Chinese
AD CN108134979-A    CN11261843    04 Dec 2017
PI CN11261843    04 Dec 2017
UT DIIDW:201846315R
ER

PT P
PN CN108095683-A
TI Method for processing eye-ground image based on deep learning, involves determining test input eye-ground image of eye-ground pathological grade identification model to obtain lesion level recognition result.
AU BAI H
   DING P
   WANG Z
   XU T
AE BEIJING DEEPCARE INFORMATION TECHNOLOGY (BEIJ-Non-standard)
GA 201844468P
AB    NOVELTY - The method involves obtaining original sample fundus image and a pathological grade parameter (Sa). Primary sample fundus image enhancement operations are carried-out (Sb) through image preprocessing operation and image data to obtain fundus image training sample. A fundus image training sample pathological level parameter is determined (Sc) corresponding to deep network training to obtain an eye-ground pathological grade identification model. A test input eye-ground image of the eye-ground pathological grade identification model is determined (Sd) to obtain a lesion level recognition result. Determination is made to check whether image pre-processing operation is included with color averaging operation, removing black operation and image size unification operation.
   USE - Method for processing eye-ground image based on deep learning.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for processing eye-ground image based on deep learning.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for processing eye-ground image based on deep learning. '(Drawing includes non-English language text)'
   Step for obtaining original sample fundus image and a pathological grade parameter (Sa)
   Step for carrying-out primary sample fundus image enhancement operations through image preprocessing operation and image data to obtain fundus image training sample (Sb)
   Step for determining fundus image training sample pathological level parameter corresponding to deep network training to obtain an eye-ground pathological grade identification model (Sc)
   Step for determining test input eye-ground image of the eye-ground pathological grade identification model to obtain a lesion level recognition result (Sd)
DC P31 (Diagnosis, surgery (A61B).); T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B1; T01-J10B2A; T01-J10B3B; T01-N01B3A; T01-N01E; T04-D08
IP A61B-003/12; A61B-003/14; G06T-007/00
PD CN108095683-A   01 Jun 2018   A61B-003/12   201840   Pages: 12   Chinese
AD CN108095683-A    CN10992731    11 Nov 2016
PI CN10992731    11 Nov 2016
CP CN108095683-A
      CN104835150-A   UNIV SHENZHEN (UYSZ)   GU Q, LI Q, LIANG P, DENG Y, SU R, XIE L
      CN104881683-A   UNIV TSINGHUA (UYQI)   ZENG Y, WANG Q, YANG J, CHEN R
      US20120257164-A1      
CR CN108095683-A
      HARRY PRATT: "Convolutional Neural Networks for Diabetic Retinopathy", PROCEDIA COMPUTER SCIENCE,relevantClaims[6-10],relevantPassages[2014-52022053]
UT DIIDW:201844468P
ER

PT P
PN CN106886543-A
TI Binding entity described knowledge map learning method, involves performing vector representation operation based on entity relationship, and determining vector space corresponding to vector representation operation.
AU LIU Y
   LIU Z
   LUAN H
   MA S
   SUN M
   XIE R
AE UNIV TSINGHUA (UYQI-C)
GA 201747180R
AB    NOVELTY - The method involves establishing a text description model. A continuous word model is established. Vector value is calculated according to the continuous word model. A convolutional neural network model is established corresponding to the vector value. An entity relationship is established between the vector value and the convolutional neural network. Vector representation operation is performed based on the entity relationship. A vector space is determined corresponding to the vector representation operation. A knowledge map is obtained based on the vector space.
   USE - Binding entity described knowledge map learning method.
   ADVANTAGE - The method enables increasing learning accuracy and practicability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a binding entity described knowledge map learning method.
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-E03; T01-J04C; T01-J05B4P; T01-N01B3; W04-W05A
IP G06F-017/30
PD CN106886543-A   23 Jun 2017   G06F-017/30   201749   Pages: 14   Chinese
AD CN106886543-A    CN10947068    16 Dec 2015
PI CN10947068    16 Dec 2015
UT DIIDW:201747180R
ER

PT P
PN US2018173687-A1; US10346450-B2
TI Method for automatically summarizing state of datacenter for e.g. office by using personal data assistant, involves annotating context graph with combined annotations, where combined annotations describe datacenter with textual description.
AU NOGUERO D S
   LLAGOSTERA J F
   SEGURA A H
   MULERO V M
   CHARLES D S
   SIMO M S
AE CA INC (CATH-C)
GA 201848667L
AB    NOVELTY - The method involves determining a set of context hashes (h1-ht) based on selected properties for each subgraph identified as a relevant region in the context graph corresponding to relevance condition, and comparing the context hashes to subgraph hashes derived from a library of subgraphs to determine a set of subgraph hashes similar to the context hashes derived for each relevant region. Annotations (514) corresponding to the set of subgraph hashes are identified. The annotations are combined in accordance with importance into annotation for each relevant region. The the context graph is annotated with the combined annotations, where the combined annotations describe a datacenter with a textual description.
   USE - Method for automatically summarizing state of a datacenter for an office or enterprise by using a computerized system (claimed) e.g. general-purpose computer and handheld device such as personal data assistant.
   ADVANTAGE - The method allows system administrators to frequently analyze metrics extracted from components of a recurrent neural network system based on relationship between the components of the recurrent neural network system. The method enables predicting the annotations for the subgraph of the context graph corresponding to the relevance condition in the datacenter so as to quickly react to anomaly in the recurrent neural network system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a recurrent neural network for generating annotations describing a set of identified regions.
   Node (A)
   Context hashes (h1-ht)
   Recurrent neural network system (500)
   Recurrent neural network (510)
   Annotations (514)
DC T01 (Digital Computers)
MC T01-J05B2B; T01-J05B4P; T01-J11A; T01-J14; T01-M06A1A; T01-N02B1A
IP G06F-017/24; G06F-017/28; G06F-017/30; G06F-016/00; G06F-016/33; G06F-016/901; H04L-012/24; H04L-012/26
PD US2018173687-A1   21 Jun 2018   G06F-017/24   201842   Pages: 15   English
   US10346450-B2   09 Jul 2019   G06F-016/00   201953      English
AD US2018173687-A1    US392577    28 Dec 2016
   US10346450-B2    US392577    28 Dec 2016
FD  US10346450-B2 Previous Publ. Patent US2018173687
PI ES031644    21 Dec 2016
UT DIIDW:201848667L
ER

PT P
PN CN108126914-A
TI Depth learning based material frame scattered multi-object robot sorting method, involves controlling mechanical arm and suction claw based on normal vector of better sortable point for sorting objects in sorting points.
AU LIU W
   QIAN H
   PAN Z
   SHAO Q
   MA J
   WANG W
   HU J
   QI J
   ZHANG T
   ZHOU B
AE SHANGHAI FANUC ROBOTICS CO LTD (SHAN-Non-standard)
GA 2018464965
AB    NOVELTY - The method involves selecting multiple sortable candidate regions. Image of each sortable candidate region and depth map are inputted into CNN sorting prediction network to judge whether each inputted candidate region is a sorting area. A center mark of the sorting area is utilized as a sorting point. A better sortable point is selected among overall sorting points by using clustering algorithm for calculating normal vector of the better sorting point. A six degree-of-freedom mechanical arm and a suction claw are controlled based on coordinate and normal vector of better sortable point for sorting objects in the sorting points.
   USE - Depth learning based material frame scattered multi-object robot sorting method.
   ADVANTAGE - The method enables utilizing excellent depth learning process for feature extraction process so as to automatically determining whether scattered component comprises sort property, thus effectively realizing cope with frequent change part types and shielding and uncertainty of sorting object.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a depth learning based material frame scattered multi-object robot sorting method.
DC P41 (Crushing: centrifuging, separating solids (B02, B03, B04).); T01 (Digital Computers); T05 (Counting, Checking, Vending, ATM and POS Systems); X25 (Industrial Electric Equipment)
MC T01-E03; T01-J04C; T01-J05B3; T01-J07B; T01-N01B3; T01-N03A2; T05-K05; X25-A03F; X25-F06
IP B07C-005/342; B07C-005/36; G06N-003/04; G06T-007/80
PD CN108126914-A   08 Jun 2018   B07C-005/342   201842   Pages: 9   Chinese
AD CN108126914-A    CN11194494    24 Nov 2017
PI CN11194494    24 Nov 2017
CP CN108126914-A
      CN101618544-A   FANUC LTD (FUFA)   ARIMATSU T, BAN K, KANNO I, WATANABE K
      CN104057459-A   SHANGHAI FANUC ROBOTICS CO LTD (SHAN-Non-standard)   WANG G, YANG H, WANG Q, TONG L
      CN104616319-A   UNIV NANJING INFORMATION SCI & TECHNOLOG (UNAI)   JIN R, XU Y, OUYANG W, HU Z, ZHAO X, LI R
      CN106204614-A   UNIV XIANGTAN (UNXT)   XU H, WANG W, ZHOU W, ZHU J, MO Y, YIN F, ZHOU B, WANG N, PENG S, WANG S
      CN106780605-A   WUHU HIT ROBOT TECHNOLOGY RES INST CO (WUHU-Non-standard)   GAO J, LI C, CAO C
      JP09323817-A      
      US5611437-A   NAKATA S (NAKA-Individual);  TING S (TING-Individual)   OKADA M
UT DIIDW:2018464965
ER

PT P
PN CN106776557-A
TI Robot emotional state recognition method, involves obtaining user information by memory map, determining statement information and user information input, and establishing emotional state recognition model to obtain emotional state of user.
AU JIAN R
   YE J
AE EMOTIBOT TECHNOLOGIES LTD (EMOT-Non-standard)
GA 201738659H
AB    NOVELTY - The method involves extracting sentence information by a user according to input sentence. User information is obtained by a memory map. Statement information and user information input are determined. An emotional state recognition model is established to obtain emotional state of the user. Personal information of the user is extracted based on the input sentence. The personal information is added to the memory map. A depth learning model is established to obtain the emotional state of the user. Key words rule is extracted. Confidence score is determined based on the sentence information.
   USE - Robot emotional state recognition method.
   ADVANTAGE - The method enables adding the user information to the memory map to perform emotion recognition process, and improving recognition user emotional state rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a robot emotional state recognition device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a robot emotional state recognition method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J11A1; T01-J30A
IP G06F-017/27
PD CN106776557-A   31 May 2017   G06F-017/27   201749   Pages: 9   Chinese
AD CN106776557-A    CN11143890    13 Dec 2016
PI CN11143890    13 Dec 2016
CP CN106776557-A
      CN105045857-A   INST COMPUTING TECH CHINESE ACAD SCI (CACT)   CHENG X, YU Z, ZHANG Q, XIONG J, XU H, ZHANG S
      CN105183848-A   BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BIDU)   CHEN H, QI C, WU H, XIN Z, YAN R, TIAN H, ZHAO S, WEN Q, ZHANG X, XU X, ZHOU X
      CN105260745-A   XIAN CANGHAI NETWORK TECHNOLOGY CO LTD (XIAN-Non-standard)   HE J, HU J, LI Y, YOU L, ZHOU K
      CN105930503-A   UNIV TSINGHUA (UYQI)   DENG J, SUN X, XU H, XU J
      CN106126502-A   SICHUAN CHANGHONG ELECTRIC CO LTD (SCCE)   WANG X, ZHONG J, ZHAO L, TAN B, YU C, HAO M, ZHAO H
UT DIIDW:201738659H
ER

PT P
PN CN106778687-A
TI Local estimation and global optimization based gaze point detecting method, involves providing characteristic with global contrast, edge contrast and local evaluation score, and determining target candidate characteristic and image context.
AU LI J
   JIANG B
   LU H
AE UNIV DALIAN TECHNOLOGY (UYDA-C)
GA 201738608J
AB    NOVELTY - The method involves extracting to-be-detected image in an object area. Depth characteristic of a target candidate is determined using a convolutional neural network. Object area attention degree evaluating process is performed according to the depth characteristic. Gaze point information is obtained. A characteristic training regression model is established to predict attention degree. Characteristic is provided with global contrast, edge contrast and local evaluation score. Target candidate characteristic and image context are determined using partial evaluation and global optimization.
   USE - Local estimation and global optimization based gaze point detecting method.
   ADVANTAGE - The method enables detecting different information characteristics to detect semantic information of object image and object image in a human eye fixation region.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a local estimation and global optimization based gaze point detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-J10B3; T01-J16C3; T01-N01B3
IP G06K-009/00; G06K-009/62
PD CN106778687-A   31 May 2017   G06K-009/00   201749   Pages: 9   Chinese
AD CN106778687-A    CN10024964    16 Jan 2017
PI CN10024964    16 Jan 2017
CP CN106778687-A
      CN104103033-A   SICHUAN JIUCHENG INFORMATION TECHNOLOGY (SICH-Non-standard)   MAO L
      CN104143102-A   SICHUAN JIUCHENG INFORMATION TECHNOLOGY (SICH-Non-standard)   MAO L
      CN105913025-A   UNIV HUBEI TECHNOLOGY (UYHI)   LIU Z, XU J, XIONG W, ZHAO S, LIU X, WU Z, XIANG M
      CN106066696-A   UNIV SOUTH CHINA TECHNOLOGY (UYSC)   QIN H, HU D, ZHUO L
CR CN106778687-A
      LIANG Y: "Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis", COMPUTER VISION AND PATTERN RECOGNITION IEEE,relevantClaims[1-2],relevantPassages[]
      : "", ,relevantClaims[1-2],relevantPassages[]
      : "", ,relevantClaims[1-2],relevantPassages[]
UT DIIDW:201738608J
ER

PT P
PN US2018157963-A1; US10345449-B2
TI Device for classifying vehicle on road, has processor for receiving global positioning system data, output layer for outputting processing result of vectors, and neural network for performing action based on classification of vehicle.
AU SALTI S
   SAMBO F
   TACCARI L
   BRAVI L
   SIMONCINI M
   LORI A
AE FLEETMATICS IRELAND LTD (FLEE-Non-standard)
   VERIZON CONNECT IRELAND LTD (VERI-Non-standard)
GA 201846173B
AB    NOVELTY - The device has a processor for receiving global positioning system (GPS) data and values for a set of metrics at a set of GPS points that form a GPS track of a vehicle and for determining additional values for additional metrics using the GPS data or the values for the set of metrics, where the processor determines a set of vectors for the set of GPS points using the GPS data, the values and the additional values and processes the set of vectors by sets of recurrent neural network (RNN) layers of a RNN. The sets of RNN layers determine a classification of the vehicle using a result of processing the set of vectors. An output layer outputs the processing result of the set of vectors. Each RNN layer performs action based on the classification of the vehicle.
   USE - Device for classifying a vehicle on a road based on un-segmented connected handwriting recognition or speech recognition by using a RNN i.e. artificial neural network. Uses include but are not limited to a motor vehicle such as bus, a railed vehicle such as train and tram, a watercraft such as ship, boat and submarine, an aircraft such as glider or unmanned aerial vehicle (UAV) and a spacecraft, an electric vehicle such as electric car, a motorcycle, a scooter and a bicycle.
   ADVANTAGE - The device reduces or eliminates a need to install physical devices along a road or at specific locations, thus reducing or eliminating installation and/or maintenance costs associated with the physical devices and/or reducing or eliminating a need for the vehicle to travel by a specific location when being classified. The device conserves less processing resources by reducing or eliminating a need to normalize every dimension of an input vector to a fixed range. The device permits a vehicle classification system to quickly and accurately classify the vehicle relative to techniques, thus conserving processing resources associated with classifying a vehicle.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a non-transitory computer-readable medium comprising a set of instructions for classifying a vehicle on a road based on un-segmented connected handwriting recognition or speech recognition
   (2) a method for classifying a vehicle on a road based on un-segmented connected handwriting recognition or speech recognition.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for classifying vehicle on a road.
   Step for receiving GPS data and values for set of metrics at set of GPS points that form GPS track of vehicle (410)
   Step for determining a set of vectors for set of GPS points using GPS data, values and additional values (420)
   Step for processing set of vectors by sets of RNN layers of RNN (430)
   Step for determining classification of vehicle using result of processing set of vectors (440)
   Step for performing action based on classification of vehicle (450)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W06 (Aviation, Marine and Radar Systems)
MC T01-C08A; T01-J05B2; T01-J07D3A; T01-S03; T04-D07E; T04-F04; W06-A03A5R; W06-B01B; W06-B03F; W06-B15U; W06-C01B1; W06-C15B; W06-C15U
IP G01S-019/14; G06N-003/04; G06N-003/08; G06N-003/02; G05B-013/02; G08G-001/01
PD US2018157963-A1   07 Jun 2018   G06N-003/04   201841   Pages: 23   English
   US10345449-B2   09 Jul 2019   G06N-003/04   201951      English
AD US2018157963-A1    US601814    22 May 2017
   US10345449-B2    US601814    22 May 2017
FD  US2018157963-A1 CIP of Application US518694
   US2018157963-A1 CIP of Application WOEP079625
   US10345449-B2 CIP of Application US518694
   US10345449-B2 CIP of Application WOEP079625
   US10345449-B2 Previous Publ. Patent US2018157963
PI WOEP079625    02 Dec 2016
   US601814    22 May 2017
UT DIIDW:201846173B
ER

PT P
PN CN108124485-A; WO2019127271-A1
TI Method for warning limb conflict behavior of vehicle driver by utilizing server, involves judging whether output result is existence of limb conflict behavior, and transmitting warning information when output result is existence of behavior.
AU LI H
   LIU G
AE SHENZHEN STREAMAX TECHNOLOGY CO LTD (SHEN-Non-standard)
   STREAMAX TECHNOLOGY CO LTD (STRE-Non-standard)
GA 2018476295
AB    NOVELTY - The method involves collecting a video image of a driver position on a vehicle in real-time, where the video image comprises live scene of a driver body. The video image is converted into a specified data format. The formatted video image is inputted into a pre-trained convolutional neural network to obtain output result of the convolutional neural network. Judgment is made to check whether the output result is existence of limb conflict behavior or absence of limb conflict behavior. Warning information is transmitted when the output result is existence of limb conflict behavior. A training group sample is collected.
   USE - Method for warning limb conflict behavior of a vehicle driver in a by utilizing a server (claimed).
   ADVANTAGE - The method enables achieving safe and effective limb conflict behavior warning function of a driver.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for warning limb conflict behavior of a vehicle driver by utilizing a server
   (2) a computer-readable storage medium for storing a set of instructions to perform a method for warning limb conflict behavior of a vehicle driver by utilizing a server.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for warning limb conflict behavior of a vehicle driver by utilizing a server. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W05 (Alarms, Signalling, Telemetry and Telecontrol)
MC T01-G11B; T01-J07D1; T01-J10B2; T01-J10D; T01-N01B3; T01-N01D1B; T01-N01D3; T01-S03; W05-B07E
IP G06K-009/00; G08B-021/02
PD CN108124485-A   05 Jun 2018   G06K-009/00   201842   Pages: 19   Chinese
   WO2019127271-A1   04 Jul 2019   G06K-009/00   201951      Chinese
AD CN108124485-A    CN80002312    28 Dec 2017
   WO2019127271-A1    WOCN119567    28 Dec 2017
FD  CN108124485-A PCT application Application WOCN119567
PI CN80002312    28 Dec 2017
   WOCN119567    28 Dec 2017
DS WO2019127271-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DJ; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IR; IS; JO; JP; KE; KG; KH; KN; KP; KR; KW; KZ; LA; LC; LK; LR; LS; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SA; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
      (Regional): AL; AT; BE; BG; BW; CH; CY; CZ; DE; DK; EA; EE; ES; FI; FR; GB; GH; GM; GR; HR; HU; IE; IS; IT; KE; LR; LS; LT; LU; LV; MC; MK; MT; MW; MZ; NA; NL; NO; OA; PL; PT; RO; RS; RW; SD; SE; SI; SK; SL; SM; ST; SZ; TR; TZ; UG; ZM; ZW
UT DIIDW:2018476295
ER

PT P
PN US2019188458-A1; TW625680-B1; CN109934080-A
TI Method for recognizing face expressions using e.g. personal digital assistant, involves training recognition model for each of expression groups to classify face images in each of expression groups into one of expression categories.
AU KANG H
   WU J
   YANG Y
   KAO C
   KANG H P
   WU J H
   KAO C C
   GAO Z
AE IND TECHNOLOGY RES INST (ITRI-C)
   IND TECHNOLOGY RES INST (ITRI-C)
GA 201953534U
AB    NOVELTY - The method involves recognizing expression categories of expressions in a set of face images and obtaining recognition results between the expression categories. Similarities between the expression categories are obtained according to the recognition results. The expression categories are classified into a set of expression groups according to the similarities. A first recognition model is trained to classify the expressions in the face images into the expression groups. A second recognition model is trained for each of the expression groups to classify the face images in each of the expression groups into one of the expression categories.
   USE - Method for recognizing face expressions using a recognition device e.g. handheld device such as mobile telephone and handheld computer e.g. personal digital assistant (PDA) and notebook, and mainframe system such as mainframe computer.
   ADVANTAGE - The method enables receiving training images including faces to recognize a face according to the training images, thus improving accuracy of face expression recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for recognizing face expressions.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a system for recognizing face expressions.
   Recognition device (110)
   Input device (112)
   Processor (114)
   Convolutional neural network (116)
   Memory (118)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B1; T01-J10B2A; T01-N01B3; T04-D07F1
IP G06K-009/00; G06T-007/00; G06K-009/62; G06T-009/00
PD US2019188458-A1   20 Jun 2019   G06K-009/00   201951   Pages: 21   English
   TW625680-B1   01 Jun 2018   G06K-009/62   201951      Chinese
   CN109934080-A   25 Jun 2019   G06K-009/00   201951      Chinese
AD US2019188458-A1    US848744    20 Dec 2017
   TW625680-B1    TW144166    15 Dec 2017
   CN109934080-A    CN11445240    27 Dec 2017
PI TW144166    15 Dec 2017
UT DIIDW:201953534U
ER

PT P
PN CN108154495-A
TI DCCAE network-based multi-temporal cross-sensor remote sensing image relative spectral alignment algorithm, has set of instructions for representing network output result in potential space to realize spectral alignment of potential space.
AU ZHOU Y
   YANG J
   FENG L
   LI C
   ZHANG T
   ZHANG Y
AE UNIV TIANJIN (UTIJ-C)
GA 2018479938
AB    NOVELTY - The algorithm has set of instructions for selecting (1) a certain number of un-changed pixels as a training sample according to groundtruth. Weights and bias values of multiple layers of a DCCAE network are determined. Network parameter is obtained to maximize correlation and minimize reconstruction error. A trained DCCAE network is constructed. The training sample is inputted to the DCCAE network. Nonlinear features of an image are extracted by using two neural networks. Correlation between the reconstruction error and characteristic representation is optimized (2) by using gradient descent process. A first image and a second image are inputted into the trained DCCAE network. A network output result is represented (3) in a potential space to realize relative spectral alignment of the potential space.
   USE - DCCAE network-based multi-temporal cross-sensor remote sensing image relative spectral alignment algorithm.
   ADVANTAGE - The algorithm enables improving accuracy of spectral alignment in an effective manner by using a deep neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a DCCAE network-based multi-temporal cross-sensor remote sensing image relative spectral alignment algorithm. '(Drawing includes non-English language text)'
   Step for selecting a certain number of un-changed pixels as a training sample according to groundtruth (1)
   Step for optimizing correlation between the reconstruction error and characteristic representation by using gradient descent process (2)
   Step for representing network output result in potential space to realize relative spectral alignment of potential space (3)
DC T01 (Digital Computers)
MC T01-J04B2; T01-J10B2; T01-N01B3
IP G06T-007/00
PD CN108154495-A   12 Jun 2018   G06T-007/00   201842   Pages: 9   Chinese
AD CN108154495-A    CN11146227    17 Nov 2017
PI CN11146227    17 Nov 2017
CP CN108154495-A
      CN103198483-A   UNIV XIDIAN (UYXN)   JIAO L, ZHANG X, GONG M, WANG G, WANG S, ZHONG H, SUN Y
      CN106203256-A   UNIV SHANDONG (USHA)   BEN X, ZHANG P, ZHANG Z, LIU J, WANG Y
      CN106529604-A   UNIV SOOCHOW (USWZ)   ZHANG Z, JIA L, LI F, ZHANG L, WANG B
CR CN108154495-A
      MICHELE VOLPI ET AL: "Multi-sensor change detection based on nonlinear canonical correlations", 2013 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM,relevantClaims[1],relevantPassages[1944-1947]
      HICHEM SAHBI: "Misalignment resilient CCA for interactive satellite image change detection", 2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION,relevantClaims[1],relevantPassages[3326-3331]
      MICHELE VOLPI ET AL: "Spectral alignment of multi-temporal cross-sensor images with automated kernel canonical correlation analysis", ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING,relevantClaims[1],relevantPassages[50-63]
      WEIRAN WANG ET AL: "On Deep Multi-View Representation Learning: Objectives and Optimization", HTTPS://ARXIV.ORG/ABS/1602.01024,relevantClaims[1],relevantPassages[1-34]
UT DIIDW:2018479938
ER

PT P
PN TW201905896-A; TW629680-B1
TI Speech confidence evaluation method and system to use the Artificial neural network model for deep learning of the tonal syllables.
AU YANG J
   CHEN J
   YANG S
   YANG J H
   CHEN J H
   YANG S F
AE CHUNGHWA TELECOM CO LTD (CHWT-C)
GA 2019515061
AB    NOVELTY - The invention discloses a speech confidence evaluation method and system, which can capture the important factor components of the syllable and use the prosodic feature parameters as the auxiliary, and use the Artificial neural network model for deep learning of the tonal syllables, and learn the correlation between the contexts before and after the syllable and the prosody. The speech confidence evaluation system outputs and accumulates the confidence scores of each syllable. This method can improve the reliability of speech confidence evaluation.
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J04B2; T01-N01B3; W04-V01
IP G10L-015/14; G10L-015/02; G10L-015/20
PD TW201905896-A   01 Feb 2019   G10L-015/14   201947   Pages: 0   Chinese
   TW629680-B1   11 Jul 2018   G10L-015/14   201947      Chinese
AD TW201905896-A    TW119947    15 Jun 2017
   TW629680-B1    TW119947    15 Jun 2017
PI TW119947    15 Jun 2017
UT DIIDW:2019515061
ER

PT P
PN CN108122254-A; WO2019113912-A1
TI Structured light based three-dimensional image reconstructing method, involves performing three-dimensional image reconstruction process on object to obtain three-dimensional image based on calibration parameter and decoding information.
AU SONG Z
   ZENG H
AE SHENZHEN INST ADVANCED TECHNOLOGY (CAAT-C)
   SHENZHEN INST ADVANCED TECHNOLOGY (CAAT-C)
GA 201845640T
AB    NOVELTY - The method involves identifying encoded element image by using a pre-trained deep learning network according to auxiliary coding feature point and primary coding feature point. Encoding information of the identification encoded element image is matched with encoding information of pre-stored coding pattern according to the preset line coding strategy. Decoding process is performed on the primary coding feature point and the auxiliary coding feature point to obtain decoding information. Three-dimensional image reconstruction process is performed on an object to obtain three-dimensional image of the object based on calibration parameter of a three-dimensional image reconstruction system and the decoding information.
   USE - Structured light based three-dimensional image reconstructing method.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a structured light based three-dimensional image reconstructing device
   (2) a computer-readable storage medium for storing a set of instructions for structured light based three-dimensional image reconstructing.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a structured light based three-dimensional image reconstructing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B2A; T01-J10B3A; T01-J10C4; T01-J10D; T01-N01B3; T01-S03
IP G06T-015/00; G06T-003/00; G06T-007/73
PD CN108122254-A   05 Jun 2018   G06T-007/73   201841   Pages: 14   Chinese
   WO2019113912-A1   20 Jun 2019   G06T-007/73   201947      Chinese
AD CN108122254-A    CN11344608    15 Dec 2017
   WO2019113912-A1    WOCN116321    15 Dec 2017
   WO2019113912-A1    WOCN116321    15 Dec 2017
PI CN11344608    15 Dec 2017
   WOCN116321    15 Dec 2017
DS WO2019113912-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DJ; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IR; IS; JO; JP; KE; KG; KH; KN; KP; KR; KW; KZ; LA; LC; LK; LR; LS; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SA; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
      (Regional): AL; AT; BE; BG; BW; CH; CY; CZ; DE; DK; EA; EE; ES; FI; FR; GB; GH; GM; GR; HR; HU; IE; IS; IT; KE; LR; LS; LT; LU; LV; MC; MK; MT; MW; MZ; NA; NL; NO; OA; PL; PT; RO; RS; RW; SD; SE; SI; SK; SL; SM; ST; SZ; TR; TZ; UG; ZM; ZW
UT DIIDW:201845640T
ER

PT P
PN CN108121860-A
TI Multi-source fusion information based biological starter Cyber-physical system modeling method, involves obtaining mapping relationship between work task data and biological state data, and performing biological starter propagation process.
AU LI Y
   SI C
   ZHANG J
   LIU Q
AE UNIV CHINA ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 2018456495
AB    NOVELTY - The method involves obtaining key factor information based on multi-source information fusion. An optimal biological starter decision result is output based on a deep learning modeling. A multi-source biological starter propagation factor and a fusion producing yeast factor are obtained. An influence factor decision framework is constructed. Key factor information is obtained. Biological state data, starter resource data and work task data are collected. A physical information fusion system model is established to obtain large data based on deep learning algorithm. A mapping relationship between the work task data and the biological state data is obtained. A biological starter propagation process is performed to realize Cyber-physical system (CPS) modeling of a starter propagation process.
   USE - Multi-source fusion information based biological starter CPS modeling method.
   ADVANTAGE - The method enables avoiding problem of modeling directly from complex sensing information and a physical device, and obtaining biological state data in a biological starter propagation process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-source fusion information based biological starter cyber-physical system modeling method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B4P; T01-J15X; T01-J16C2; T01-J30A
IP G06F-017/50
PD CN108121860-A   05 Jun 2018   G06F-017/50   201841   Pages: 5   Chinese
AD CN108121860-A    CN11315323    12 Dec 2017
PI CN11315323    12 Dec 2017
UT DIIDW:2018456495
ER

PT P
PN CN108090565-A
TI Convolutional neural network parallelization training method, involves calculating local error of single character image, and stopping convolutional neural network training when local error is less than preset threshold value.
AU HONG Q
   RUAN A
   SHI A
AE UNIV CHINA ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 201843260N
AB    NOVELTY - The method involves creating model structure parameter and training parameter to access a shared memory of a CPU and a field programmable gate array (FPGA) based on FPGA convolutional neural network parallelization process, where the structure parameter comprises output level of a network and local error and the training parameter comprises convolutional layer biasing vector, full connection weight matrix and full connection layer biasing vector. Gradient of a single character image is calculated to update weight parameter. The local error of the single character image is calculated for updating offset parameter. Convolutional neural network training is stopped when the local error is less than preset threshold value.
   USE - Convolutional neural network parallelization training method.
   ADVANTAGE - The method enables reducing problem of insufficient storage space of a large-scale convolutional neural network structure by utilizing FPGA training sample.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network parallelization training method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-F06; T01-H03D; T01-J04A; T01-N01B3
IP G06N-003/04; G06N-003/08
PD CN108090565-A   29 May 2018   G06N-003/08   201841   Pages: 10   Chinese
AD CN108090565-A    CN10037896    16 Jan 2018
PI CN10037896    16 Jan 2018
UT DIIDW:201843260N
ER

PT P
PN CN108364293-A
TI On-line training thyroid tumor ultrasound image identifying method, involves performing malignant recognition by utilizing thyroid tumor ultrasound image recognition model after selecting tumor area and amplifying certain edge range.
AU XIANG J
   LU H
   GUAN Q
   WANG F
   WANG Y
   LI D
   DU J
   QIN Y
AE UNIV FUDAN SHANGHAI CANCER CENT (UYFU-C)
   UNIV SHANGHAI JIAOTONG (USJT-C)
GA 201862773R
AB    NOVELTY - The method involves storing cut image in an image database. A first training set is formed. A selected convolutional neural network is trained by utilizing the first training set t o form a first thyroid tumor ultrasound image recognition model. Multiple thyroid tumor ultrasound images are obtained. A certain edge range is amplified. A part of an original image in the image database is combined to form a second training set. A second thyroid tumor ultrasound image recognition model is formed by utilizing the second training set. An ultrasound image of a to-be-identified thyroid tumor is obtained. A malignant recognition is performed by utilizing the second thyroid tumor ultrasound image recognition model after selecting a tumor area and amplifying certain edge range.
   USE - On-line training thyroid tumor ultrasound image identifying method.
   ADVANTAGE - The method enables realizing the reuse of case images, learning, memory and image characteristics of thyroid tumor and improving the generalization ability and prediction accuracy of the model.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an on-line training thyroid tumor ultrasound image identifying device
   (2) a computer-readable storage medium for storing set of computer programs for an on-line training thyroid tumor ultrasound image identifying method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an on-line training thyroid tumor ultrasound image identifying device. '(Drawing includes non-English language text)'
DC B04 (Natural products and polymers. Including testing of body fluids (other than blood typing or cell counting), pharmaceuticals or veterinary compounds of unknown structure, testing of microorganisms for pathogenicity, testing of chemicals for mutagenicity or human toxicity and fermentative production of DNA or RNA. General compositions.); P31 (Diagnosis, surgery (A61B).); S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC B11-C08J; B11-C11; B12-K04C1; B12-K04G2A; S05-D03; T01-J05B2A; T01-J05B4F; T01-J10B2A; T01-N01B3; T01-N01E; T01-S03
IP A61B-008/00; A61B-008/08; G06T-007/00
PD CN108364293-A   03 Aug 2018   G06T-007/00   201861   Pages: 17   Chinese
AD CN108364293-A    CN10318236    10 Apr 2018
PI CN10318236    10 Apr 2018
UT DIIDW:201862773R
ER

PT P
PN KR1874564-B1
TI Method of controlling approach of harmful animals and birds, involves performing operation of camera flash, sounding of buzzer, and operation of rotary arm for sneezing or sneezing by automatic injector.
AU CHOI S Y
   YEONGA P
AE GWANGJIN ENTERPRISE CO (GWAN-Non-standard)
GA 201853698L
AB    NOVELTY - The method involves performing a power-ON process in which a control apparatus receives the power from a power source unit (10). The harmful animal or bird is sensed by using a PIR sensor (20). The camera flash (30) and an infrared (IR) sensor (40) are operated to perform the flash and IR sensor operation process, when an IoT deep learning camera (90) senses and recognizes harmful animals or bird. The buzzer is made to sound an alarm sound by a control unit (50), when the harmful animal or bird approaches within preset distance. A short range recognition process of determining whether the control unit continues to approach the harmful animal or bird within a predetermined distance, is performed. The operation of the camera flash, sounding of the buzzer, and the operation of a rotary arm (80) for sneezing or sneezing by an automatic injector (70) are performed in stepwise.
   USE - Method of controlling approach of harmful animals and birds by IoT fusion with IoT deep learning camera and recognition technology.
   ADVANTAGE - Since the solar panel or the battery is used as the power source and the control operation is performed only when the harmful animal or bird is recognized by the camera, the maintenance cost is efficiently managed, the production cost is minimized, the burden on the farmhouse is reduced.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic diagram illustrating the method of controlling approach of harmful animals and birds by IoT fusion.
   Power source unit (10)
   PIR sensor (20)
   Camera flash (30)
   IR sensor (40)
   Control unit (50)
   Automatic injector (70)
   Rotary arm (80)
   IoT deep learning camera (90)
DC S02 (Engineering Instrumentation); W04 (Audio/Video Recording and Systems); W05 (Alarms, Signalling, Telemetry and Telecontrol)
MC S02-A03; S02-A10B; W04-M01G7; W04-M01H5; W05-A02; W05-B01C5
IP A01M-029/06; A01M-029/12; A01M-029/16; G01B-011/02; G08B-013/196; G08B-003/10
PD KR1874564-B1   04 Jul 2018   A01M-029/16   201847   Pages: 14   
AD KR1874564-B1    KR007540    22 Jan 2018
PI KR007540    22 Jan 2018
UT DIIDW:201853698L
ER

PT P
PN US10007863-B1
TI Method for detecting logo in images in video frames selected from video stream involves deciding most likely logo match by combining results from first match, second match and third match.
AU PEREIRA J P
   BROCKLEHURST K
   KULKARNI S S
   WENDT P
AE GRACENOTE INC (GRAC-Non-standard)
GA 201849761G
AB    NOVELTY - The method involves applying saliency analysis and segmentation of selected regions in selected video frame to determine the segmented likely logo regions. The segmented likely logo regions are processed with feature matching using correlation to generate first match, neural network classification using convolutional neural network to generate second match and text recognition using character segmentation and string matching to generate third match. A most likely logo match is decided by combining results from the first match, the second match, and the third match.
   USE - Method for detecting logo in images in video frames selected from video stream.
   ADVANTAGE - Achieves high robustness and detection accuracy by recognizing logo in images and videos and further verifying with feature matching and neural net classification. The media recognition system for both image and video content recognition and media fingerprinting is readily scalable to large multimedia databases, and exhibits high accuracy in finding correct clip, low probability of misidentifying wrong clip and robustness to various types of distortion.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an apparatus; and
   (2) a non-transitory computer readable storage medium.
   DESCRIPTION OF DRAWING(S) - The drawing is the flowchart of the process for logo recognition including image processing, logo detection and recognition.
   Generating signatures for likely logo segments (211)
   String matching and logo match scoring (213)
   Logo Search (214)
   Applying decision logic (215)
   Recognizing brand and location (216)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J05B2; T01-J05B4P; T01-J10B2A; T01-J16C2; T01-N01D1B; T01-N03A2; T04-D02; T04-D03; T04-D04; T04-D07B
IP G06K-009/34; G06K-009/46; G06K-009/52; G06K-009/62; G06K-009/66; G06T-007/00; G06T-007/60
PD US10007863-B1   26 Jun 2018   G06K-009/34   201844   Pages: 29   English
AD US10007863-B1    US172826    03 Jun 2016
FD  US10007863-B1 Provisional Application US171820P
PI US171820P    05 Jun 2015
   US172826    03 Jun 2016
FS 348/0E7071; 348/589000; 375/240080; 382/100000; 382/103000; 382/141000; 382/155000; 382/156000; 382/159000; 382/173000; 382/181000; 382/190000; 382/195000; 382/209000; 382/217000; 382/218000; 707/736000; 707/737000; 707/754000; 715/201000; 715/230000; 715/704000; 715/764000; 725/019000
CP US10007863-B1
      US20030076448-A1      
      US6226041-B1   SARNOFF CORP (STRI)   FLORENCIO D A F, JONSSON R H
      US6282317-B1   EASTMAN KODAK CO (EAST)   LUO J, ETZ S, SINGHAL A
      US9158995-B2   XEROX CORP (XERO)   RODRIGUEZ-SERRANO J A, LARLUS-LARRONDO D
      US9628837-B2   DAVIDSON Y (DAVI-Individual);  GABBER A (GABB-Individual)   DAVIDSON Y, GABBER A
      EP2259207-A1   VICOMTECH-VISUAL INTERACTION&COMMUNICA (VICO-Non-standard)   AGINAKO BENGOA N, GARCIA OLAIZOLA I, LABAYEN ESNAOLA M
      US20140079321-A1      
      US8655878-B1   ZEITERA LLC (ZEIT-Non-standard)   KULKARNI S S, PEREIRA J P, GAJJAR P D, MERCHANT S, RAMANATHAN P, STOJANCIC M
      US8171030-B2   ZEITERA LLC (ZEIT-Non-standard)   PEREIRA J P, STOJANCIC M M, MERCHANT S
      US8189945-B2   ZEITERA LLC (ZEIT-Non-standard)   STOJANCIC M, RAMANATHAN P, WENDT P, PEREIRA J P
      US8195689-B2   ZEITERA LLC (ZEIT-Non-standard)   RAMANATHAN P, PEREIRA J P, MERCHANT S
      US8229227-B2   ZEITERA LLC (ZEIT-Non-standard)   STOJANCIC M M, PEREIRA J P, MERCHANT S
      US8335786-B2   ZEITERA LLC (ZEIT-Non-standard)   PEREIRA J P, KULKARNI S S, MERCHANT S, RAMANATHAN P, GAJJAR P D
      US8959108-B2   ZEITERA LLC (ZEIT-Non-standard)   PEREIRA J P, MERCHANT S, RAMANATHAN P, KULKARNI S S, STOJANCIC M
CR US10007863-B1
      Richard J.M. Den Hollander et al., "Logo Recognition in Video Stills by String Matching, " Delft University of Technology, Oct. 2003 (4 pages).
      Goncalo Filipe Palaio Oliveira, "Sabado_Smart Brand Detection, " Sep. 2, 2015 (92 pages).
UT DIIDW:201849761G
ER

PT P
PN CN108154505-A
TI Deep neural network based diabetic retinopathy detection method, involves determining diabetic retinopathy degree if classification result is effective, and outputting detection result based on retinopathy degree.
AU ZHANG Y
   ZHONG J
   GUO J
   GUO Q
   CHEN Y
   ZHANG L
   LI J
   GAO Z
   HE T
   ZHANG W
   SHU X
   XU X
   DU X
AE UNIV SICHUAN (USCU-C)
   SICHUAN PROVINCIAL PEOPLES HOSPITAL SICH (SICH-Non-standard)
GA 201847992Y
AB    NOVELTY - The method involves collecting multiple fundus color image data that are input to a preset depth neural network model. A classification result is acquired by the depth neural network model corresponding to a confidence level. Judgment is made to check whether the classification result is effective. Diabetic retinopathy degree is determined corresponding to the classification result if the classification result is effective. A detection result is output based on the diabetic retinopathy degree. A test result output by the depth neural network model is acquired. The deep neural network model is positioned in a server if the test result is matched with a preset comparison tag.
   USE - Deep neural network based diabetic retinopathy detection method.
   ADVANTAGE - The method enables uploading an eye ground image by a user to effectively improve accuracy of the obtained detection result, thus providing user convenience, reducing processing cost, effectively improving diagnosis efficiency under condition of lack of professional doctors and saving time for a patient.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep neural network based diabetic retinopathy detection device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep neural network based diabetic retinopathy detection method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J10B2; T01-J10B3B; T01-N01E1; T01-N02A3C; T04-D08; T04-K03B
IP G06N-003/04; G06N-003/08; G06T-007/00
PD CN108154505-A   12 Jun 2018   G06T-007/00   201845   Pages: 13   Chinese
AD CN108154505-A    CN11439401    26 Dec 2017
PI CN11439401    26 Dec 2017
UT DIIDW:201847992Y
ER

PT P
PN CN108053841-A; WO2019080502-A1
TI Method for predicting disease by using voice data in application server, involves inputting patient voice data to deep neural network model, and determining type of patient voice data according to output state of deep neural network model.
AU LIANG H
   WANG J
   XIAO J
AE PINGAN SCI & TECHNOLOGY SHENZHEN CO LTD (PING-C)
   PINGAN TECHNOLOGY SHENZHEN CO LTD (PING-C)
GA 201840785H
AB    NOVELTY - The method involves training a deep neural network model based on training data. A voice classification process is performed based on the training data. The deep neural network model is provided with an input layer and an output layer. An output state of voice type is determined by the output layer. Real-time patient voice data is obtained by performing data processing. The real-time patient voice data is input to the input layer of the trained deep neural network model. Type of the patient voice data is determined according to the output state of the trained deep neural network model.
   USE - Method for predicting disease by using voice data in an application server (claimed).
   ADVANTAGE - The method enables realizing obtaining the patient voice data in a rapid manner so as to provide data supporting and reference to a formal diagnosis of subsequent doctor, thus improving convenience of the doctor and the patient.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computer-readable storage medium for storing set of instruction for predicting disease by using voice data in an application server.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for predicting disease by using voice data in an application server. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J05B2; T01-N01B3; T01-N01E1; T01-N02A3C; T01-S03; W04-V01; W04-V04A4; W04-V05
IP G10L-015/02; G10L-015/06; G10L-025/30; G10L-025/66
PD CN108053841-A   18 May 2018   G10L-025/66   201837   Pages: 15   Chinese
   WO2019080502-A1   02 May 2019   G10L-025/66   201932      Chinese
AD CN108053841-A    CN10995691    23 Oct 2017
   WO2019080502-A1    WOCN089428    01 Jun 2018
PI CN10995691    23 Oct 2017
DS WO2019080502-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DJ; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IR; IS; JO; JP; KE; KG; KH; KN; KP; KR; KW; KZ; LA; LC; LK; LR; LS; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SA; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
      (Regional): BW; GH; GM; KE; LR; LS; MW; MZ; NA; RW; SD; SL; ST; SZ; TZ; UG; ZM; ZW; EA; AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; OA
CP    WO2019080502-A1
      CN102342858-A   UNIV SHANGHAI TCM (UTCM)   CHEN C, GUO R, YAN H, WANG Y, YAN J
      CN106709254-A   TIANJIN INTELLIGENT TECH INST CASIA CO (TIAN-Non-standard)   SUN Z, CAO D, LI Q, TAN T
      CN106710599-A   SHENZHEN SAHALA DATA TECHNOLOGY CO LTD (SHEN-Non-standard)   CAI G
      CN108053841-A   PINGAN SCI & TECHNOLOGY SHENZHEN CO LTD (PING)   LIANG H, WANG J, XIAO J
      WO2016192612-A1   CHEN K (CHEN-Individual)   CHEN K
UT DIIDW:201840785H
ER

PT P
PN CN107909566-A
TI Deep learning based skin melanoma image recognizing method, involves adjusting image data according parameter curve concatenation, and sorting adjusted image data to recognize malignant melanin tumor, seborrheic keratosis, and benign nevus.
AU WANG Y
   ZHOU C
   LIU J
AE UNIV HANGZHOU DIANZI (UYHH-C)
   ZHOU C (ZHOU-Individual)
GA 201830763M
AB    NOVELTY - The method involves identifying a skin specific characteristic and a model parameter in image data through a transfer learning module. Image classification is performed in the image data to adjust parameter through a network parameter adjustment module. A training result of a convolutional neural network is displayed for a visual real- time display. The image data is adjusted according to a set value and parameter value curve concatenation of the convolutional neural network. The adjusted image data is sorted to recognize malignant melanin tumor, seborrheic keratosis, and benign nevus.
   USE - Deep learning based skin melanoma image recognizing method.
   ADVANTAGE - The method enables improving accuracy of the skin lesion classification, and avoiding limitation of manual selecting characteristic, stronger adaptability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning based skin melanoma image recognizing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-E01A; T01-J05B2; T01-J10B2A; T01-N01B3; T01-N01D1B; T04-D04; W04-W05A
IP G06K-009/62; G06T-007/00
PD CN107909566-A   13 Apr 2018   G06T-007/00   201828   Pages: 10   Chinese
AD CN107909566-A    CN11030895    28 Oct 2017
PI CN11030895    28 Oct 2017
UT DIIDW:201830763M
ER

PT P
PN CN107767413-A
TI Convolutional neural network based image depth estimating method, involves determining convolution-deconvolution neural network model training parameter, and inputting to-be-processed image to neural network model to obtain depth map.
AU LI G
   YU X
AE UNIV SOUTH CHINA TECHNOLOGY (UYSC-C)
GA 201820268L
AB    NOVELTY - The method involves establishing a convolution-reverse convolution neural network model (S1), where the convolution-reverse convolution neural network model comprises a set of different convolutional layers and multiple convolution-deconvolution and activation layers. A training set is selected (S2). Convolution-deconvolution neural network model training parameter is determined (S3) to minimize a loss function to form an image depth estimation neural network model. A to-be-processed image is input (S4) to an image depth estimation neural network model to obtain a depth map.
   USE - Convolutional neural network based image depth estimating method.
   ADVANTAGE - The method enables improving depth map gray value obtaining accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based image depth estimating method. '(Drawing includes non-English language text)'
   Step for establishing convolution-reverse convolution neural network model (S1)
   Step for selecting training set (S2)
   Step for determining convolution-deconvolution neural network model training parameter to minimize loss function to form image depth estimation neural network model (S3)
   Step for inputting to-be-processed image to image depth estimation neural network model to obtain depth map (S4)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B2; T01-N01B3; T04-D07D3
IP G06T-007/50
PD CN107767413-A   06 Mar 2018   G06T-007/50   201820   Pages: 10   Chinese
AD CN107767413-A    CN10850577    20 Sep 2017
PI CN10850577    20 Sep 2017
UT DIIDW:201820268L
ER

PT P
PN US2018046458-A1; JP2018026027-A
TI Arithmetic processing device, has data storing unit for storing first data and second data, and arithmetic unit for performing operation that uses first data and second data by using results of row portion operations.
AU KURAMOTO M
AE FUJITSU LTD (FUIT-C)
   FUJITSU LTD (FUIT-C)
GA 201813673S
AB    NOVELTY - The device (1) has a data storing unit for storing first data and second data each including element data that forms a matrix. An arithmetic unit repeats row portion operation for each of first and second predetermined rows of the first and second data that are stored in the data storing unit, by using the element data included in the first predetermined row and the element data included in the second predetermined row based on a number of columns of the second data, and performs operation that uses the first data and the second data by using results of the row portion operations.
   USE - Arithmetic processing device.
   ADVANTAGE - The device acquires output data in each of the layers as feature values used for image recognition and performs image recognition by performing deep learning that repeatedly updates a parameter by using the feature values acquired in each of the layers if the input data is assumed to be the input image, thus improving accuracy of image recognition. The device reduces a number of times that the same row is used as the operation is performed while shifting the row to be used, thus efficiently using input data. The device reduces a number of times of rewriting in the register files and speeds up arithmetic operation process, while suppressing increase in cost.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for controlling an arithmetic processing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an arithmetic processing device.
   Convolution forward operation direction (P1)
   Convolution backward operation direction (P2)
   Arithmetic processing device (1)
   Input data (10)
   Output data (20)
   Arithmetic operation process layers (101-103)
DC T01 (Digital Computers)
MC T01-E02D; T01-F03; T01-F05F; T01-J04B2; T01-J04C; T01-J10B1; T01-J10B2A; T01-J10B3A
IP G06F-009/30; G06F-017/10; G06F-017/16
PD US2018046458-A1   15 Feb 2018   G06F-009/30   201814   Pages: 58   English
   JP2018026027-A   15 Feb 2018   G06F-017/16   201814   Pages: 48   Japanese
AD US2018046458-A1    US651651    17 Jul 2017
   JP2018026027-A    JP158379    12 Aug 2016
PI JP158379    12 Aug 2016
UT DIIDW:201813673S
ER

PT P
PN CN107644208-A
TI Method for detecting human face through server, involves inputting physical characteristic information to neural network to obtain face detection result, and obtaining relationship between characteristic information and detection result.
AU DU K
AE BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BIDU-C)
GA 2018103102
AB    NOVELTY - The method involves obtaining a to-be-detected image. The to-be-detected image is input to a first convolutional neural network for training. Human face characteristic information and physical characteristic information are obtained. Face characteristic is extracted by using a convolutional neural network. The physical characteristic information is input to a second convolutional neural network for training to obtain a face detection result. A relationship between the physical characteristic information and a face detection result is obtained.
   USE - Method for detecting a human face through a server (claimed).
   ADVANTAGE - The method enables improving accuracy of human face detecting under face size relatively small.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for detecting human face through a server
   (2) a computer readable storage medium for storing a set of instructions for detecting a human face through a server.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for detecting human face through server. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-N01B3; T01-N02A3C
IP G06K-009/00; G06N-003/04
PD CN107644208-A   30 Jan 2018   G06K-009/00   201813   Pages: 15   Chinese
AD CN107644208-A    CN10858133    21 Sep 2017
PI CN10858133    21 Sep 2017
UT DIIDW:2018103102
ER

PT P
PN CN107644426-A
TI Pyramid coding and decoding structure-based image semantic segmentation method, involves classifying deep feature map of decoding network by convolutional layer comprising output channels as classifier of Softmax layer.
AU LIU B
   TAN Z
   YU N
AE UNIV CHINA SCI & TECHNOLOGY (UCST-C)
GA 201811292X
AB    NOVELTY - The method involves processing (1) an input image through a coding network comprising a convolutional neural network model and a pyramid pooling model. The deep feature map is recovered (2) to the same resolution as the input image through a decoding network comprising reverse convolutional neural network model. The deep feature map of the decoding network is classified (3) by a convolutional layer comprising the output channels as the number of categories and a classifier of Softmax layer to realize image semantic segmentation.
   USE - Pyramid coding and decoding structure-based image semantic segmentation method.
   ADVANTAGE - The image semantic segmentation method improves the capability of the network to semantic segmentation of image, and considers the network model size and speed of operation.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the pyramid coding and decoding structure-based image semantic segmentation method. (Drawing includes non-English language text)
   Step for processing an input image through a coding network comprising a convolutional neural network model and a pyramid pooling model (1)
   Step for recovering the deep feature map to the same resolution as the input image through a decoding network comprising reverse convolutional neural network model (2)
   Step for classifying the deep feature map of the decoding network by a convolutional layer (3)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B1; T01-J10B2; T01-J10D; T01-J16C3; T04-D04
IP G06K-009/62; G06N-003/04; G06T-007/11
PD CN107644426-A   30 Jan 2018   G06T-007/11   201812   Pages: 10   Chinese
AD CN107644426-A    CN10948567    12 Oct 2017
PI CN10948567    12 Oct 2017
UT DIIDW:201811292X
ER

PT P
PN CN107609470-A
TI Outdoor fire early smoke video detection method, involves performing smoke diffusion process by using moving direction features, and identifying candidate smoke regions by using convolutional neural network.
AU WEI W
   CHENG Y
   WEI M
   CHEN S
   LIU J
AE UNIV CHENGDU INFORMATION TECHNOLOGY (UYUH-C)
GA 201808445B
AB    NOVELTY - The method involves decompressing a video shot to obtain a frame sequence, utilizing an average filter to filter an original image, and removing noise by morphological process. Fire smoke candidate area extracting process is performed according to initial motion characteristics of fire smoke. Motion foreground area extracting process is performed by using a Gaussian mixture model (GMM). Smoke diffusion process is performed by using moving direction features. Candidate smoke regions are identified by using a convolutional neural network.
   USE - Outdoor fire early smoke video detection method.
   ADVANTAGE - The method enables improving outdoor fire early smoke video detection rate and application range.
   DETAILED DESCRIPTION - The convolutional neural network comprises three convolutional layers and an output layer.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an outdoor fire early smoke video detection method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B1; T01-J10B2; T04-D02; T04-D03A; T04-D04
IP G06K-009/00; G06K-009/32; G06K-009/62; G06T-007/254; G06T-007/277
PD CN107609470-A   19 Jan 2018   G06K-009/00   201811   Pages: 9   Chinese
AD CN107609470-A    CN10636033    31 Jul 2017
PI CN10636033    31 Jul 2017
UT DIIDW:201808445B
ER

PT P
PN CN107578436-A
TI Fully-convolutional neural network based on monocular image depth estimation method, involves training parameter of fully-convolutional neural network, and obtaining red-green-blue image prediction depth in convolutional neural network.
AU ZHU P
   HUO Z
AE UNIV NANJING POSTS & TELECOM (UNPT-C)
GA 201806024Q
AB    NOVELTY - The method involves obtaining training image data. A fully convolutional neural network (FCN) is established. The training image data is inputs to the FCN. A feature image is obtained from a tank layer in the FCN. Amplifying process is performed on the feature image. Size of the feature image is calculated according to a preset tank layer. A predicted depth image is obtained by utilizing forward characteristic image fusion. A parameter of the FCN is trained by utilizing stochastic gradient descent (SGD) process. Red-green-blue (RGB) image prediction depth is obtained in the FCN.
   USE - FCN based on monocular image depth estimation method.
   ADVANTAGE - The method enables improving de-convolution process output image resolution in FCN, and removing fully connected layer from the FCN so as to effectively reduce network parameter.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a tank layer. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B1; T01-J10B2; T01-J10D; T01-N01B3; T04-D04
IP G06K-009/62; G06N-003/04; G06T-007/55
PD CN107578436-A   12 Jan 2018   G06T-007/55   201809   Pages: 8   Chinese
AD CN107578436-A    CN10649934    02 Aug 2017
PI CN10649934    02 Aug 2017
UT DIIDW:201806024Q
ER

PT P
PN CN107301389-A
TI Human face characteristic based user gender identifying method, involves obtaining gender identification process of candidate user corresponding to sex of candidate user, and obtaining candidate user set for counting number of images.
AU ZENG Y
AE GUANGDONG OPPO MOBILE COMMUNICATION CO L (GDOP-C)
GA 201774059R
AB    NOVELTY - The method involves extracting human face characteristic information of a candidate user to perform gender identification process of a convolutional neural network. Gender of the candidate user is obtained from a preview image extraction process for identifying face feature information of the user, if a face of the face feature information is same as a target face of alternative face feature information of the user. A gender identification process of the candidate user is obtained corresponding to sex of the candidate user. A candidate user set is obtained for counting number of images.
   USE - Method for identifying user gender based on human face characteristic in a terminal (claimed).
   ADVANTAGE - The method enables improving gender identification process accuracy and speed according to the sex, obtaining different image processing parameter, thus improving image processing efficiency.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for identifying user gender based on human face characteristic in a terminal
   (2) a non-transitory computer-readable storage medium for storing set of instructions to perform a method for identifying user gender based on human face characteristic in a terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face characteristic based user gender identifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B1; T01-J10B2; T01-S03
IP G06K-009/00
PD CN107301389-A   27 Oct 2017   G06K-009/00   201777   Pages: 14   Chinese
AD CN107301389-A    CN10458359    16 Jun 2017
PI CN10458359    16 Jun 2017
UT DIIDW:201774059R
ER

PT P
PN CN107291696-A
TI Deep learning based comment word emotion analyzing method, involves adding extension vector to word vector sequence, where word vector sequence is added with expanded word vector that inputs to emotion judging model.
AU JI Z
   CHEN Y
   JI D
   GUI H
   JIANG Y
AE DAERGUAN INFORMATION TECHNOLOGY SHANGHAI (DAER-Non-standard)
GA 2017751992
AB    NOVELTY - The method involves receiving commented text. Word sequence is obtained by carry outing word segmentation process on the commented text. The word sequence is transformed into corresponding word vector sequence by utilizing a word vector generating model. Judgment is made to check whether the word sequence provides emotion trend word based on field emotion dictionary. Expanded word vector is obtained based on the emotion trend word. The extension vector is added to the word vector sequence. The word vector sequence is added with the expanded word vector that inputs to an emotion judging model.
   USE - Deep learning based comment word emotion analyzing method.
   ADVANTAGE - The method enables expanding traditional word vector based on field emotion dictionary so as to enhance information intensity of field emotion word, so that emotion tendency in a specific field can be accurately identified, thus effectively improving accuracy of emotion tendency analysis.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning based comment word emotion analyzing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based comment word emotion analyzing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J11A1; T01-J30A; W04-W05A
IP G06F-017/27
PD CN107291696-A   24 Oct 2017   G06F-017/27   201777   Pages: 12   Chinese
AD CN107291696-A    CN10510221    28 Jun 2017
PI CN10510221    28 Jun 2017
CP CN107291696-A
      CN104794212-A   UNIV TSINGHUA (UYQI);  UNIV TSINGHUA WUXI RES INST APPLIED TECH (UYQI)   XU H
      CN105005553-A   UNIV SICHUAN (USCU)   ZHANG H, ZHANG Y
      CN105809186-A   BEIJING THINKIT TECHNOLOGY CO LTD (BEIJ-Non-standard);  INST ACOUSTICS CHINESE ACAD SCI (CAAI)   MA C, XU W, YAN Y, ZHAO X
      CN105930503-A   UNIV TSINGHUA (UYQI)   DENG J, SUN X, XU H, XU J
      CN105975594-A   UNIV TSINGHUA (UYQI)   DENG J, SUN X, XU H, XU J
      CN106599933-A   HARBIN INST TECHNOLOGY (HAIT)   XU B, YANG M, YANG Y, ZHAO T, ZHENG D, ZHU C, CAO H
      CN106776581-A   UNIV ZHEJIANG GONGSHANG (UYZG)   SHI H, LI X, CHEN N
UT DIIDW:2017751992
ER

PT P
PN CN107292297-A
TI Deep learning and overlapping rate tracking based video vehicle detection method, involves judging whether boundary frame satisfies ending condition, and performing vehicle detection process if boundary frame satisfies ending condition.
AU NIU X
   QI S
   WANG C
   LAI D
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 201775185S
AB    NOVELTY - The method involves detecting current frame image of a vehicle. Image is obtained in a vehicle frame. Judgment is made to check whether detected image is in the vehicle frame according to queue frame overlapping rate. The image is stored in the queue frame based on the vehicle frame. Judgment is made to check whether the vehicle frame is in a boundary frame. The boundary frame is deleted in the queue frame. Judgment is made to check whether the boundary frame satisfies ending condition. Vehicle detection process is performed if the boundary frame satisfies the ending condition.
   USE - Deep learning and overlapping rate tracking based video vehicle detection method.
   ADVANTAGE - The method enables reducing video vehicle detection problem in a real time manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning and overlapping rate tracking based video vehicle detection method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); T07 (Traffic Control Systems); W04 (Audio/Video Recording and Systems)
MC T01-J07D1; T01-J10B2; T01-J10B3A; T01-J10E; T01-J30A; T04-D03; T04-D04; T07-A01C; W04-W05A
IP G06K-009/00; G06K-009/46; G06K-009/62; G06N-003/04; G06N-003/08; G08G-001/065
PD CN107292297-A   24 Oct 2017   G06K-009/00   201777   Pages: 13   Chinese
AD CN107292297-A    CN10674811    09 Aug 2017
PI CN10674811    09 Aug 2017
CP CN107292297-A
      CN101303727-A   BEIJING ZHONGXING MICROELECTRONICS CO LT (BEIJ-Non-standard)   LU X, WANG L
      CN101477641-A   BEIJING VIMICRO CORP (BZMR)   HUANG Y
      JP09128514-A      
CR CN107292297-A
      &#21608;&#26631;: "&#20132;&#36890;&#30417;&#25511;&#35270;&#39057;&#20013;&#30340;&#36710;&#36742;&#26816;&#27979;&#25216;&#26415;&#30740;&#31350;", &#12298;&#20013;&#22269;&#20248;&#31168;&#30805;&#22763;&#23398;&#20301;&#35770;&#25991;&#20840;&#25991;&#25968;&#25454;&#24211; &#24037;&#31243;&#31185;&#25216;&#8545;&#36753;&#12299;,relevantClaims[1-10],relevantPassages[&#31532;1&#12289;12-59&#39029;]
      &#33931;&#33391;&#21355;: "&#22270;&#20687;&#24207;&#21015;&#20013;&#30446;&#26631;&#36319;&#36394;&#25216;&#26415;&#30740;&#31350;", &#12298;&#20013;&#22269;&#21338;&#22763;&#23398;&#20301;&#35770;&#25991;&#20840;&#25991;&#25968;&#25454;&#24211; &#20449;&#24687;&#31185;&#25216;&#36753;&#12299;,relevantClaims[6],relevantPassages[&#31532;26-27&#39029;]
UT DIIDW:201775185S
ER

PT P
PN CN107247702-A
TI Method for analyzing and processing text emotion of computing device, involves obtaining text word by word tool, and obtaining grammar information, semantic information, syntactic information and mood information by neural network.
AU HUANG W
   DU M
   SUN X
   WEI W
AE UNIV GUILIN ELECTRONIC TECHNOLOGY (UYGE-C)
GA 201771857Y
AB    NOVELTY - The method involves obtaining text word by a word tool for carrying out participle procession. Word vector training is performed by a word vector tool. Text word of binary file is obtained, where the binary file is included with the text word of the word vector. Emotion characteristic word is extracted from the binary file according to dependency syntax analysis. Syntactic feature information and emotional characteristic word are obtained. Grammar information, semantic information, syntactic information and mood information are obtained by a convolutional neural network.
   USE - Method for analyzing and processing text emotion of a computing device.
   ADVANTAGE - The method enables determining extracted features to synchronously improve result accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for analyzing and processing text emotion of a computing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for analyzing and processing text emotion of a computing device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J11A1; T01-J16C3; T01-N01B3; T01-N01D2
IP G06F-017/27; G06N-003/04
PD CN107247702-A   13 Oct 2017   G06F-017/27   201776   Pages: 15   Chinese
AD CN107247702-A    CN10313628    05 May 2017
PI CN10313628    05 May 2017
UT DIIDW:201771857Y
ER

PT P
PN CN107203782-A
TI Large dynamic signal-to-noise ratio lower communication interference signal identification method, involves obtaining output characteristic value, and classifying type of interference signal according to output value of machine classifier.
AU WU Z
   LUO H
   YIN Z
   YANG Z
   ZHOU S
AE HARBIN INST TECHNOLOGY (HAIT-C)
GA 2017677100
AB    NOVELTY - The method involves receiving an interference signal of signal-to-noise ratios. Interference samples is filtered in a convolutional neural network. A softmax layer of the interference samples are compared. A connection parameter of a weight offset parameter neuron unit is obtained. An interference signal is transmitted to the weight offset parameter neuron unit for obtaining the connection parameter. An output characteristic value of a connection layer is obtained. A type of the interference signal is classified according to an output value of a support vector machine classifier.
   USE - Convolutional neural network based large dynamic signal-to-noise ratio lower communication interference signal identification method.
   ADVANTAGE - The method enables solving interference signal type identification problem and characteristic extracting difficulty problem, improving the interference signal accuracy, reducing audio frequency interference, co-channel space narrowband interference, sweep frequency interference, rectangular pulse interference and spread spectrum interference of the interference signals, constructing the convolutional neural network model for improving robustness characteristics under large dynamic noise and the support vector machine classifier to classify the nterference signal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a large dynamic signal-to-noise ratio lower communication interference signal identification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-N01D1A; T04-D03A; T04-D04
IP G06K-009/62; G06N-003/08
PD CN107203782-A   26 Sep 2017   G06K-009/62   201777   Pages: 13   Chinese
AD CN107203782-A    CN10370299    23 May 2017
PI CN10370299    23 May 2017
UT DIIDW:2017677100
ER

PT P
PN CN107169556-A
TI Deep learning based automatic stem cell counting method, involves outputting output result by CNN for indicating whether cell is identified, and counting number of stem cells in candidate stem cell image to obtain dry cell counting result.
AU PU X
   WANG Z
   PANG J
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 201765650N
AB    NOVELTY - The method involves removing cell artifacts generated in phase contrast microscope shooting process. A stem cell image is segmented to obtain multiple candidate stem cell images after removing the cell artifacts. A training set is established according to the candidate stem cell images. A The training set is input to CNN for training the training set. An output result is output by the CNN for indicating whether a cell is identified. Number of stem cells in the candidate stem cell images is counted to obtain a dry cell counting result.
   USE - Deep learning based automatic stem cell counting method.
   ADVANTAGE - The method enables reducing manpower intensity and counting stem cells in an automatic, stable and efficient manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of stem cells. '(Drawing includes non-English language text)'
DC B04 (Natural products and polymers. Including testing of body fluids (other than blood typing or cell counting), pharmaceuticals or veterinary compounds of unknown structure, testing of microorganisms for pathogenicity, testing of chemicals for mutagenicity or human toxicity and fermentative production of DNA or RNA. General compositions.); T01 (Digital Computers); T05 (Counting, Checking, Vending, ATM and POS Systems)
MC B04-F02B; B11-C08J; B11-C11; B12-K04; T01-J10B2; T01-J30A; T05-A02
IP G06M-011/02; G06N-003/08; G06T-007/10
PD CN107169556-A   15 Sep 2017   G06M-011/02   201774   Pages: 12   Chinese
AD CN107169556-A    CN10339326    15 May 2017
PI CN10339326    15 May 2017
UT DIIDW:201765650N
ER

PT P
PN CN107169035-A
TI Mixed long term memory network and convolutional neural network text classification method, involves pre-processing sentence in text, and outputting result of full connection strata according to softmax layer of sub sentence.
AU SU J
   HUO Z
   OUYANG Z
AE UNIV SOUTH CHINA TECHNOLOGY (UYSC-C)
GA 201765672B
AB    NOVELTY - The method involves pre-processing sentence in text. Combined training corpus length distribution and average variance of a sentence are calculated. Sentence length of a length threshold value is calculated. Continuous and dense vector matrix is constructed. Input sentence sub word vector is calculated. Upper-text information and reverse LSTM network learning context information of multiple words are determined. A result of full connection strata is outputted according to a softmax layer of a sub sentence. Context information of each word is read.
   USE - Mixed long term memory network and convolutional neural network text classification method.
   ADVANTAGE - The method enables improving model classification accuracy, commonality and corpus testing efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a mixed long term memory network and convolutional neural network text classification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-E03; T01-J04C; T01-J05B2; T01-J05B4P; T01-N01B3A; T01-N01D
IP G06F-017/30; G06N-003/04; G06N-003/08
PD CN107169035-A   15 Sep 2017   G06F-017/30   201772   Pages: 8   Chinese
AD CN107169035-A    CN10257132    19 Apr 2017
PI CN10257132    19 Apr 2017
CP CN107169035-A
      CN104572892-A   CHINESE ACAD SCI AUTOMATION INST (CAZD)   LIU K, ZHAO J, XU L, LAI S
      CN104834747-A   CHINESE ACAD SCI AUTOMATION INST (CAZD)   HAO H, XU B, WANG F, WANG P
      CN106547735-A   UNIV FUDAN (UYFU)   ZHENG X, FENG J
      US20170032221-A1      
CR CN107169035-A
      MINLIE HUANG: ""Modeling Rich Contexts for Sentiment Classification with LSTM"", ARXIV PREPRINT ARXIV:1605.01478,relevantClaims[1-9],relevantPassages[]
      : "", ,relevantClaims[1-9],relevantPassages[98-104]
UT DIIDW:201765672B
ER

PT P
PN CN107153887-A
TI Convolutional neural network based mobile user action prediction method, involves inputting behavior forecast model of target user data unit to neural network, and obtaining convolution prediction target user probability of action label.
AU HU J
   ZHU R
AE UNIV SOUTH CHINA TECHNOLOGY (UYSC-C)
GA 201764021T
AB    NOVELTY - The method involves selecting an action generating time of a mobile user history record from a present predicted mobile user historic action record. The mobile user historic action record is included in a two-dimensional target user data unit according to the time sequence, where the selected action generating time of the mobile user history record comprises a recording time point of an action label. A behavior forecast model of a two-dimensional target user data unit is input to a convolutional neural network. A convolution prediction target user probability of the action label is obtained.
   USE - Convolutional neural network based mobile user action prediction method.
   ADVANTAGE - The method enables effectively improving accuracy of mobile user behavior prediction, and reducing working quantity of mobile user behavior.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based mobile user action prediction method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W01 (Telephone and Data Transmission Systems)
MC T01-J04B2; T01-N01A2; W01-A06A3; W01-C01
IP G06N-003/08; G06Q-010/04; G06Q-030/02; G06Q-030/06; G06Q-050/00; H04M-001/725
PD CN107153887-A   12 Sep 2017   G06Q-010/04   201772   Pages: 9   Chinese
AD CN107153887-A    CN10243498    14 Apr 2017
PI CN10243498    14 Apr 2017
UT DIIDW:201764021T
ER

PT P
PN CN107123131-A
TI Deep learning based moving object detecting method, involves judging whether suspicious movement target is in frame image, obtaining suspicious target position, if yes, and considering collected camera scene as background image if no.
AU ZHANG K
   HE J
   NI X
AE ANHUI TSINGLINK INFORMATION TECHNOLOGY (ANHU-Non-standard)
GA 201761435S
AB    NOVELTY - The method involves obtaining depth neural network model (S1). Motion target with resolution capability of a target neural network model is obtained. Target neural network model is obtained (S2). An initial background image is determined under current camera scene. A real-time frame image is acquired. Judgment is made to check whether suspicious movement target is in frame image. Suspicious movement target position is obtained, if yes. Candidate movement target is determined. Collected camera scene is considered as a background image if no suspicious moving target.
   USE - Deep learning based moving object detecting method.
   ADVANTAGE - The method enables reducing memory and detecting moving target in an accurate manner and reducing calculation cost.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based moving object detecting method. '(Drawing includes non-English language text)'
   Step for obtaining depth neural network model (S1)
   Step for obtaining target neural network model (S2)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J10B2; T01-N01A; T01-N01B3; W04-W05A
IP G06N-003/04; G06T-007/246; G06T-007/292
PD CN107123131-A   01 Sep 2017   G06T-007/292   201769   Pages: 12   Chinese
AD CN107123131-A    CN10226749    10 Apr 2017
PI CN10226749    10 Apr 2017
CP CN107123131-A
      CN101236656-A   SHANGHAI HUAPING INFORMATION TECHNOLOGY (SHAN-Non-standard)   GUO C, XIONG M
      CN103020577-A   PCI-SUNTEKTECH CO LTD (PCIS-Non-standard)   FENG Y, LIANG P, WANG G
      CN104166861-A   YE M (YEMM-Individual)   GOU Q, LI X, PENG M, WANG M, YE M
CR CN107123131-A
      : "", ,relevantClaims[1-6],relevantPassages[I138-1721]
UT DIIDW:201761435S
ER

PT P
PN CN107103758-A
TI Deep learning based urban area traffic flow predicting method, involves performing model training process for selecting smallest error model as optimal prediction model, and predicting traffic flow according to optimal prediction model.
AU FAN X
   ZHENG Z
   LI J
   WANG C
   CHEN L
   ZANG Y
   WEN C
AE UNIV XIAMEN (UYXI-C)
GA 201760719Y
AB    NOVELTY - The method involves dividing urban road into grid areas. Latitude and longitude of a LPR device are mapped in the grid areas. LPR data is obtained to determine a time space for recording and combining a mapping relationship of the LPR device. Traffic flow data in the time space is obtained. A convolutional neural network prediction model is established by using a convolution length memory network. Predictive model training process is performed for selecting a smallest error model as an optimal prediction model. Urban area traffic flow is predicted according to the optimal prediction model.
   USE - Deep learning based urban area traffic flow predicting method.
   ADVANTAGE - The method enables improving urban area traffic flow predicting efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning based urban area traffic flow predicting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J04B2; T01-J21C; T01-N01B3; W04-W05A
IP G08G-001/01
PD CN107103758-A   29 Aug 2017   G08G-001/01   201768   Pages: 12   Chinese
AD CN107103758-A    CN10427408    08 Jun 2017
PI CN10427408    08 Jun 2017
CP CN107103758-A
      CN101111741-A   XANAVI INFORMATICS CORP (XANV)   ENDO Y, AMAYA S
      CN101154317-A   XANAVI INFORMATICS CORP (XANV)   YAMANE K, TAKAHASHI H, OKUDE M
      CN105160866-A   ZHEJIANG HIGH-SPEED INFORMATION ENG TECH (ZHEJ-Non-standard)   HUANG B, FANG J
      CN105389980-A   UNIV SHANGHAI JIAOTONG (USJT)   PAN L, TIAN Y
      CN106251625-A   UNIV SHANGHAI JIAOTONG (USJT)   YUAN C, LI D, XI Y
      KR1742042-B1   KOREA INST SCI & TECHNOLOGY INFORMATION (KOAD)   YI H S, JUNG H J
      KR1742043-B1   KOREA INST SCI & TECHNOLOGY INFORMATION (KOAD)   YI H S, JUNG H J
CR CN107103758-A
      YONGXUE TIAN, etc.: "Predicting Short-term Traffic Flow by Long Short-Term Memory Recurrent Neural Network", "COMPUTER SOCIETY",relevantClaims[1-6],relevantPassages[153-158]
      Yin Shaolong et al.: "Practical research in the prediction of urban traffic flow", Modern Electronic Technology,relevantClaims[1-6],relevantPassages[158-162]
UT DIIDW:201760719Y
ER

PT P
PN CN107092926-A
TI Deep learning based service robot object recognition algorithm, has set of instructions for determining type of target object according to image captured in camera of service robot, and completing object identification of robot.
AU ZHU Q
   ZHANG S
   ZHANG Z
   YAO Y
AE UNIV HARBIN ENGINEERING (UHEG-C)
GA 2017597147
AB    NOVELTY - The algorithm has a set of instructions for collecting an image of a service robot. A training set and a validation set of an image data set are created. An object recognition model is obtained under basis of a deep learning framework according to a designed convolutional neural network structure. An object recognition of a complex indoor environment is identified by using the tested object recognition model. A type of a target object is determined according to the image captured in a camera of the service robot. An object identification of the service robot is completed.
   USE - Deep learning based service robot object recognition algorithm.
   ADVANTAGE - The algorithm enables realizing function of the service robot object identification under complex indoor environment based on the deep learning process, achieving better real-time performance and improving accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a deep learning based service robot object recognition algorithm. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2A; T01-J10B3; T01-N01B3
IP G06K-009/62
PD CN107092926-A   25 Aug 2017   G06K-009/62   201768   Pages: 8   Chinese
AD CN107092926-A    CN10202158    30 Mar 2017
PI CN10202158    30 Mar 2017
UT DIIDW:2017597147
ER

PT P
PN CN107044976-A
TI Stack type RBM and LIBS soil heavy metal content analytical prediction method based on deep learning technology, involves forming neural network layer, and carrying out test sample of soil metal content analysis and prediction operation.
AU CHEN T
   WANG R
   XIE C
   ZHANG J
   LI R
   CHEN H
AE CHINESE ACAD SCI HEFEI INST PHYS SCI (CAWZ-C)
GA 201756924R
AB    NOVELTY - The method involves obtaining gaining and pre-processing soil sample. The soil sample is divided into training sample and testing sample. A spectrometer is utilized for obtaining the training sample and testing sample of laser. Breakdown spectroscopy data is obtained. Restricted Boltzmann machine is constructed based on stacked deep learning technology. A training sample prediction model is established. A neural network layer is formed on the restricted Boltzmann machine. A test sample of soil heavy metal content analysis and prediction operation is carried out.
   USE - Stack type RBM and LIBS soil heavy metal content analytical prediction method based on deep learning technology.
   ADVANTAGE - The method enables increasing soil heavy metal content detection speed.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a stack type RBM and LIBS soil heavy metal content analytical prediction method based on deep learning technology. '(Drawing includes non-English language text)'
DC S03 (Scientific Instrumentation); T01 (Digital Computers)
MC S03-E04A; S03-E04D; T01-J05B1; T01-N01A2; T01-N01B3A; T01-N01D2; T01-N02B1B; T01-N02B2C
IP G01N-021/71; G06N-003/08; G06Q-010/04; G06Q-050/02
PD CN107044976-A   15 Aug 2017   G01N-021/71   201766   Pages: 9   Chinese
AD CN107044976-A    CN10325155    10 May 2017
PI CN10325155    10 May 2017
CP CN107044976-A
      CN103729678-A   CHINESE ACAD SCI INFORMATION ENG INST (CAIE)   HUANG C, LI Q, NIU W, LIU P, SUN W, GUO L, HU Y, GUAN Y
      CN103823845-A   UNIV ZHEJIANG (UYZH)   YANG J, JIANG L, ZHENG G, CHEN H, WU C, YAO J, HUANG M
      CN104535544-A   CHINESE ACAD SCI HEFEI INST PHYS SCI (CAWZ)   FANG L, LIU J, LIU W, MA M, WANG Y, YU Y, ZHAO N, XIAO X, MENG D
      CN104794534-A   LINYI ELECTRIC POWER SUPPLY CO STATE GRI (SGCC)   JI S, LU Z, SUN H, WU X, ZHANG Y
      CN105844333-A   UNIV XIAMEN (UYXI)   ZENG N, ZHANG H, ZHU P, YOU Y
      CN105911003-A   UNIV CHONGQING (UYCQ)   HUANG H, HUANG Y, SHI G, JIN Y
      CN106124449-A   CHINESE ACAD SCI HEFEI INST PHYS SCI (CAWZ)   WANG R, CHEN T, XIE C, ZHANG J, LI R, CHEN H, SONG L, WANG Y
      CN106198909-A   UNIV CENT SOUTH (UYCS)   CHEN B, GAO Y, WANG B, LIU L
      CN106326899-A   UNIV ZHENGZHOU (UYZH-Non-standard)   SHEN J, PEI L, LIU R, MU X
      CN202421069-U   CHINESE ACAD SCI HEFEI INST PHYSICAL SCI (CAWZ)   CHEN P, LU C, SONG L, WANG Y, WANG R, ZHUANG Z, WANG L
CR CN107044976-A
      : "", ,relevantClaims[1-6],relevantPassages[2.12.4.35]
      : "CrBa", ,relevantClaims[1-6],relevantPassages[2134121351]
      : "", ,relevantClaims[4],relevantPassages[16421654]
UT DIIDW:201756924R
ER

PT P
PN CN107038429-A
TI Multi-task based deep learning cascade face alignment method, involves determining attribute label of key point, predicting face attribute of key point and human face, and outputting face attribute of key point and human face in real-time.
AU LIU Y
AE SICHUAN YUNTU WISESIGHT TECHNOLOGY CO LTD (SICH-Non-standard)
GA 2017557227
AB    NOVELTY - The method involves determining attribute label of a key point and a mark face. A human face image is obtained by using grey scaling and normalization process. A face alignment model is trained using a convolutional neural network. The convolution layer is arranged on a normalization layer, an active layer and a cell layer. The normalization layer is arranged on a connection layer after predicting a branch network by the connection layer. Face attribute of the key point and the human face are predicted. The face attribute of the key point and the human face are outputted in real-time.
   USE - Multi-task based deep learning cascade face alignment method.
   ADVANTAGE - The method enables realizing multi-task learning process of a dual-layer network so as to realize face key point location detection and improve face alignment of facial expression, posture, gender for shielding robustness.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-task based deep learning cascade face alignment method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-F02; T01-F03A; T01-J04B2; T01-J10B2; T01-N01B3; W04-W05A
IP G06K-009/00; G06N-003/08
PD CN107038429-A   11 Aug 2017   G06K-009/00   201766   Pages: 9   Chinese
AD CN107038429-A    CN10304638    03 May 2017
PI CN10304638    03 May 2017
UT DIIDW:2017557227
ER

PT P
PN CN106980683-A
TI Deep learning based blog text abstract generating method, involves determining depth learning encoder-order model, separating and combining parts after completing training, generating prediction abstract according to generated data.
AU YANG W
   ZHOU Y
   HUANG L
AE SUZHOU INST ADVANCED STUDY USTC (SUZH-Non-standard)
GA 201752809F
AB    NOVELTY - The method involves obtaining crawling blog data for selecting blog text data. Vector matrix data is determined according to a selected blog text data. A lemma vector is obtained from a dictionary according to vector matrix data. A depth learning encoder-order (coder-decoder) model is determined and the encoder and the decoder are separated from the depth learning encoder-order model. Separated parts are combined after completion of training. Data is generated after completing training for generating a prediction abstract based on a model.
   USE - Deep learning based blog text abstract generating method.
   ADVANTAGE - The method enables generating text, displaying the main content of the blog, and improves wide application prospect.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning based blog text abstract generating method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-E01B; T01-J05B4P; T01-J11A1; T01-J30A; W04-W05A
IP G06F-017/30; G06N-003/08
PD CN106980683-A   25 Jul 2017   G06F-017/30   201763   Pages: 14   Chinese
AD CN106980683-A    CN10204696    30 Mar 2017
PI CN10204696    30 Mar 2017
UT DIIDW:201752809F
ER

PT P
PN CN106910497-A
TI Method for predicting pronunciation of Chinese words, involves traning deep neural network to obtain pronunciation prediction model of Chinese words according to training set, so as to predict pronunciation of target word.
AU WANG Z
   LI X
   LI H
AE ALIBABA GROUP HOLDING LTD (ABAB-C)
GA 2017465176
AB    NOVELTY - The method involves obtaining (S201) a training set that comprises corresponding relation between each word and pronunciation, phonemes, and sequences. The each word contained in the words is represented by a word vector that reflects semantic representation of the word. A deep neural network is trained (S202) to obtain a pronunciation prediction model of Chinese words according to the training set, so as to predict the pronunciation of the target word. A word vector generation model maps the word into the word vector of a predetermined dimension.
   USE - Method for predicting pronunciation of Chinese words.
   ADVANTAGE - The each word can be obtained from the voice recognition pronunciation dictionary, and the prediction model can be prepared according to the generated Chinese words to predict the phonetic phoneme sequences corresponding to the new words and added to the current voice recognition pronunciation dictionary, which can improve the accuracy of speech recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a device for predicting pronunciation in Chinese words.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the method for predicting pronunciation of Chinese words. (Drawing includes non-English language text)
   Step for obtaining a training set that comprises corresponding relation between each word and pronunciation, phonemes, and sequences (S201)
   Step for training a deep neural network to obtain a pronunciation prediction model of Chinese words according to the training set, so as to predict the pronunciation of the target word (S202)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-C08A; T01-J11A1; T01-J16B; T01-J16C3; T01-N01B3; W04-V01; W04-V04A; W04-V05
IP G10L-015/16; G10L-025/48
PD CN106910497-A   30 Jun 2017   G10L-015/16   201752   Pages: 18   Chinese
AD CN106910497-A    CN10976061    22 Dec 2015
PI CN10976061    22 Dec 2015
UT DIIDW:2017465176
ER

PT P
PN CN106874923-A
TI Goods style classification determining method, involves obtaining commodity pictures of merchandises to train convolutional neural network, and determining goods style classification according to initial center clustering result.
AU FENG Z
   DAN K
AE ALIBABA GROUP HOLDING LTD (ABAB-C)
GA 2017444991
AB    NOVELTY - The method involves obtaining commodity pictures of merchandises to train a convolutional neural network. Characteristics of the commodity pictures are extracted. Characteristic vector clustering density is calculated. Characteristic vector is calculated according to the characteristic vector clustering density. Characteristic vector clustering initial amount is determined. An initial center clustering result is obtained under a clustering stable condition for vector clustering. Goods style classification is determined according to the initial center clustering result.
   USE - Goods style classification determining method.
   ADVANTAGE - The method enables automatically, quickly, accurately and reliably providing classification basis, improving goods style classification determining accuracy and efficiency and reducing workload of an operation staff.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a goods style classification determining device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a goods style classification determining method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-E03; T01-E04; T01-J04C; T01-J05B2; T01-J05B3; T01-J10B2; T01-N01A2C; T01-N01A2F; T01-N03A2; T04-D04
IP G06K-009/62
PD CN106874923-A   20 Jun 2017   G06K-009/62   201757   Pages: 29   Chinese
AD CN106874923-A    CN10922584    14 Dec 2015
PI CN10922584    14 Dec 2015
UT DIIDW:2017444991
ER

PT P
PN CN106845515-A
TI Robot target recognition and pose reconstruction virtual sample depth learning method, involves controlling robot end camera view and precise pose by correcting method, and calculating precise post by using profile features visual angle.
AU GU C
   ZHANG L
   WU K
   GUAN X
AE UNIV SHANGHAI JIAOTONG (USJT-C)
GA 201742342W
AB    NOVELTY - The method involves extracting a operating position of a target region in a camera image by using a convolutional neural network (CNN) area detector. The operating position of the target region is obtained. A robot end camera view and a precise pose obtaining optimal visual observation cent deviation are estimated by using a CNN pose classifier. The robot end camera view and the precise pose obtaining visual observation cent deviation controls by a correcting method. The precise post is calculated by using profile features best visual angle to realize the target position.
   USE - Robot target recognition and pose reconstruction virtual sample depth learning method.
   ADVANTAGE - The method enables reducing an outline matching visual feature caused by a large deviation, and improving an activity and object pose of a robot vision sensing reconstruction algorithm flexibility.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a robot target recognition and pose reconstruction virtual sample depth learning method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J07B; T01-J10B2A; T01-J10C4; T01-N01B3; T04-D04; T04-D07D5
IP G06K-009/62; G06T-017/00; G06T-007/33
PD CN106845515-A   13 Jun 2017   G06K-009/62   201755   Pages: 10   Chinese
AD CN106845515-A    CN11111441    06 Dec 2016
PI CN11111441    06 Dec 2016
UT DIIDW:201742342W
ER

PT P
PN CN106780499-A
TI Multi-mode brain tumor image segmenting method, involves obtaining lesion area classification information, determining number of divided image, and performing image edge opening and closed operation after performing smoothing process.
AU DING Y
   QIN Z
   DONG R
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 2017385681
AB    NOVELTY - The method involves determining a hidden node of a system architecture evolution (SAE) deep learning network. Initial network parameter is determined. Initial network parameter tuning process is performed to determine final network parameter. A size of an image block is determined. A grey level matrix is inputted to a classification network to obtain initial classification result. Lesion area classification information is obtained. Number of a divided image is determined. Image edge opening and closed operation is performed after performing smoothing process.
   USE - Automatic coding network stack based multi-mode brain tumor image segmenting method.
   ADVANTAGE - The method enables realizing abnormal brain tissue region classification process, and analyzing medical image by using computer aided design (CAD), image processing, mathematical modeling, artificial intelligence and other subject cross field so as to improve brain anatomical structure describing accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a multi-mode brain tumor image segmenting device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04E; T01-J05B2; T01-J10B2; T01-J15X; T01-N01B3; T01-N01E1
IP G06T-007/10
PD CN106780499-A   31 May 2017   G06T-007/10   201748   Pages: 6   Chinese
AD CN106780499-A    CN11115061    07 Dec 2016
PI CN11115061    07 Dec 2016
CP CN106780499-A
      CN104834943-A   UNIV ELECTRONIC SCI & TECHNOLOGY (UEST)   CHEN H, CHEN Y, HUANG R, LAN T, QIN Z, XU L, ZHANG C, XIAO Z, YANG X
      CN104851101-A   UNIV ELECTRONIC SCI & TECHNOLOGY (UEST)   CHEN H, CHEN Y, DING Y, HUANG R, LAN T, QIN Z, XU L, ZHANG C, XIAO Z, YANG X
      CN104866596-A   UNIV BEIJING POSTS & TELECOM (UBPT)   LI L, LI R, LIU Y, LU P, LU X, YUAN C, ZHOU Y
      CN105608698-A   UNIV NORTHWESTERN POLYTECHNICAL (UNWP)   LI Y, LIU T, XU L
      CN106127230-A   UNIV SHANGHAI MARITIME (USHM)   GUO Y, WANG X, ZHANG H
CR CN106780499-A
      ZHE XIAO : "A deep Learning-based Segmentation method for brain tumor in MR images", 2016 IEEE 6TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL ADAVANCES IN BIO AND MEDICAL SCIENCEICCABS,relevantClaims[1-3],relevantPassages[1-6]
      KIRAN VAIDHYA : "Multi-modal Brain Tumor Segmentation Using Stacked Denoising Autoencoders", BRAINLESION2015GLIOMAMULTIPLE SCLEROSISSTROKE AND TRAUMATIC BRAIN INJURIE,relevantClaims[1-3],relevantPassages[4-52-4]
UT DIIDW:2017385681
ER

PT P
PN CN106710599-A
TI Sound source detection method based on depth neural network, involves generating acoustic feature vector using DNN training model.
AU CAI G
AE SHENZHEN SAHALA DATA TECHNOLOGY CO LTD (SHEN-Non-standard)
GA 201736215F
AB    NOVELTY - The sound source detection (S10) method involves extracting the real-time sound signals of the acoustic feature. The acoustic feature vector is generated (S20) using the DNN training model for detecting and judging the acoustic feature vector. The DNN training model uses deep neural network method for training to establish the preset voice signal.
   USE - Sound source detection method based on depth neural network.
   ADVANTAGE - The higher accuracy of single frame detection and high real-time are achieved. The system has strong practicability.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a depth-based neural network specific sound detecting system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart of a specific sound source detection method that is based on a depth neural network.
   Step for detection method involves extracting the real-time sound signals of the acoustic feature (S10)
   Step for generating acoustic feature vector (S20)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-N01B3; W04-V04A
IP G10L-017/02; G10L-017/04; G10L-017/18
PD CN106710599-A   24 May 2017   G10L-017/02   201742   Pages: 14   Chinese
AD CN106710599-A    CN11099733    02 Dec 2016
PI CN11099733    02 Dec 2016
UT DIIDW:201736215F
ER

PT P
PN CN106709431-A
TI Iris identification method, involves obtaining user iris image, deleting mischosen region based on iris region candidate region prior, and performing position coordinating process to generate iris region output.
AU CHEN S
   ZHU S
AE XIAMEN ZHONGKONG BIOLOGICAL RECOGNITION (XIAM-Non-standard)
GA 201735167B
AB    NOVELTY - The method involves obtaining a user iris image. The iris image is located in an iris region by using a convolution neural network model. Character extracting and identifying operations are performed. Iris image scanning process is performed by using the convolution neural network model. An iris region candidate is generated. Iris region candidate area verification process is performed by using the convolution neural network model. A mischosen region is deleted based on iris region candidate region prior. Position coordinating process is performed to generate iris region output.
   USE - Iris identification method.
   ADVANTAGE - The method enables realizing deep learning and providing expressive iris image, so as to improve iris identification accuracy and reduce misjudgment rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an iris identification device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an iris identification method. '(Drawing includes non-English language text)'
DC S06 (Electrophotography and Photography); T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC S06-K07C1; S06-K07C6; S06-K99D; T01-J04B2; T01-J10B1; T01-J10B2; T01-J10B3A; T01-N01B3; W04-E20G
IP G06K-009/00
PD CN106709431-A   24 May 2017   G06K-009/00   201745   Pages: 17   Chinese
AD CN106709431-A    CN11099294    02 Dec 2016
PI CN11099294    02 Dec 2016
CP CN106709431-A
      CN101059836-A   UNIV SOUTH CHINA TECHNOLOGY (UYSC)   QIN H, GAO Y
      CN102306289-A   UNIV LANZHOU (ULAN)   MA Y, SHAO Y, XU G, ZHANG Z, ZHAO R
      CN102314589-A   BYD CO LTD (BYDB)   ZHANG K, WANG C, XU J, WANG Z
      CN102411709-A   UNIV HUNAN (UYHU)   SHENG X, LI S
      CN105701513-A   UNIV TSINGHUA SHENZHEN GRADUATE SCHOOL (UYQI);  SHENZHEN FUTURE MEDIA TECHNOLOGY INST (SHEN-Non-standard)   GUO Z, ZHANG L, BAO X
      CN105912990-A   SHENZHEN INST ADVANCED TECHNOLOGY (CAAT)   LI Z, QIAO Y, ZHANG K
      CN105981041-A   ZHOU E (ZHOU-Individual);  FAN H (FANH-Individual);  CAO Z (CAOZ-Individual);  JIANG Y (JIAN-Individual);  YIN Q (YINQ-Individual)   ZHOU E, FAN H, CAO Z, JIANG Y, YIN Q
UT DIIDW:201735167B
ER

PT P
PN CN106682574-A
TI Single-dimensional underwater depth convolutional network multiple target identification method, involves determining network structure parameter by extreme learning machine to identify class and obtain output characteristic of network.
AU WANG K
   LU A
   XING X
AE UNIV HARBIN ENGINEERING (UHEG-C)
GA 2017329612
AB    NOVELTY - The method involves selecting branch frame sound signal length in window function to intercept a signal. Input length transmission duration of a convolutional neural network is for about 170ms frames. Input convolutional network feature is extracted from a sound signal by using a one-dimensional depth convolutional network. Selected network parameter is adjusted by using a training sample set. A network structure parameter is determined by an extreme learning machine to identify a class and obtain output characteristic of the one-dimensional depth convolutional network.
   USE - Single-dimensional underwater depth convolutional network multiple target identification method.
   ADVANTAGE - The method enables realizing underwater depth convolutional network feature extraction in an automatic manner to reduce manual extracting features.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a single-dimensional underwater depth convolutional network multiple target identification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J10B2; T01-N01B3; T01-N01D1A; W04-W05A
IP G06K-009/00; G06N-003/04; G06N-099/00
PD CN106682574-A   17 May 2017   G06K-009/00   201742   Pages: 16   Chinese
AD CN106682574-A    CN11019290    18 Nov 2016
PI CN11019290    18 Nov 2016
UT DIIDW:2017329612
ER

PT P
PN CN106649853-A
TI Deep learning based short text clustering method, involves calculating semantic similarity between texts by using convolutional neural network, and performing short text clustering operation by adopting clustering algorithm.
AU DONG M
   MIAO X
   YANG H
AE RUN TECHNOLOGY CO LTD (RUNT-Non-standard)
GA 201734029P
AB    NOVELTY - The method involves calculating semantic similarity between texts by using a convolutional neural network (S101). Short text clustering process is performed (S102) by adopting a clustering algorithm. Training text is selected in the form of short text. Original order of a short text space is determined. Vector of a word is determined in a specific text space representation and mapping matrix. Multiple combinations of the text are finished. Text semantic information is obtained to determine a global feature matrix. Feature vector is input to a multi-layer sensing machine.
   USE - Deep learning based short text clustering method.
   ADVANTAGE - The method enables improving mass quantity short text clustering accuracy so as to quickly and accurately perform cluster analysis operation. The method enables calculating convolutional neural network computing text similarity so as to reduce pre-processing defects of the input short text data.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based short text clustering method. '(Drawing includes non-English language text)'
   Step for calculating semantic similarity between texts (S101)
   Step for performing short text clustering operation (S102)
DC T01 (Digital Computers)
MC T01-J04A; T01-J05B3; T01-J11A1; T01-J16C1; T01-J16C3; T01-N01B3; T01-N03A2
IP G06F-017/27; G06F-017/30; G06N-003/08
PD CN106649853-A   10 May 2017   G06F-017/30   201738   Pages: 12   Chinese
AD CN106649853-A    CN11260575    30 Dec 2016
PI CN11260575    30 Dec 2016
UT DIIDW:201734029P
ER

PT P
PN CN106658821-A; CN106658821-B
TI Non-vision photo biological effect based intelligent LED illumination system, has client interaction interface connected with illumination system main body that adjusts temperature color, and dimming drive circuit unit for driving LED lamp.
AU ZHOU X
   CHEN L
   LIU D
   CENG W
   ZHOU B
   ZENG W
AE UNIV SOUTH CHINA TECHNOLOGY (UYSC-C)
   UNIV SOUTH CHINA TECHNOLOGY (UYSC-C)
GA 201732366S
AB    NOVELTY - The system has a control unit connected with a dimming system server through a wireless fidelity (WIFI) unit. A desktop console is connected is connected with the control unit through a ZigBee wireless network. The dimming system server establishes a biological research dimming model. A client interaction interface is connected with an illumination system main body that adjusts temperature color. The control unit monitors and displays working surface illumination parameters. A dimming drive circuit unit drives the LED lamp.
   USE - Non-vision photo biological effect based intelligent LED illumination system.
   ADVANTAGE - The system realizes long-term tracking and analyzing process to obtain deep learning user dimming data so as to satisfy user requirements.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a non-vision photo biological effect based intelligent led illumination system controlling method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a non-vision photo biological effect based intelligent led illumination system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); U14 (Memories, Film and Hybrid Circuits); W01 (Telephone and Data Transmission Systems)
MC T01-C03C; T01-N01B3; T01-N02A3C; T01-N02B2; U14-J03A; W01-A06C4A; W01-A06C4E
IP H05B-033/08
PD CN106658821-A   10 May 2017   H05B-033/08   201736   Pages: 15   Chinese
   CN106658821-B   09 Apr 2019   H05B-033/08   201928      Chinese
AD CN106658821-A    CN10951304    26 Oct 2016
   CN106658821-B    CN10951304    26 Oct 2016
FD  CN106658821-B Previous Publ. Patent CN106658821
PI CN10951304    26 Oct 2016
CP CN106658821-A
      CN102231261-A   ZHONGHANG HUADONG OPTOELECTRONIC CO LTD (ZHON-Non-standard)   CHENM W, DONG D, LIU B, ZHANG X, ZHAO X, ZHU B
      CN104320877-A   UNIV SOUTH CHINA TECHNOLOGY (UYSC)   GAO Y, JIANG Z, HUANG Y, HUANG W, LONG X
      CN105491720-A   UNIV SOUTH CHINA TECHNOLOGY (UYSC)   LUO D, ZHANG H, ZHOU X
      CN204733413-U   UNIV NANJING INFORMATION SCI & TECHNOLOG (UNAI)   CHENG Y, SONG L, YANG L
      WO2012140152-A1   GERBEC A (GERB-Individual)   GERBEC A
   CN106658821-B
      CN102231261-A   ZHONGHANG HUADONG OPTOELECTRONIC CO LTD (ZHON-Non-standard)   CHENM W, DONG D, LIU B, ZHANG X, ZHAO X, ZHU B
      CN104320877-A   UNIV SOUTH CHINA TECHNOLOGY (UYSC)   GAO Y, JIANG Z, HUANG Y, HUANG W, LONG X
      CN105491720-A   UNIV SOUTH CHINA TECHNOLOGY (UYSC)   LUO D, ZHANG H, ZHOU X
      CN204733413-U   UNIV NANJING INFORMATION SCI & TECHNOLOG (UNAI)   CHENG Y, SONG L, YANG L
      WO2012140152-A1   GERBEC A (GERB-Individual)   GERBEC A
UT DIIDW:201732366S
ER

PT P
PN WO2017072250-A1; EP3367897-A1; US2019167143-A1
TI Automatic method for delineating or categorizing electrocardiogram (ECG), involves assigning score for wave selected among P-wave, QRS complex, and T-wave to each time point of cardiac signal.
AU LI J
   MASSIAS M
   RAPIN J
   POMIER R
   SCABELLONE C
   GAUDEFROY C
   BARRE B
   FONTANARAVA J
   GARDELLA C
   SORNAY M
   BORDIER T
AE CARDIOLOGS TECHNOLOGIES (CARD-Non-standard)
   CARDIOLOGS TECHNOLOGIES (CARD-Non-standard)
   CARDIOLOGS TECHNOLOGIES SAS (CARD-Non-standard)
GA 2017296688
AB    NOVELTY - The automatic method involves using a convolutional neural network to read each time point of a cardiac signal. Each time point of the cardiac signal is analyzed temporally. A score for a wave selected among the P-wave, QRS complex, and T-wave is then assigned to each time point of the cardiac signal.
   USE - Automatic method for delineating or categorizing ECG. Can be used in a convolutional neural network for computerizing delineation of a cardiac signal with multiple time points.
   ADVANTAGE - Allows the processing of cardiac signals of any duration, and analyzes and qualifies all types of waves in the same way in a single step without being constrained by their positions. Ensures that the input of the neural network may be one or multi-lead cardiac signals with variably length, possibly preprocessed to remove noise and baseline wandering due to patients movements, and expresses the signal at a chosen frequency. Ensures that the same network can be used to handle cardiac signals with different leads. Ensures improved reliability and leverages context information for the waves characterization due to unified steps of delineation. Ensures more reliable recovery and characterization of P waves, allowing to provide the conductivity pattern of the cardiac signal. Produces a multi-label classification helped by the identification of the cardiac signal waves.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a computer device with a software for implementing the automatic method;
   (2) a multi-label classification computerization method used for a cardiac signal; and
   (3) a delineation and multi-label classification computerization method used for a cardiac signal.
DC P31 (Diagnosis, surgery (A61B).); S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D01A1; T01-J05B2; T01-J16C1; T01-N01E
IP A61B-005/0452; A61B-005/0456; A61B-005/0468; A61B-005/046; A61B-005/0472; A61B-005/00
PD WO2017072250-A1   04 May 2017   A61B-005/0452   201733   Pages: 51   English
   EP3367897-A1   05 Sep 2018   A61B-005/0452   201860      English
   US2019167143-A1   06 Jun 2019   A61B-005/0468   201942      English
AD WO2017072250-A1    WOEP075972    27 Oct 2016
   EP3367897-A1    EP787481    27 Oct 2016
   US2019167143-A1    US267380    04 Feb 2019
FD  EP3367897-A1 PCT application Application WOEP075972
   EP3367897-A1 Based on Patent WO2017072250
   US2019167143-A1 Provisional Application US549994P
   US2019167143-A1 CIP of Application WOEP072912
   US2019167143-A1 CIP of Application US771807
   US2019167143-A1 CIP of Application WOEP075972
   US2019167143-A1 CIP of Application US924239
PI EP191769    27 Oct 2015
   US924239    27 Oct 2015
DS WO2017072250-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DJ; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IR; IS; JP; KE; KG; KN; KP; KR; KW; KZ; LA; LC; LK; LR; LS; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SA; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
      (Regional): BW; GH; GM; KE; LR; LS; MW; MZ; NA; RW; SD; SL; ST; SZ; TZ; UG; ZM; ZW; EA; AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; OA
EP3367897-A1: 
		      (Regional): AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LI; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; BA; ME; MA; MD
CP WO2017072250-A1
      CN104970789-A   CHINESE ACAD SCI SUZHOU NANO-TECH & NANO (CASZ)   DONG J, JIN L
      US20130237776-A1      
      US20140148714-A1      
      US8332017-B2   OXFORD BIOSIGNALS LTD (OXFO-Non-standard)   TARASSENKO L, PATTERSON A, STRACHAN I G
      US8903479-B2   TEXAS INSTR INC (TEXI)   ZOICA V
CR WO2017072250-A1
      JIN LINPENG: "Electrocardiogram classification method and system, abstract of CN104970789", 14 October 2015 (2015-10-14), XP055264877, Retrieved from the Internet &lt;URL:epodoc&gt; [retrieved on 20160413],relevantClaims[1-29],relevantPassages[&lt;pp&gt;W&lt;/pp&gt;]
      KIRANYAZ S ET AL: "Convolutional Neural Networks for patient-specific ECG classification", 2015 37TH ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY (EMBC). PROCEEDINGS IEEE PISCATAWAY, NJ, USA, August 2015 (2015-08-01), pages 2608 - 2611, XP002756380, ISBN: 978-1-4244-9270-1,relevantClaims[1-29],relevantPassages[&lt;pp&gt;W&lt;/pp&gt;]
      SERKAN KIRANYAZ ET AL: "Real-Time Patient-Specific ECG Classification by 1-D Convolutional Neural Networks", IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING., vol. 63, no. 3, 14 August 2015 (2015-08-14), PISCATAWAY, NJ, USA., pages 664 - 675, XP055264879, ISSN: 0018-9294, DOI: 10.1109/TBME.2015.2468589,relevantClaims[1-29],relevantPassages[&lt;pp&gt;W&lt;/pp&gt;]
      MEGHRICHE ET AL.: "On The Analysis of a Compound Neural Network for Detecting Atrio Ventricular Heart Block (AVB) in an ECG Signal", INTERNATIONAL JOURNAL OF MEDICAL, HEALTH, BIOMEDICAL, BIOENGINEERING AND PHARMACEUTICAL ENGINEERING, vol. 2, no. 3, 2008, pages 68 - 78, XP002765772,relevantClaims[1-29],relevantPassages[pages 71,73: chapters B and C]
      MARTINEZ ET AL., IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, vol. 51, no. 4, April 2004 (2004-04-01), pages 570 - 581
      ALMEIDA ET AL., IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, vol. 56, no. 8, August 2009 (2009-08-01), pages 1996 - 2005
      BOICHAT ET AL., PROCEEDINGS OF WEARABLE AND IMPLANTABLE BODY SENSOR NETWORKS, 2009, pages 256 - 261
      COAST ET AL., IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, vol. 37, no. 9, September 1990 (1990-09-01), pages 826 - 836
      HUGHES ET AL., PROCEEDINGS OF NEURAL INFORMATION PROCESSING SYSTEMS, 2004, pages 611 - 618
      CHAZAL ET AL., IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, vol. 51, 2004, pages 1196 - 1206
      KIRANYAZ ET AL., IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, vol. 63, 2016, pages 664 - 675
      RUSSAKOVSKY ET AL., ARXIV:1409.0575V3, 30 January 2015 (2015-01-30)
      PRINEAS ET AL.: "Minnesota Code", 2009, SPRINGER
      "Statement of Validation and Accuracy for the Glasgow 12-Lead ECG Analysis Program", PHYSIO CONTROL, 2009
      SHEN ET AL., BIOMEDICAL ENGINEERING AND INFORMATICS (BMEI), vol. 3, 2010, pages 960 - 964
      JIN; DONG, SCIENCE CHINA PRESS, vol. 45, no. 3, 2015, pages 398 - 416
      BISHOP: "Pattern Recognition and Machine Learning", 2006, SPRINGER
      ROSENBLATT, PSYCHOLOGICAL REVIEW, vol. 65, no. 6, 1958, pages 386 - 408
      CYBENKO, MATH. CONTROL SIGNALS SYSTEMS, vol. 2, 1989, pages 303 - 314
      FUKUSHIMA, BIOL. CYBERNETICS, vol. 36, 1980, pages 193 - 202
      LECUN ET AL., NEURAL COMPUTATION, vol. 1, 1989, pages 541 - 551
      LONG ET AL., PROCEEDINGS OF COMPUTER VISION AND PATTERN RECOGNITION, 2015, pages 3431 - 3440
      KRIZHEVSK ET AL., PROCEEDINGS OF NEURAL INFORMATION PROCESSING SYSTEMS, 2012, pages 1097 - 1105
      DONAHUE ET AL., ARXIV:1411.4389V3, 17 February 2015 (2015-02-17)
      MNIH ET AL., ARXIV:1406.6247VL, 24 June 2014 (2014-06-24)
      PIGOLI; SANGALLI, COMPUTATIONAL STATISTICS AND DATA ANALYSIS, vol. 56, 2012, pages 1482 - 1498
      KAUR ET AL.: "Proceedings", 2011, INTERNATIONAL JOURNAL OF COMPUTER APPLICATIONS, pages: 30 - 36
      CHRISTOV ET AL., BIOMEDICAL ENGINEERING ONLINE, vol. 5, 2006, pages 31 - 38
      MOODY ET AL., COMPUTERS IN CARDIOLOGY, vol. 17, 1990, pages 185 - 188
UT DIIDW:2017296688
ER

PT P
PN CN106599520-A; CN106599520-B
TI Long short-term memory-recurrent neural network based air pollutants concentration forecasting method, involves modifying model parameters for establishing air pollutants concentration forecasting model, and obtaining forecasting result.
AU CHEN S
   CUI Y
   KANG Y
   LI Z
   WANG X
AE UNIV SCI & TECHNOLOGY CHINA (UCST-C)
   UNIV CHINA SCI & TECHNOLOGY (UCST-C)
GA 2017295856
AB    NOVELTY - The method involves collecting target city air pollutant concentration data. The pollutant concentration data is stored in a database. History data is obtained for establishing long short-term memory-recurrent neural network (LSTM-RNN) model. Training sample data of the LSTM-RNN model is obtained. The sample data is verified by a testing sample data training structure to obtain LSTM-RNN model parameters. The model parameters are modified for establishing an air pollutants concentration forecasting model. An air pollutant concentration forecasting result is obtained in a certain time period.
   USE - LSTM-RNN based air pollutants concentration forecasting method.
   ADVANTAGE - The method enables improving model accuracy by modifying the model parameters.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a long short-term memory-recurrent neural network based air pollutants concentration forecasting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B2B; T01-J05B4P; T01-J15B; T01-J15X; T01-J16C1; T01-N01A2; T01-N01B3A
IP G06F-017/50; G06Q-010/04
PD CN106599520-A   26 Apr 2017   G06F-017/50   201733   Pages: 13   Chinese
   CN106599520-B   03 Aug 2018   G06F-017/50   201856      Chinese
AD CN106599520-A    CN11267916    31 Dec 2016
   CN106599520-B    CN11267916    31 Dec 2016
FD  CN106599520-B Previous Publ. Patent CN106599520
PI CN11267916    31 Dec 2016
CP CN106599520-A
      CN103514366-A   UNIV CENT SOUTH (UYCS)   ZHENG Z, ZOU B
      CN104200103-A   ZHEJIANG HONGCHENG COMPUTER SYSTEM CO LT (ZHEJ-Non-standard)   ZHAO J, CHEN L, LI F, WANG J, LU D, YUAN C
      CN105809249-A   UNIV ZHEJIANG TECHNOLOGY (UYZT)   FU M, WANG X, WANG C
      CN106056210-A   UNIV ZHEJIANG TECHNOLOGY (UYZT)   FU M, WANG C, WANG X
CR CN106599520-A
      &#26472;&#35757;&#25919;&#31561;: "&#22522;&#20110;LSTM-RNN&#27169;&#22411;&#30340;&#31354;&#27668;&#27745;&#26579;&#29289;&#27987;&#24230;&#39044;&#25253;&#26041;&#27861;", &#12298;&#30005;&#27668;&#33258;&#21160;&#21270;&#12299;,relevantClaims[1-8],relevantPassages[&#31532;0&#12289;1&#12289;2&#12289;3.1&#12289;3.2&#33410;]
UT DIIDW:2017295856
ER

PT P
PN CN106600571-A; WO2018082084-A1; US2019026897-A1
TI Convolution neural networks and conditional random fields combination based automatic brain tumor image segmentation method, involves performing brain tumor segmentation operation and obtaining brain tumor segmentation result.
AU WU Y
   ZHAO X
AE CHINESE ACAD SCI AUTOMATION INST (CAZD-C)
   CHINESE ACAD SCI AUTOMATION INST (CAZD-C)
GA 201728096W
AB    NOVELTY - The method involves performing luminance regularization operation based on uneven field offset. A first magnetic resonance image of brain tumor is generated by utilizing N4ITK algorithm. Convolutional neural network fusion operation is performed. Brain tumor segmentation operation is performed to obtain a second magnetic resonance image. Brain tumor segmentation result is obtained. Magnetic resonance image linear brightness adjusting operation is performed. Luminance histogram of the first magnetic resonance image is obtained. Feature map is generated based on convolutional neural network.
   USE - Convolution neural networks and conditional random fields combination based automatic brain tumor image segmentation method.
   ADVANTAGE - The method enables improving operation efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a combined convolution neural networks and conditional random fields based brain tumor automatic segmentation. '(Drawing includes non-English language text)'
EA    (WO2018082084-A1)   NOVELTY - The method involves processing a first magnetic resonance image comprising a brain tumor image (Step 1) by using a non-uniform offset correction and luminance regularization process to generate a second magnetic resonance image. Brain tumor segmentation on the second magnetic resonance image is performed (Step 2) by using a neural network fusing a full convolutional neural network and a conditional random field. The brain tumor segmentation results are output (Step 3). Magnetic resonance image is generated.
   USE - Method for facilitating automatic segmentation of brain tumor for children.
   ADVANTAGE - The method enables performing brain tumor segmentation in an end-to-end and slice-to-slice manner, thus achieving higher operational efficiency for brain tumor segmentation.
DC T01 (Digital Computers); S03 (Scientific Instrumentation)
MC T01-J04B2; T01-J10B1; T01-J10B2; T01-J16C1; S03-E07A
IP G06T-005/40; G06T-007/11; G06T-007/136; G06T-011/00
PD CN106600571-A   26 Apr 2017   G06T-005/40   201732   Pages: 14   Chinese
   WO2018082084-A1   11 May 2018   G06T-007/11   201832   Pages: 27   Chinese
   US2019026897-A1   24 Jan 2019   G06T-007/11   201908      English
AD CN106600571-A    CN10972506    07 Nov 2016
   WO2018082084-A1    WOCN104849    07 Nov 2016
   WO2018082084-A1    WOCN104849    07 Nov 2016
   US2019026897-A1    US16070882    18 Jul 2018
   US2019026897-A1    US16070882    18 Jul 2018
FD  US2019026897-A1 PCT application Application WOCN104849
PI CN10972506    07 Nov 2016
   WOCN104849    07 Nov 2016
   US16070882    18 Jul 2018
DS WO2018082084-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DJ; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IR; IS; JP; KE; KG; KN; KP; KR; KW; KZ; LA; LC; LK; LR; LS; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SA; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
      (Regional): AL; AT; BE; BG; BW; CH; CY; CZ; DE; DK; EA; EE; ES; FI; FR; GB; GH; GM; GR; HR; HU; IE; IS; IT; KE; LR; LS; LT; LU; LV; MC; MK; MT; MW; MZ; NA; NL; NO; OA; PL; PT; RO; RS; RW; SD; SE; SI; SK; SL; SM; ST; SZ; TR; TZ; UG; ZM; ZW
CP    WO2018082084-A1
      CN105979244-A   12 DIMENSION BEIJING TECHNOLOGY CO LTD (TWEL-Non-standard)   QU Y, ZHANG Y, ZHAO T
      CN106600571-A   CHINESE ACAD SCI AUTOMATION INST (CAZD)   WU Y, ZHAO X
CR    WO2018082084-A1
      KAMNITSAS, K ET AL.: "Efficient Multi-scale 3D CNN with Fully Connected CRF for Accurate Brain Lesion Segmentation", MEDICAL IMAGE ANALYSIS, vol. 36, 29 October 2016 (2016-10-29), pages 61 - 78, XP029882414,relevantClaims[1-10]
      TUSTISON, N.J. ET AL.: "N4ITK: Improved N3 Bias Correction", IEEE TRANSACTIONS ON MEDICAL IMAGING, vol. 29, no. 6, 30 June 2010 (2010-06-30), pages 1310 - 1320, XP011307119,relevantClaims[1-10]
      ZHENG, SHUAI ET AL.: "Conditional Random Fields as Recurrent Neural Networks", IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, 18 February 2016 (2016-02-18), pages 1532 - 1536, XP032866501, ISSN: 2380-7504,relevantClaims[1-10]
      ZHOU, HAO ET AL.: "Image Semantic Segmentation Based on FCN-CRF Model", INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING, 22 September 2016 (2016-09-22), pages 9 - 14, XP032968073,relevantClaims[1-10]
UT DIIDW:201728096W
ER

PT P
PN CN106529569-A; US2018101752-A1; US10049299-B2
TI Deep learning based three-dimensional model simplification feature learning classification method, involves constructing convolutional neural network learning model, and obtaining model corresponding to output characteristic of class label.
AU CHEN X
   GUO K
   ZOU D
   ZHAO Q
AE UNIV BEIHANG (UNBA-C)
   UNIV BEIHANG (UNBA-C)
   UNIV BEIHANG (UNBA-C)
GA 201721498T
AB    NOVELTY - The method involves constructing a deep convolutional neural network learning model. The deep convolutional neural network learning model is provided with a first convolutional layer, a first down-sampling layer, a second convolutional layer and a second down-sampling layer. Initial characteristic of a triangular three-dimensional model is determined according to depth of a convolutional neural network. A category label is determined corresponding to output characteristic. The triangular three-dimensional model is obtained corresponding to the output characteristic of a class label.
   USE - Deep learning based three-dimensional model simplification feature learning classification method.
   ADVANTAGE - The method enables improving capability of the triangular three-dimensional model and ensuring accuracy of a result and determining triangular three-dimensional model feature of learning and classification process.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning based three-dimensional model simplification feature learning classification device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based three-dimensional model simplification feature learning classification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J10B2; T01-J16C1; T01-N01B3; T04-D04; T04-D07B1; T04-D07D3
IP G06K-009/62; G06N-003/08; G06T-019/00; G06N-003/04; G06K-009/00
PD CN106529569-A   22 Mar 2017   G06K-009/62   201725   Pages: 20   Chinese
   US2018101752-A1   12 Apr 2018   G06K-009/62   201825      English
   US10049299-B2   14 Aug 2018   G06K-009/00   201854      English
AD CN106529569-A    CN10889177    11 Oct 2016
   US2018101752-A1    US439896    22 Feb 2017
   US10049299-B2    US439896    22 Feb 2017
FD  US10049299-B2 Previous Publ. Patent US2018101752
PI CN10889177    11 Oct 2016
CP CN106529569-A
      CN103268635-A   UNIV BEIJING JIAOTONG (UBJI)   WAN L
      CN103578136-A   UNIV SUZHOU ZHANGJIAGANG IND TECHNOLOGY (USWZ)   CHEN G, LIN R, SUN R, WANG Z, LI M
      CN104680508-A   HUAWEI TECHNOLOGIES CO LTD (HUAW)   OUYANG W, XU C, LIU J, WANG X
      CN104966104-A   SUN J (SUNJ-Individual)   LI J, SUN J, ZHAO D
      US20180101752-A1      
   US2018101752-A1
      US20050228591-A1      
      US20100172567-A1      
      US20120136860-A1      
      US20160098619-A1      
      US9424492-B2   XEROX CORP (XERO)   MURRAY N, PERRONNIN F C
   US10049299-B2
      US20050228591-A1      
      US20100172567-A1      
      US20120136860-A1      
      US20160098619-A1      
      US9424492-B2   XEROX CORP (XERO)   MURRAY N, PERRONNIN F C
CR CN106529569-A
      KAN GUO: "Geometric Based Structure Propagation and Texture Matching for 3D Texture Completion", 2013 INTERNATIONAL CONFERENCE ON VIRTUAL REALITY AND VISUALIZATION,relevantClaims[1-10],relevantPassages[87-93]
      : "-", ,relevantClaims[1-10],relevantPassages[2058-2064]
   US10049299-B2
      Zhenyu Shu et al., "Unsupervised 3D shape segmentation and co-segmentation via deep learning" Computer Aided Geometric Design, vol. 43, (Mar. 2016), pp. 39-52.
UT DIIDW:201721498T
ER

PT P
PN KR2017031985-A
TI Fault detection and diagnosis method for air conditioning system, involves drawing diagnosis result to diagnose whether malfunction is absent in diagnosis model, and providing machine learning algorithm with deep belief network.
AU HAN I S
   LEE B D
   LEE D K
   PARK D H
   SHIN J W
AE HYUNDAI ENG & CONSTR CO LTD (HYMR-C)
GA 201722472C
AB    NOVELTY - The method involves producing a machine learning diagnosis model to determine about malfunctions, and operating an air conditioning system. Data is measured by sensors and applied to the machine learning diagnosis model. A diagnosis result is drawn to diagnose whether the malfunction is absent in the machine learning diagnosis model. A machine learning algorithm is provided with a deep belief network. The machine learning diagnosis model is modified based on real time data. The diagnosis result is drawn from the machine learning diagnosis model.
   USE - Fault detection and diagnosis method for an air conditioning system.
   ADVANTAGE - The method enables providing the machine learning algorithm with the deep belief network, thus reducing gradual malfunction of heating ventilation and cooling system, and waste of energy and stability degradation of the system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a fault detection and diagnosis method for an air conditioning system. '(Drawing includes non-English language text)'
DC Q74 (Heating, ranges, ventilating (F24)); T01 (Digital Computers)
MC T01-J16C2; T01-N01B3
IP F24F-011/00; F24F-011/02
PD KR2017031985-A   22 Mar 2017   F24F-011/00   201726   Pages: 10   
AD KR2017031985-A    KR129632    14 Sep 2015
PI KR129632    14 Sep 2015
UT DIIDW:201722472C
ER

PT P
PN CN106528528-A
TI Text emotion analyzing method, involves containing emotion expressing target with original corpus sentence, and classifying input of three-dimensional convolutional neural network to obtain emotion classification result.
AU XU R
   CHEN T
   HUANG J
AE HARBIN INST TECHNOLOGY SHENZHEN GRADUATE (HAIT-C)
GA 201721522Y
AB    NOVELTY - The method involves obtaining original corpus sentence by using a condition random airport bidirectional LSTM neural network for analyzing and identifying of emotional expression target sentence in an original language material (S1). The emotional expression target sentence is identified (S2) by a judged emotion expressing target corresponding to multiple emotion expression. The emotion expressing target is contained with the original corpus sentence. Input of a three-dimensional convolutional neural network is classified (S3) to obtain an emotion classification result.
   USE - Text emotion analyzing method.
   ADVANTAGE - The method enables improving analysis efficiency and accuracy and text emotion classification tolerance so as to satisfy large scale corpus processing requirements.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a text emotion analyzing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a text emotion analyzing method. '(Drawing includes non-English language text)'
   Step for obtaining original corpus sentence by using condition random airport bidirectional LSTM neural network for analyzing and identifying of emotional expression target sentence in original language material (S1)
   Step for identifying emotional expression target sentence by judged emotion expressing target corresponding to multiple emotion expression (S2)
   Step for classifying input of three-dimensional convolutional neural network to obtain emotion classification result (S3)
DC T01 (Digital Computers)
MC T01-J05B2; T01-J05B4P; T01-J11A1; T01-J16C1
IP G06F-017/27; G06F-017/30
PD CN106528528-A   22 Mar 2017   G06F-017/27   201724   Pages: 16   Chinese
AD CN106528528-A    CN10907156    18 Oct 2016
PI CN10907156    18 Oct 2016
CP CN106528528-A
      CN101876974-A   NEC CORP (NIDE)   ZHAO K, HU C, QIU L
      CN102682124-A   UNIV SOOCHOW (USWZ)   LI S, ZHOU G, ZHANG H
      WO2016106383-A2   BOSCH GMBH ROBERT (BOSC);  DAS S (DASS-Individual);  VACA G (VACA-Individual);  SOUSA J P (SOUS-Individual)   DAS S, VACA G, SOUSA J P
CR CN106528528-A
      &#40644;&#31215;&#26472;: "&#8220;&#22522;&#20110;&#21452;&#21521;LSTMN&#31070;&#32463;&#32593;&#32476;&#30340;&#20013;&#25991;&#20998;&#35789;&#30740;&#31350;&#20998;&#26512;&#8221;", &#12298;&#20013;&#22269;&#20248;&#31168;&#30805;&#22763;&#23398;&#20301;&#35770;&#25991;&#20840;&#25991;&#25968;&#25454;&#24211; &#20449;&#24687;&#31185;&#25216;&#36753;&#12299;,relevantClaims[1-10],relevantPassages[&#35770;&#25991;&#31532;52&#39029;]
UT DIIDW:201721522Y
ER

PT P
PN US2017076196-A1; DE202016008302-U1; DE102016125838-A1; WO2017151203-A1; CN107145940-A; IN201847028080-A; KR2018096779-A; EP3398115-A1; JP2019512760-W
TI System for implementing long-short term memory layer with compressed gating function, has gates to generate intermediate gate output vector by multiplying gate input vector and gate parameter matrix which is toeplitz-like structured matrix.
AU SAINATH T N
   SINDHWANI V
   SENAT T N
   SYNDERVANY V
AE GOOGLE INC (GOOG-C)
   GOOGLE LLC (GOOG-C)
   GOOGLE LLC (GOOG-C)
   GOOGLE LLC (GOOG-C)
GA 201718565U
AB    NOVELTY - The system (100) has a recurrent neural network (110) receives a respective neural network input and generates a respective neural network output. The recurrent neural network comprises a first long short-term memory (LSTM) layer to generate a new layer state (124) and a new layer output (126) by applying the gates to a current layer input (122), state and output. The gates generates a respective intermediate gate output vector by multiplying a gate input vector and a gate parameter matrix. The gate parameter matrix for the gates is a toeplitz-like structured matrix.
   USE - System for implementing long-short term memory layers with compressed gating functions.
   ADVANTAGE - The recurrent neural network having a compressed LSTM layer can be effectively trained to achieve word error rates comparable to full size e.g. uncompressed recurrent neural networks. The training and processing of LSTM networks can be accelerated because they allow matrix-vector product and gradient computations to be performed faster. The displacement rank can be used to control the computational complexity, storage requirements and modeling capacity of for a compression scheme.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a non-transitory computer storage media encoded with a computer program product comprising instructions that when executed by the computers cause the computers to perform operations for implementing long-short term memory layer with compressed gating function.
   DESCRIPTION OF DRAWING(S) - The drawing shows an explanatory view of a neural network system.
   Neural network system (100)
   Recurrent neural network (110)
   Current layer input (122)
   New layer state (124)
   New layer output (126)
DC T01 (Digital Computers)
MC T01-F04; T01-J16C1; T01-N01B3; T01-S03
IP G06N-003/04; G06N-003/08; G06N-003/02; G06N-003/10; G10L-015/16
PD US2017076196-A1   16 Mar 2017   G06N-003/04   201721   Pages: 14   English
   DE202016008302-U1   24 Aug 2017   G06N-003/02   201757      German
   DE102016125838-A1   07 Sep 2017   G06N-003/02   201760      German
   WO2017151203-A1   08 Sep 2017   G06N-003/04   201760      English
   CN107145940-A   08 Sep 2017   G06N-003/08   201761      Chinese
   IN201847028080-A   03 Aug 2018   G06N-003/04   201853      English
   KR2018096779-A   29 Aug 2018   G06N-003/04   201860      
   EP3398115-A1   07 Nov 2018   G06N-003/04   201875      English
   JP2019512760-W   16 May 2019   G06N-003/04   201936   Pages: 21   Japanese
AD US2017076196-A1    US172457    03 Jun 2016
   DE202016008302-U1    DE20008302    29 Dec 2016
   DE102016125838-A1    DE10125838    29 Dec 2016
   WO2017151203-A1    WOUS065417    07 Dec 2016
   CN107145940-A    CN11226122    27 Dec 2016
   IN201847028080-A    IN47028080    26 Jul 2018
   KR2018096779-A    KR721542    07 Dec 2016
   EP3398115-A1    EP820415    07 Dec 2016
   JP2019512760-W    JP539123    07 Dec 2016
FD  US2017076196-A1 Provisional Application US172018P
   US2017076196-A1 Provisional Application US301734P
   IN201847028080-A PCT application Application WOUS065417
   IN201847028080-A Based on Patent WO2017151203
   KR2018096779-A PCT application Application WOUS065417
   KR2018096779-A Based on Patent WO2017151203
   EP3398115-A1 PCT application Application WOUS065417
   EP3398115-A1 Based on Patent WO2017151203
   JP2019512760-W PCT application Application WOUS065417
   JP2019512760-W Based on Patent WO2017151203
PI US172018P    05 Jun 2015
   US301734P    01 Mar 2016
   US172457    03 Jun 2016
   DE20008302    29 Dec 2016
DS WO2017151203-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DJ; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IR; IS; JP; KE; KG; KN; KP; KR; KW; KZ; LA; LC; LK; LR; LS; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SA; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
      (Regional): BW; GH; GM; KE; LR; LS; MW; MZ; NA; RW; SD; SL; ST; SZ; TZ; UG; ZM; ZW; EA; AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; OA
EP3398115-A1: 
		      (Regional): AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LI; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; BA; ME; MA; MD
CP    WO2017151203-A1
      US20150170020-A1      
      US20160035344-A1      
CR    WO2017151203-A1
      VIKAS SINDHWANI ET AL: "Structured Transforms for Small-Footprint Deep Learning", 6 October 2015 (2015-10-06), pages 1 - 9, XP055352899, Retrieved from the Internet &lt;URL:https://arxiv.org/pdf/1510.01722.pdf&gt; [retrieved on 20170308],relevantClaims[1-20],relevantPassages[&lt;pp&gt;1&lt;/pp&gt;&lt;ppl&gt;8&lt;/ppl&gt;]
      ALEX GRAVES: "Generating Sequences With Recurrent Neural Networks", ARXIV.ORG, 5 June 2014 (2014-06-05), pages 1 - 43, XP055313042, Retrieved from the Internet &lt;URL:https://arxiv.org/abs/1308.0850&gt; [retrieved on 20161021],relevantClaims[1-20],relevantPassages[&lt;pp&gt;1&lt;/pp&gt;&lt;ppl&gt;40&lt;/ppl&gt;]
      LEMMERLING P ET AL: "Fast algorithm for solving the Hankel/Toeplitz structured total least squares problem", NUMERICAL ALGORITHMS, BALTZER, AMSTERDAM, NL, vol. 23, no. 4, 1 January 2000 (2000-01-01), pages 371 - 392, XP002411864, ISSN: 1017-1398, DOI: 10.1023/A:1019116520737,relevantClaims[1-20],relevantPassages[&lt;pp&gt;W&lt;/pp&gt;]
      LU ZHIYUN ET AL: "Learning compact recurrent neural networks", 2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), IEEE, 20 March 2016 (2016-03-20), pages 5960 - 5964, XP032901747, DOI: 10.1109/ICASSP.2016.7472821,relevantClaims[1-20],relevantPassages[&lt;pp&gt;W&lt;/pp&gt;]
UT DIIDW:201718565U
ER

PT P
PN CN106504190-A
TI Three-dimensional convolutional neural network based three-dimensional video generating method, involves obtaining output video source by combining two eye video sources, and transmitting video source to display unit.
AU WANG X
   ZHU Y
   WANG H
AE UNIV ZHEJIANG GONGSHANG (UYZG-C)
GA 201719027C
AB    NOVELTY - The method involves obtaining training data from a three-dimensional (3D) animation cinema in a network. The training data is divided into a left eye view and a right eye view. Target training area configuring process is performed in a 3D convolutional neural network. A stereo video is generated in a left input part of a trained 3D convolutional neural network. An output video source is obtained by combining a right eye video source and a left eye video source. The output video source is transmitted to a display unit according to display characteristic.
   USE - 3D convolutional neural network based three-dimensional video generating method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a 3D convolutional neural network based three-dimensional video generating method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B3A; T01-J10C4; T01-J10C5; T01-J16C1; T01-N01B3; T01-N01D1B
IP G06N-003/04; G06N-003/08; G06T-003/00
PD CN106504190-A   15 Mar 2017   G06T-003/00   201723   Pages: 9   Chinese
AD CN106504190-A    CN11243656    29 Dec 2016
PI CN11243656    29 Dec 2016
CP CN106504190-A
      CN104616032-A   UNIV ZHEJIANG GONGSHANG (UYZG)   CHEN W, WANG H, WANG X, HE X
      CN105160310-A   UNIV XIDIAN (UYXN)   HAN H, JIAO L, LI Y, MA W, WANG S, WANG W, ZHANG D, YE X
      CN105160678-A   UNIV SHANDONG (USHA)   QU C, MA L, ZHANG H, ZHANG W
      CN105955708-A   XIAN BRISION INFORMATION TECHNOLOGY CO (XIAN-Non-standard)   LIU Z, WANG J, ZHANG S
      CN106097391-A   UNIV ZHEJIANG GONGSHANG (UYZG)   WANG H, YANG Y
      CN106157307-A   UNIV ZHEJIANG GONGSHANG (UYZG)   ZHU Y, WANG X, WANG H
      CN106203318-A   UNIV ZHEJIANG GONGSHANG (UYZG)   WANG X, WANG H, YAN G
      US8345984-B2   NEC LAB AMERICA INC (NIDE)   JI S, XU W, YANG M, YU K
      US8442927-B2   NEC LAB AMERICA INC (NIDE)   CHAKRADHAR S, SANKARADAS M, JAKKULA V S, CADAMBI S
CR CN106504190-A
      : "", ,relevantClaims[1-8],relevantPassages[2827-2830]
UT DIIDW:201719027C
ER

PT P
PN CN106485233-A
TI Method for detecting travel region of electronic device, involves obtaining road image information of driving environment, and determining running region in driving environment according to obstacle in environment boundary.
AU HUANG C
   LIANG J
   YU K
   YU Y
AE SHENZHEN HORIZON ROBOTICS TECHNOLOGY CO (SHEN-Non-standard)
GA 201719791Y
AB    NOVELTY - The method involves obtaining road image information of driving environment by an imaging device. Environment boundary of the driving environment is detected according to the road image information. Obstacle in the driving environment is detected. Running region in the driving environment is determined according to the obstacle in the environment boundary. Road boundary is determined according to a depth learning model. Training sample data is determined according to the road boundary. Position coordinate of the obstacle is determined according to calibration parameters.
   USE - Method for detecting travel region of an electronic device (claimed).
   ADVANTAGE - The method enables detecting a running environment driving area in a reliable manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for detecting travel region of an electronic device
   (2) a computer program product comprises a set of instructions for detecting travel region of electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for detecting travel region. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J21; T01-J30A; T01-S03; T04-D07D3; T04-D07D5
IP G06K-009/00
PD CN106485233-A   08 Mar 2017   G06K-009/00   201723   Pages: 15   Chinese
AD CN106485233-A    CN10921207    21 Oct 2016
PI CN10921207    21 Oct 2016
CP CN106485233-A
      CN102138769-A   SHENZHEN ADVANCED TECHNOLOGY RES INST (CAAT)   ZHANG J, HU Y, SONG Z, LIU H
      CN102865872-A   HITACHI LTD (HITA)   KAZAMA Y, KUJIRAI T, WATANABE T
      CN103679127-A   RICOH KK (RICO)   YOU G, LU Y, SHI Z, WANG G
      CN103914698-A   UNIV BEIJING SCI & TECHNOLOGY (UNBS)   LAN J, LIU M, YU D, AIBIBU R, CENG Y
      CN104228837-A   HYUNDAI MOTOR CO LTD (HYMR)   YOU B Y, OH Y C, CHOI T S, KWON S R
      CN104850834-A   CHINESE ACAD SCI HEFEI INST PHYS SCI (CAWZ)   CHEN T, LIANG H, WANG Z
      CN105740802-A   BEIJING ZHONGKE HUIYAN TECHNOLOGY CO LTD (BEIJ-Non-standard)   CUI F, CHEN X, MENG R, XIE Q, ZHU H, JIANG A
      CN105957145-A   BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BIDU)   ZHONG Z
CR CN106485233-A
      DAN LEVI : "StixelNet: A Deep Convolutional Network for Obstacle Detection and Road Segmentation", BRITISH MACHINE VISION CONFERENCE 2015,relevantClaims[3-9],relevantPassages[34]
      : "", ,relevantClaims[3-9|1-210-12],relevantPassages[81-8291]
      : "", ,relevantClaims[1-12],relevantPassages[360-365]
      : "Kinect", ,relevantClaims[1-12],relevantPassages[176-179]
      : "", ,relevantClaims[1-12],relevantPassages[43-47]
UT DIIDW:201719791Y
ER

PT P
PN CN106404388-A; CN106404388-B
TI Scraper conveyor chains fault diagnostic method, involves inputting sound signal character to classifier of SVM for classifying sound signal to diagnose scraping plate conveyor chains fault in bending section.
AU MA H
   DONG G
   NA Y
   ZHANG X
   MAO Q
   NIE Z
   NAN Y
AE UNIV XIAN SCI & TECHNOLOGY (UYXS-C)
GA 2017224178
AB    NOVELTY - The method involves performing voice signal recognition model dependent classification based on depth of a convolutional neural network (CNN) and a support vector machine (SVM) to diagnose whether chains failure occurs in a scraper conveyor bent section. Principal component analysis is performed for associated input sample. Sound signal character is extracted by the CNN. The sound signal character is inputted to a classifier of the SVM for classifying the sound signal to diagnose scraping plate conveyor chains fault in a bending section.
   USE - Scraper conveyor chains fault diagnostic method.
   ADVANTAGE - The method enables simplifying scraper conveyor chains fault diagnostic process and implementation process and improving experiment verification.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a scraper conveyor chains fault diagnostic method. '(Drawing includes non-English language text)'
DC S02 (Engineering Instrumentation); T01 (Digital Computers)
MC S02-J03; T01-J05B2; T01-J16C1
IP G01M-013/02
PD CN106404388-A   15 Feb 2017   G01M-013/02   201726   Pages: 11   Chinese
   CN106404388-B   19 Oct 2018   G01M-013/02   201872      Chinese
AD CN106404388-A    CN10821415    13 Sep 2016
   CN106404388-B    CN10821415    13 Sep 2016
FD  CN106404388-B Previous Publ. Patent CN106404388
PI CN10821415    13 Sep 2016
CP CN106404388-A
      CN103996056-A   UNIV ZHEJIANG TECHNOLOGY (UYZT)   ZHANG Y, XIAO G, GAO S, XIAO J
      CN104614069-A   UNIV SHANDONG (USHA)   TIAN L, ZHANG K, WANG B, WANG H
      CN105320965-A   UNIV NORTHWESTERN POLYTECHNICAL (UNWP)   LI Y, LIU T, ZHANG H
      CN105858126-A   UNIV CHINA MINING & TECHNOLOGY XUZHOU (UYMT)   CAO G, HUA C, JIANG F, LU H, LI W, PENG Y, ZHOU G, ZHEN Z, WU S
      CN204660729-U   NANTONG TIANLAN ENVIRONMENTAL PROTECTION (NANT-Non-standard)   CHEN Z, HE Z, LI J, ZHONG Y, DUAN G
CR CN106404388-A
      XIAO-XIAO NIU&#31561;: "A novel hybrid CNN&#8211;SVM classifier for recognizing handwritten digits", &#12298;PATTERN RECOGNITION&#12299;,relevantClaims[1],relevantPassages[&#31532;1318-1325&#39029;]
UT DIIDW:2017224178
ER

PT P
PN CN106408015-A
TI Convolutional neural network based intersection identification and depth estimation method, involves forming characteristic pattern, and scanning characteristic pattern to form vector, and combining detection window with output window.
AU ZHANG L
   XU C
   YE M
   LU C
   LIAO D
   ZHANG M
   WU H
   XU B
   CHEN X
   CHEN P
   TANG W
AE UESTC CHENGDU RES INST (CHEN-Non-standard)
GA 201713033C
AB    NOVELTY - The method involves collecting an outdoor road sample and an interworking sample of scene. The interworking sample is preprocessed to train a CNN junction detector. An image is detected and preprocessed. An image pyramid is constructed for the detected image. Features of the image pyramid are extracted using a feature extractor. A characteristic pattern of the image is formed by multiple convolution and down-sampling. The characteristic pattern is scanned to form a characteristic vector that is classified using a classifier. A detection window is combined with an output window.
   USE - Convolutional neural network based intersection identification and depth estimation method.
   ADVANTAGE - The method enables obtaining depth information of road junction using the classifier of a convolutional neural network so as to improve robustness and interference resistance.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a convolutional neural network. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J04C; T01-J10B2; T01-J16C1; T01-J21
IP G06K-009/46; G06K-009/62
PD CN106408015-A   15 Feb 2017   G06K-009/62   201716   Pages: 10   Chinese
AD CN106408015-A    CN10818250    13 Sep 2016
PI CN10818250    13 Sep 2016
UT DIIDW:201713033C
ER

PT P
PN CN106340309-A
TI Depth learning based dog called emotion recognizing method, involves designing storage layer and signal end point detecting layer, and performing emotion identifying process to obtain visualized result.
AU JU Y
   LIU J
   LIU M
AE NANJING DAKONGYI INFORMATION TECHNOLOGY CO LTD (NANJ-Non-standard)
GA 201708477T
AB    NOVELTY - The method involves designing a storage layer and a signal end point detecting layer. A forward depth neural network model of an attention module is established. A convolution neural network model and a linear integration model of the attention module are established. A dog called voice signal is stored in the storage layer. Voice endpoint detecting process is performed to obtain MFCC features. The forward depth neural network model and the convolution neural network model are integrated with the linear integration model. Emotion identifying process is performed to obtain a visualized result.
   USE - Depth learning based dog called emotion recognizing method.
   ADVANTAGE - The method enables realizing dog called emotion recognizing process in an efficient manner, monitoring whether dog barking emotion is changed or not and displaying emotion state through a remote.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a depth learning based dog called emotion recognizing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a depth learning based dog called emotion recognizing device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J04B2; T01-J16C1; T01-N01B3; T01-N02B2; W04-V01; W04-V04A3; W04-V04A4; W04-V05; W04-W05A
IP G10L-017/26; G10L-025/24; G10L-025/30; G10L-025/63
PD CN106340309-A   18 Jan 2017   G10L-025/63   201713   Pages: 8   Chinese
AD CN106340309-A    CN10709581    23 Aug 2016
PI CN10709581    23 Aug 2016
CP CN106340309-A
      CN104700829-A   UNIV SOUTH CENT NATIONALITIES (USOC)   CHEN S, LIU H, YANG C, SU J, HOU J
      US20050049877-A1      
      US20110082574-A1      
      US20150037778-A1      
CR CN106340309-A
      : "", ,relevantClaims[1-10],relevantPassages[1-6]
      CSABA MOLNAR,: "Classication of dog barks: A machine learning approach", ANIM COGN (2008),relevantClaims[1-10],relevantPassages[389-400]
UT DIIDW:201708477T
ER

PT P
PN CN106328121-A; CN106328121-B
TI Communication network based traditional Chinese musical instrument classification method, involves outputting label corresponding to instrument type compared with actual instrument type to improve instrument classification accuracy rate.
AU LI Y
   WANG F
   ZHU Y
   JI W
   ZHOU Z
   HONG H
   GU C
AE UNIV NANJING SCI & TECHNOLOGY (UNSC-C)
GA 201706366C
AB    NOVELTY - The method involves inputting primary characteristics of a deep belief network, and adding a corresponding label for an audio file of a Chinese traditional musical instrument. Primary feature of the China traditional musical audio file is obtained A layer of softmax regression is set with a trained confidence network model. An output result is provided in a tag of the musical instrument corresponding to the audio file. A predicted label is output corresponding to instrument type compared with actual instrument type to improve musical instrument classification accuracy rate.
   USE - Deep belief network based traditional Chinese musical instrument classification method.
   ADVANTAGE - The method enables simplifying instrument classification process, improving classification accuracy of a Chinese traditional musical instrument and providing effective information in a music information retrieval field.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a communication network based traditional Chinese musical instrument classification method. '(Drawing includes non-English language text)'
DC W04 (Audio/Video Recording and Systems)
MC W04-V01; W04-V05
IP G10L-015/02; G10L-015/06; G10L-015/08; G10L-025/12; G10L-025/24
PD CN106328121-A   11 Jan 2017   G10L-015/02   201712   Pages: 12   Chinese
   CN106328121-B   27 Jun 2017   G10L-015/02   201747      Chinese
AD CN106328121-A    CN10790284    30 Aug 2016
   CN106328121-B    CN10790284    30 Aug 2016
FD  CN106328121-B Previous Publ. Patent CN106328121
PI CN10790284    30 Aug 2016
CP CN106328121-A
      CN103325382-A   UNIV DALIAN NATIONALITIES (UYND)   LI M, WU B, ZHANG J, ZHENG R
      CN105809198-A   UNIV XIDIAN (UYXN)   HOU B, JIAO L, MA W, WANG S, ZHANG D, ZHANG Y, GUO Y, ZHAO F
      US20140032570-A1      
   CN106328121-B
      CN103325382-A   UNIV DALIAN NATIONALITIES (UYND)   LI M, WU B, ZHANG J, ZHENG R
      CN105809198-A   UNIV XIDIAN (UYXN)   HOU B, JIAO L, MA W, WANG S, ZHANG D, ZHANG Y, GUO Y, ZHAO F
      US20140032570-A1      
UT DIIDW:201706366C
ER

PT P
PN CN106295714-A
TI Deep learning based multi-source remote sensing image fusion method, involves performing test sample image pre-processing operation, testing convolutional neural network model, and obtaining feature type output result by using classifier.
AU CHEN K
   ZHOU Z
   XU G
   FU K
   ZHANG D
AE INST ELECTRONICS CHINESE ACAD SCI (CAEI-C)
GA 2017043713
AB    NOVELTY - The method involves performing (SA) test sample image pre-processing operation. An image spectral feature is extracted to obtain a training data set and a testing data set. A deep convolutional neural network model is constructed (SB). The convolutional neural network model is trained (SC). A convolutional neural network model is tested (SD) based on depth of the testing data set. A characteristic set is obtained to perform normalization process. A depth characteristic of the testing data set is obtained (SE). A feature type output result is obtained (SF) by using a classifier.
   USE - Deep learning based multi-source remote sensing image fusion method.
   ADVANTAGE - The method enables realizing multiple semantic abstract levels characteristic obtaining process, thus improving multi-source remote sensing image fusion precision.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based multi-source remote sensing image fusion method. '(Drawing includes non-English language text)'
   Step for performing test sample image pre-processing operation (SA)
   Step for constructing deep convolutional neural network model (SB)
   Step for training convolutional neural network model (SC)
   Step for testing convolutional neural network model (SD)
   Step for obtaining depth characteristic of testing data set (SE)
   Step for obtaining feature type output result by using classifier (SF)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J10B1; T01-J10B2; T01-J16C1; T01-J16C3; T01-N01B3A; W04-W05A
IP G06K-009/62
PD CN106295714-A   04 Jan 2017   G06K-009/62   201710   Pages: 13   Chinese
AD CN106295714-A    CN10703682    22 Aug 2016
PI CN10703682    22 Aug 2016
CP CN106295714-A
      CN103020945-A   CHINESE ACAD SCI ELECTRONICS INST (CAEI)   FU K, SUN X, SHI Y
      CN104851099-A   UNIV ZHOUKOU NORMAL (UYZH-Non-standard)   CHEN L, HUANG W, ZHANG S, CHEN S, CHEN Z
      CN105740894-A   UNIV BEIHANG (UNBA)   JIANG Z, SHI Z, ZHANG H, YANG Z
      JP2015207235-A   NEC CORP (NIDE)   OBA K
UT DIIDW:2017043713
ER

PT P
PN CN106295645-A
TI License plate character identifying method, involves obtaining deep learning network of license plate identifying model, obtaining character similarity information, and obtaining color information of license plate character image.
AU WEN W
   GU A
   XU J
   WAN D
AE NETPOSA TECHNOLOGIES LTD (NETP-Non-standard)
GA 201704372V
AB    NOVELTY - The method involves obtaining a color license plate character image. Color license plate character image processing operation is performed. A license plate identification model is established. Preset character information is determined by using the license plate identification model. A deep learning network of the license plate identifying model is obtained. Character similarity information is obtained. Color information of a license plate character image is obtained. A pixel value of an image channel is calculated. An image mean value is calculated.
   USE - License plate character identifying method.
   ADVANTAGE - The method enables improving license plate character recognition rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a license plate character identifying device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a license plate character identifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J10B2A; T01-J10B3B; T01-N01B3; W04-W05A
IP G06K-009/32; G06K-009/62
PD CN106295645-A   04 Jan 2017   G06K-009/32   201709   Pages: 18   Chinese
AD CN106295645-A    CN10680606    17 Aug 2016
PI CN10680606    17 Aug 2016
CP CN106295645-A
      CN101673338-A   NANJING SHUSHENG SCI & TECHNOLOGY CO LTD (NANJ-Non-standard)   CAI S, TAN J, WANG C, YU M
      CN101944174-A   UNIV XIDIAN (UYXN)   BAI J, ZHANG X, SHEN P
      CN103390156-A   SHENZHEN JIESHUN SCI & TECHN IND CO LTD (SHEN-Non-standard)   TANG J, TAO K, WU W, LIAO Z
      CN104636748-A   ZHANG W (ZHAN-Individual)   ZHANG W
      EP2595092-A2   XEROX CORP (XERO)   RODRIGUEZ SERRANO J A, BALA R, PERRONNIN F, SAUNDERS C J, ZHAO Y
UT DIIDW:201704372V
ER

PT P
PN US2016379109-A1; WO2017003887-A1; CN107836001-A; EP3314542-A1
TI Hardware acceleration component e.g. field-programmable gate arrays, for implementing convolutional neural network, has array of M weights data buffers for storing weights data, where M columns functional units provide M planes output data.
AU CHUNG E
   STRAUSS K
   OVTCHAROV K
   KIM J
   RUWASE O
AE MICROSOFT TECHNOLOGY LICENSING LLC (MICT-C)
GA 201700113L
AB    NOVELTY - The component has an array of N input data buffers for storing input data. An array of M weights data buffers for storing a weights data. A functional unit in a row of functional units receives a same set of input data from the input data buffer coupled to the row. A functional unit in a column of functional units receives a same set of weights data from the weights data buffer coupled to the row. Each of the functional units performs a convolution of the received input data and the received weights data. M columns of functional units provide M planes of output data.
   USE - Hardware acceleration component e.g. field-programmable gate arrays and application-specific integrated circuit, for implementing a convolutional neural network by software-driven computing devices.
   ADVANTAGE - The component maintains a resultant data processing system efficiently due to the component leverages existing physical links found in an existing data processing environment. The component provides data processing system comprising an efficient and flexible mechanism for allowing host components to access any acceleration resources provided by hardware acceleration plane without narrowly pairing host components to specific fixed acceleration resources, and without burdening the host components with managing hardware acceleration plane.
   DETAILED DESCRIPTION - The input data comprise image data. An INDEPENDENT CLAIM is also included for a method for implementing a multi-layer convolutional neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of an overview of a data processing system that includes a software plane and a hardware acceleration plane.
   Data processing system (102)
   Management functionality (122)
   Location determination component (124)
   Data store (126)
   Service mapping component (128)
DC T01 (Digital Computers)
MC T01-J04B2; T01-J16C1; T01-N02A3
IP G06N-003/04; G06N-003/063; G06F-015/76
PD US2016379109-A1   29 Dec 2016   G06N-003/063   201704   Pages: 72   English
   WO2017003887-A1   05 Jan 2017      201704      English
   CN107836001-A   23 Mar 2018   G06N-003/063   201822      Chinese
   EP3314542-A1   02 May 2018   G06N-003/04   201830      English
AD US2016379109-A1    US754367    29 Jun 2015
   WO2017003887-A1    WOUS039464    27 Jun 2016
   CN107836001-A    CN80039028    27 Jun 2016
   EP3314542-A1    EP735802    27 Jun 2016
FD  CN107836001-A PCT application Application WOUS039464
   CN107836001-A Based on Patent WO2017003887
   EP3314542-A1 PCT application Application WOUS039464
   EP3314542-A1 Based on Patent WO2017003887
PI US754367    29 Jun 2015
DS WO2017003887-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IR; IS; JP; KE; KG; KN; KP; KR; KZ; LA; LC; LK; LR; LS; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SA; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
      (Regional): BW; GH; GM; KE; LR; LS; MW; MZ; NA; RW; SD; SL; ST; SZ; TZ; UG; ZM; ZW; EA; AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; OA
EP3314542-A1: 
		      (Regional): AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LI; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; BA; ME; MA; MD
CP    WO2017003887-A1
      US20110029471-A1      
      US20120303932-A1      
CR    WO2017003887-A1
      Z. DU ET AL: "ShiDianNao: shifting vision processing closer to the sensor", PROCEEDINGS OF THE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA'15), 13 June 2015 (2015-06-13), pages 92 - 104, XP055301503, DOI: 10.1145/2749469.2750389,relevantClaims[1-15],relevantPassages[the whole document, in particular figures 3-5]
      C. ZHANG ET AL: "Optimizing FPGA-based accelerator design for deep convolutional neural networks", PROCEEDINGS OF THE 2015 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS (FPGA'15), 22 February 2015 (2015-02-22), pages 161 - 170, XP055265150, DOI: 10.1145/2684746.2689060,relevantClaims[1-15],relevantPassages[the whole document, in particular figure 6 and table 2]
      M. PEEMEN ET AL: "Memory-centric accelerator design for convolutional neural networks", PROCEEDINGS OF THE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD'13), 6 October 2013 (2013-10-06), pages 13 - 19, XP055195589, DOI: 10.1109/ICCD.2013.6657019,relevantClaims[1-15],relevantPassages[the whole document, in particular figures 2-5]
      K. OVTCHAROV ET AL: "Accelerating deep convolutional neural networks using specialized hardware in the datacenter", THE 4TH WORKSHOP ON THE INTERSECTIONS OF COMPUTER ARCHITECTURE AND RECONFIGURABLE LOGIC (CARL'15), 14 June 2015 (2015-06-14), XP055301504, Retrieved from the Internet &lt;URL:http://www.ece.cmu.edu/~calcm/carl/lib/exe/fetch.php?media=carl15_chung.pdf&gt; [retrieved on 20150618],relevantClaims[1-15],relevantPassages[&lt;pp&gt;W&lt;/pp&gt;]
UT DIIDW:201700113L
ER

PT P
PN CN106250832-A
TI Integrated convolutional neural network based national identifying method, involves determining base classifier outputs national category information according to convolutional neural network classifier.
AU WEN G
   QIU S
AE UNIV SOUTH CHINA TECHNOLOGY (UYSC-C)
GA 201700727J
AB    NOVELTY - The method involves establishing a convolutional neural network classifier. A face database is arranged with a national training set. The convolutional neural network classifier is formed as a base classifier. Base classifier outputs national category information is determined according to the convolutional neural network classifier. Video intercepting acquired picture of a camera is determined. Local facial tracking algorithm is analyzed according to the video intercepting acquired picture of the camera. Cam shift target face detection process is performed.
   USE - Integrated convolutional neural network based national identifying method.
   ADVANTAGE - The method enables performing integrated convolutional neural network based national identifying process with high identification accuracy and wide application range.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an integrated convolutional neural network based national identifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B4P; T01-J10B2; T01-J16C1; T01-N01B3; T04-D07F1
IP G06K-009/00; G06N-003/08
PD CN106250832-A   21 Dec 2016   G06K-009/00   201706   Pages: 21   Chinese
AD CN106250832-A    CN10590156    25 Jul 2016
PI CN10590156    25 Jul 2016
CP CN106250832-A
      CN102346829-A   UNIV CHONGQING (UYCQ)   TANG C, LI C, YAN M, SHI H, YANG L, ZHANG X
      CN102819745-A   UNIV HANGZHOU DIANZI (UYHH)   LIU J, ZUO Y, CHEN H, GUO B, XU Y, GU Y, GUO Y, PENG D
      CN102831413-A   SHANGHAI ZHONGYUAN ELECTRONICS TECHNOLOG (CSHI)   ZHU T, QIN H, LIU Q, YAO G
      CN105426875-A   UNIV WUHAN SCI & TECHNOLOGY (UWSC)   CHENG G, HE Y, WU H, ZHONG R, CHEN J
      CN105590102-A   CHINA COMMUNICATIONS SERVICES PUBLIC INF (CHCO-Non-standard)   CAI X, WANG A, SHU H, CHEN C
      CN105787437-A   UNIV SOUTHEAST (UYSE)   LIAN J, ZHAO C, QI H
CR CN106250832-A
      : "", ,relevantClaims[1-10],relevantPassages[I138-17764.2]
      : "", ,relevantClaims[1-10],relevantPassages[73-77]
UT DIIDW:201700727J
ER

PT P
PN CN106251375-A
TI Automatic depth learning universal steganalysis data coding method, involves adopting descent algorithm to calculate minimum parameter function value, and performing network training process according to optimization parameter value.
AU ZHAO H
   DAI Q
   WEI W
   REN J
   RONG C
AE UNIV GUANGDONG POLYTECHNIC NORMAL TIANHE (UGTH-C)
GA 2017007176
AB    NOVELTY - The method involves performing image partitioning process to obtain input image sample. An image signal is generated by performing Laplace transform and signal variance processing operations. A statistical relation is obtained according to pediatric rule. A threshold variance is calculated by performing image signal distribution and conditional probability distribution processes. A stochastic gradient descent algorithm is adopted to calculate a minimum parameter function value. USoSAE network training process is performed according to a final optimization parameter value.
   USE - Automatic depth learning universal steganalysis data coding method.
   ADVANTAGE - The method enables improving dimensional data summary representing efficiency and steganalysis data classification precision and reducing feature data dimension according to sparsity constraints prior conditions.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a USoSAE network. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J03; T01-J04A; T01-J05B2; T01-J10D; T01-N01B3; T01-N01D1B
IP G06T-009/00
PD CN106251375-A   21 Dec 2016   G06T-009/00   201706   Pages: 11   Chinese
AD CN106251375-A    CN10642550    03 Aug 2016
PI CN10642550    03 Aug 2016
CP CN106251375-A
      CN103116762-A   UNIV NANJING (UNAJ)   PAN L, YANG Y, TANG Y
      CN103955702-A   UNIV XIDIAN (UYXN)   HOU B, JIAO L, LI Y, LIU F, MA J, MA W, WANG S, YANG S, HAN J
      CN104166859-A   UNIV XIDIAN (UYXN)   HOU B, JIAO L, LI Y, LIU F, MA J, MA W, WANG S, LIU C
      CN104182771-A   UNIV BEIJING AERONAUTICS & ASTRONAUTICS (UNBA)   GUO L, QIAN C, WANG Y
      CN104346617-A   UNIV NANJING INFORMATION SCI & TECHNOLOG (UNAI)   XU J, XIANG L
      CN104819846-A   UNIV BEIJING AERONAUTICS & ASTRONAUTICS (UNBA)   DING Y, LI L, LV C, MA J, WANG Z, ZHAO W
CR CN106251375-A
      KC AU : "Learning Hidden Markov Model Topology Based on KL Divergence for Information Extraction", ADVANCES IN KNOWLEDGE DISCOVERY AND DATA MINING,relevantClaims[1],relevantPassages[590-594]
UT DIIDW:2017007176
ER

PT P
PN CN106228977-A
TI Deep learning-based multimode coupling song emotion recognizing method, involves training song voice information feature to obtain song multi-modality emotion classification and recognition model, and recognizing emotion category of song.
AU SUN X
   CHEN W
   REN F
AE UNIV HEFEI TECHNOLOGY (UYHE-C)
GA 2016801009
AB    NOVELTY - The method involves collecting a lyrics text database and an audio database of a song. Voice data in the audio database is extracted to obtain first voice information feature. A prosodic feature of the audio data is extracted. A depth learning process is applied to the first voice information feature. The first voice information feature and a second speech information feature are fused to obtain a song voice information feature. A song voice information feature is trained to obtain a song multi-modality emotion classification and recognition model. Emotion category of the song is recognized.
   USE - Deep learning-based multimode fusion song emotion recognizing method.
   ADVANTAGE - The method enables improving human-computer interaction in song, and determining emotional state in accurate manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning-based multimode coupling song emotion recognizing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J05B2; T01-J05B4P; T01-J30A; W04-V01; W04-V04A6; W04-W05A
IP G10L-015/02; G10L-015/06; G10L-015/18; G10L-015/26
PD CN106228977-A   14 Dec 2016   G10L-015/02   201705   Pages: 11   Chinese
AD CN106228977-A    CN10625990    02 Aug 2016
PI CN10625990    02 Aug 2016
CP CN106228977-A
      CN101201980-A   UNIV BEIJING COMMUNICATION (UBJI)   MIAO Z, MING Y, JI X
      CN101599271-A   UNIV HUAZHONG SCI&TECHNOLOGY (UYHZ)   CHEN G, GONG L, JIANG X, LIU F, LI X, WANG T, YU Y
      CN101963972-A   SHENZHEN HONGKONG PRODN & RES BASE (SHEN-Non-standard);  SHENZHEN BEIKE RUISHENG TECHNOLOGY CO (SHEN-Non-standard);  BEIJING INST TECHNOLOGY SHENZHEN INST (BEIT)   HUANG S, LIU Y, WANG M, XIE X
      CN103488782-A   UNIV NORTH CHINA ELECTRIC POWER (UYHD)   HE H
      CN104200804-A   UNIV HEFEI TECHNOLOGY (UYHE)   SUN X, LI C, REN F, CHEN W
      CN105719664-A   YANCHENG INST TECHNOLOGY (YANH)   SUN D, WANG R, ZHOU F, ZHOU L
      KR2013059093-A   SALTLUX (SALT-Non-standard)   CHAE S M, LEE K I, CHOI K S, SHIN S H
UT DIIDW:2016801009
ER

PT P
PN CN106202010-A
TI Method for constructing legal text syntax tree based on depth neural network, involves obtaining dependent description of training text according to semantic annotation of training text.
AU LI J
   LI P
   LIU T
   PENG D
   SUN J
   ZHAO F
AE CHONGQING ZHAOGUANG TECHNOLOGY CO LTD (CHON-Non-standard)
GA 201681327N
AB    NOVELTY - The method involves obtaining (301) a training text of a legal service. The word vector sparse representation is performed (302) on the training text. The training text is semantically annotated (303) by a deep neural network (DNN) according to the word vector in the sparse representation. The dependent description of the training text is obtained according to semantic annotation of the training text. The legal text syntax tree is constructed (304).
   USE - Method for constructing legal text syntax tree based on depth neural network in information processing field.
   ADVANTAGE - The systematic approach is provided to build a syntax tree for professional language in the field of legal services. The influence in the semantic deviation in legal language can be reduced.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a device for constructing legal text syntax tree.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the method for constructing legal text syntax tree. (Drawing includes non-English language text)
   Step for obtaining training text of legal service (301)
   Step for performing word vector sparse representation (302)
   Step for semantically annotating training text (303)
   Step for constructing legal text syntax tree (304)
DC T01 (Digital Computers)
MC T01-J07B; T01-J11A; T01-J16C1; T01-J16C3; T01-N01B3
IP G06F-017/22; G06N-003/02
PD CN106202010-A   07 Dec 2016   G06F-017/22   201706   Pages: 13   Chinese
AD CN106202010-A    CN10546350    12 Jul 2016
PI CN10546350    12 Jul 2016
CP CN106202010-A
      CN102662931-A   UNIV XIAMEN (UYXI)   CHEN Y, SHI X, HUANG Z, ZHOU C
      CN104008092-A   UNIV FUDAN (UYFU)   WANG W, WANG X, XIAO Y
      CN104021115-A   BEIJING INST TECHNOLOGY (BEIT)   LIAO C, YANG S, ZHANG C, FENG C
      CN104462066-A   BEIJING BAIDU NETCOM SCI & TECHNOLOGY CO (BIDU)   WU X
      US7484219-B2   MICROSOFT CORP (MICT)   MITRA K
      WO2005045695-A1   EDUCATIONAL TESTING SERVICE (EDUC-Non-standard)   HIGGINS D, BURSTEIN J, MARCU D, GENTILE C
CR CN106202010-A
      &#23004;&#38678; &#31561;: "&#37319;&#29992;&#36830;&#32493;&#35789;&#34955;&#27169;&#22411;(CBOW)&#30340;&#39046;&#22495;&#26415;&#35821;&#33258;&#21160;&#25277;&#21462;&#30740;&#31350;", &#12298;&#29616;&#20195;&#22270;&#20070;&#24773;&#25253;&#25216;&#26415;&#12299;,relevantClaims[3-5],relevantPassages[&#35770;&#25991;&#31532;3&#33410;]
UT DIIDW:201681327N
ER

PT P
PN CN106202054-A; CN106202054-B
TI Deep-learning facing medical field based named entity recognizing method, involves obtaining word sequence LSTM input of training corpus, calculating initial value of LSTM model, and calculating recognition evaluation standard F-value.
AU ZHU C
   ZHAO T
   YANG M
   XU B
   CAO H
   ZHENG D
AE HARBIN INST TECHNOLOGY (HAIT-C)
GA 2016782193
AB    NOVELTY - The method involves obtaining vocabulary word vector corresponding to supplemental medical field corpus. A linguistic word type total number is determined. Training vector is determined by a long-term memory network unit according to marked training language corpus. Optimized target is determined according to gradient descent algorithm. A neural network parameter is determined. Mark word sequence LSTM input of training corpus is obtained. An initial value of a LSTM model is calculated. Named entity recognition evaluation standard F-value is calculated.
   USE - Deep-learning facing medical field based named entity recognizing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep-learning facing medical field based named entity recognizing method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-G02G9; T01-J04A; T01-J04E; T01-J11A1; T01-J16C1; T01-N01B3; T01-N01E
IP G06F-017/27; G06F-019/00; G16H-070/40
PD CN106202054-A   07 Dec 2016   G06F-017/27   201723   Pages: 17   Chinese
   CN106202054-B   14 Dec 2018   G06F-017/27   201902      Chinese
AD CN106202054-A    CN10590151    25 Jul 2016
   CN106202054-B    CN10590151    25 Jul 2016
FD  CN106202054-B Previous Publ. Patent CN106202054
PI CN10590151    25 Jul 2016
CP CN106202054-A
      CN101075228-A   MATSUSHITA ELECTRIC IND CO LTD (MATU)   YAN P, SUN Y, TAKASHI T
      CN102314417-A   UNIV XIDIAN (UYXN)   CHEN D, HE W, LIU Z, QU J, WANG J, WANG W, WANG Y, WANG Z, YAO Y, ZHAO H, ZHU X
      CN103544392-A   UNIV CHINA ELECTRONIC SCI & TECHNOLOGY (UEST)   CHEN L, CAI H, HU X, LIU Q, PU X, QIU H
      CN104298651-A   UNIV DALIAN TECHNOLOGY (UYDA)   JIANG Z, LI L
      CN104899304-A   BEIJING JINGDONG CENTURY TRADING CO LTD (JDOF);  BEIJING JINGDONG SHANGKE INFORMATION TEC (JDOF)   JIANG W
      CN105244020-A   BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BIDU)   CHEN Z, LI X, XU Y, FU X
      US20090326923-A1      
CR CN106202054-A
      &#27573;&#36229;&#32676;: "&#38754;&#21521;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#39046;&#22495;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#30740;&#31350;", &#12298;&#20013;&#22269;&#20248;&#31168;&#30805;&#22763;&#23398;&#20301;&#35770;&#25991;&#20840;&#25991;&#25968;&#25454;&#24211; &#20449;&#24687;&#31185;&#25216;&#36753;&#12299;,relevantClaims[1&#12289;4-7&#12289;10],relevantPassages[&#27491;&#25991;&#31532;14&#39029;&#31532;2.3.1&#23567;&#33410;&#33267;&#31532;31&#39029;3.2.2&#23567;&#33410;]
      &#32993;&#26032;&#36784;: "&#22522;&#20110;LSTM&#30340;&#35821;&#20041;&#20851;&#31995;&#20998;&#31867;&#30740;&#31350;", &#12298;&#20013;&#22269;&#20248;&#31168;&#30805;&#22763;&#23398;&#20301;&#35770;&#25991;&#20840;&#25991;&#25968;&#25454;&#24211; &#20449;&#24687;&#31185;&#25216;&#36753;&#12299;,relevantClaims[1&#12289;4-7&#12289;10],relevantPassages[&#27491;&#25991;&#31532;24&#39029;3.1.1.3&#23567;&#33410;&#33267;&#31532;38&#39029;3.2.6&#23567;&#33410;&#65292;&#22270;3-9&#33267;&#22270;3-16]
UT DIIDW:2016782193
ER

PT P
PN CN106127684-A; CN106127684-B
TI Neural network based recursive convolution image super-resolution enhancement method, involves inputting sequence to network model, and obtaining high resolution image of sample sequence corresponding to low resolution image block.
AU HUANG K
   XU R
   ZHANG J
AE CHINESE ACAD SCI AUTOMATION INST (CAZD-C)
   CHINESE ACAD SCI AUTOMATION INST (CAZD-C)
GA 201673221P
AB    NOVELTY - The method involves obtaining a low resolution image block by sliding window process of a low resolution image. A high-resolution image block is obtained corresponding to the low resolution image block. Resolution enhancement process is performed based on a bi-directional recursive convolutional neural network model. A sample sequence is obtained by processing the low resolution image block. The sample sequence is inputted to the convolutional neural network model. A high resolution image of the sample sequence is obtained corresponding to the low resolution image block.
   USE - Neural network based recursive convolution image super-resolution enhancement method.
   ADVANTAGE - The method enables obtaining texture detail information of the high-resolution image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a neural network based recursive convolution image super-resolution enhancement method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J10B1; T01-J10B3A; T01-J10D; T01-J16C1
IP G06T-003/40; G06T-005/00
PD CN106127684-A   16 Nov 2016   G06T-003/40   201680      Chinese
   CN106127684-B   15 Mar 2019   G06T-003/40   201922      Chinese
AD CN106127684-A    CN10458327    22 Jun 2016
   CN106127684-B    CN10458327    22 Jun 2016
FD  CN106127684-B Previous Publ. Patent CN106127684
PI CN10458327    22 Jun 2016
CP CN106127684-A
      CN106508045-B      
      US9207759-B1   EDGE3 TECHNOLOGIES INC (EDGE-Non-standard)   EL DOKOR T, KING J, HAUPTMAN R
CR CN106127684-A
      : "", ,relevantClaims[1-8],relevantPassages[556-564]
UT DIIDW:201673221P
ER

PT P
PN CN106127204-A
TI Full convolutional neural network meter reading direction region detecting method, involves acquiring training data and meter image by camera, obtaining multiple direction meter reading area detection results by rotating rectangular frame.
AU JIN L
   LIU X
AE UNIV SOUTH CHINA TECHNOLOGY (UYSC-C)
GA 201673231P
AB    NOVELTY - The method involves acquiring training data and a meter image through a camera. Convolutional neural network designing process is performed. A full convolution parameter is optimized by a neural network. Multiple channel characteristic patterns obtaining process is performed. A multi-channel image is scanned by a sliding window. A meter reading area is primarily selected by a rectangle window. Area position information extraction characteristic pattern is determined. Multiple direction meter reading area detection results are obtained by a rotating rectangular frame.
   USE - Full convolutional neural network meter reading direction region detecting method.
   ADVANTAGE - The method enables improving detecting accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a full convolutional neural network meter reading direction region detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J07B; T01-J10B2; T01-J15A4; T01-J16C1; T01-N01B3; T04-D02; T04-D04; T04-D07D5
IP G06K-009/32; G06K-009/62; G06N-003/02
PD CN106127204-A   16 Nov 2016   G06K-009/32   201681   Pages: 9   Chinese
AD CN106127204-A    CN10515007    30 Jun 2016
PI CN10515007    30 Jun 2016
CP CN106127204-A
      CN104217225-A   CHINESE ACAD SCI AUTOMATION INST (CAZD)   HUANG K, REN W, WANG C, ZHANG J
      CN104680508-A   HUAWEI TECHNOLOGIES CO LTD (HUAW)   OUYANG W, XU C, LIU J, WANG X
      CN105260710-A   BEIJING INST PETROCHEMICAL TECHNOLOGY (SNPC)   LI J, TIAN X, ZHANG N, FAN C
      CN105631519-A   UNIV BEIJING TECHNOLOGY (UYBT)   HUANG Q, LIN H, PANG J, YIN B
CR CN106127204-A
      SHAOQING REN ET AL: "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 ,relevantClaims[1-6],relevantPassages[3-4]
      KAIMING HE ET AL: "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition", EUROPEAN CONFERENCE ON COMPUTER VISION,relevantClaims[1-6],relevantPassages[346-361]
      ROSS GIRSHICK ET AL: "Fast R-CNN", THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION ,relevantClaims[1-6],relevantPassages[1440-1448]
UT DIIDW:201673231P
ER

PT P
PN EP3091486-A2; US2016328643-A1; CN106127217-A; EP3091486-A3; US9633306-B2
TI Method for anatomical object detection in medical image, involves detecting anatomical object in received medical image of patient using approximation of trained deep neural network.
AU COMANICIU D
   GEORGESCU B
   KRETSCHMER J
   LAY N
   LIU D
   NGUYEN H
   SINGH V K
   ZHENG Y
   ZHOU S K
   COMANICHU D
   LAI N
   SINGER V K
   JEONG Z
   ZHOU S
AE SIEMENS AG (SIEI-C)
   SIEMENS HEALTHCARE GMBH (SIEI-C)
GA 201669550S
AB    NOVELTY - The method involves training (102) a deep neural network to detect the anatomical object in medical images. An approximation of the trained deep neural network that reduces the computational complexity of the trained deep neural network is calculated (104). The weights of the filters are sparsified for each of multiple layers of the trained deep neural network. The anatomical object in a received medical image of a patient is detected (108) using the approximation of the trained deep neural network.
   USE - Method for anatomical object detection in medical image.
   ADVANTAGE - The reduction in the computational complexity of the trained deep neural network is achieved by calculating the approximation of the trained deep neural network.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) an apparatus for anatomical object detection in a medical image; and
   (2) a non-transitory computer readable medium storing a computer program for anatomical object detection in a medical image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of the method for anatomical object detection in the medical image using the deep neural network approximation.
   Step for training the deep neural network (102)
   Step for calculating the approximation of the trained deep neural network (104)
   Step for receiving the medical image of the patient (106)
   Step for detecting the anatomical object in the received medical image of the patient (108)
   Step for outputting the anatomical object detection result (110)
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D08A; S05-G02G9; T01-J10B2; T01-J10B3A; T01-J16C1; T01-N01B3; T01-N01E; T01-S03
IP G06N-003/08; G06T-007/00; G06K-009/00; G06K-009/62; G06N-003/04
PD EP3091486-A2   09 Nov 2016   G06N-003/08   201677   Pages: 34   English
   US2016328643-A1   10 Nov 2016   G06N-003/08   201677      English
   CN106127217-A   16 Nov 2016   G06K-009/62   201678      Chinese
   EP3091486-A3   22 Mar 2017   G06N-003/08   201721      English
   US9633306-B2   25 Apr 2017   G06N-003/08   201729      English
AD EP3091486-A2    EP167707    29 Apr 2016
   US2016328643-A1    US706108    07 May 2015
   CN106127217-A    CN10296717    06 May 2016
   EP3091486-A3    EP167707    29 Apr 2016
   US9633306-B2    US706108    07 May 2015
PI US706108    07 May 2015
DS EP3091486-A2: 
		      (Regional): AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LI; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; BA; ME; MA; MD
EP3091486-A3: 
		      (Regional): AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LI; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; BA; ME; MA; MD
CP EP3091486-A2
      US2016174902-A1   SIEMENS AG (SIEI)   GEORGESCU B, ZHENG Y, HGUYEN H, SINGH V K, COMANICIU D, LIU D
   CN106127217-A
      CN104134062-A   ZHU Y (ZHUY-Individual)   ZHU Y
      CN104361328-A   CHINESE ACAD SCI CHONGQING GREEN & INTEL (CAGI)   LIU Y, ZHOU X
      US20080270055-A1      
      US20130138589-A1      
      US20130177235-A1      
CR    CN106127217-A
      Y. XU: "Deep learning for feature representation with multiple instance learning for medical image analysis", 2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTIC, SPEECH AND SIGNAL PROCESSING,relevantClaims[2-10],relevantPassages[1626-1629]
      T.X. HE: "Reshaping deep neural network for fast decoding by node-pruning", 2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTIC, SPEECH AND SIGNAL PROCESSING,relevantClaims[5-10],relevantPassages[245-248]
UT DIIDW:201669550S
ER

PT P
PN CN106096638-A; CN106096638-B
TI Data processing method, involves transmitting social behavior data stream from data space into low-dimensional space vector, where low-dimensional space vector inputs social behavior data stream to multiple levels.
AU DUAN P
   CHEN Q
   LIU Z
AE TENCENT TECHNOLOGY SHENZHEN CO LTD (TNCT-C)
GA 201671629D
AB    NOVELTY - The method involves obtaining social behavior data stream to be processed. Social behavior data stream pre-processing operation is processed. The social behavior data stream is transmitted from a data space into a low-dimensional space vector, where the low-dimensional space vector inputs the social behavior data stream to multiple levels of restricted Boltzmann machine (RBM) stack for calculating process to determine extracted hidden features in the social behavior data stream. The social behavior data stream is classified.
   USE - Data processing method.
   ADVANTAGE - The method enables automatically extracting the hidden features of the social behavior data stream in abstract by the levels of the RBM stack, improving efficiency and reducing development cost.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a data processing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a data processing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T04-D04
IP G06K-009/62
PD CN106096638-A   09 Nov 2016   G06K-009/62   201678   Pages: 16   Chinese
   CN106096638-B   07 Aug 2018   G06K-009/62   201854      Chinese
AD CN106096638-A    CN10394934    03 Jun 2016
   CN106096638-B    CN10394934    03 Jun 2016
FD  CN106096638-B Previous Publ. Patent CN106096638
PI CN10394934    03 Jun 2016
CP CN106096638-A
      CN103345656-A   CHINESE ACAD SCI AUTOMATION INST (CAZD)   HUANG Y, TAN T, WANG W, WANG L
      CN103440352-A   CHINESE ACAD SCI AUTOMATION INST (CAZD)   XU C, SANG J, YUAN Z
      CN105045857-A   INST COMPUTING TECH CHINESE ACAD SCI (CACT)   CHENG X, YU Z, ZHANG Q, XIONG J, XU H, ZHANG S
   CN106096638-B
      CN103345656-A   CHINESE ACAD SCI AUTOMATION INST (CAZD)   HUANG Y, TAN T, WANG W, WANG L
      CN103440352-A   CHINESE ACAD SCI AUTOMATION INST (CAZD)   XU C, SANG J, YUAN Z
      CN105045857-A   INST COMPUTING TECH CHINESE ACAD SCI (CACT)   CHENG X, YU Z, ZHANG Q, XIONG J, XU H, ZHANG S
CR CN106096638-A
      HUGO LAROCHELLE&#31561;: "Learning Algorithms for the Classification Restricted Boltzmann Machine", &#12298;JOURNAL OF MACHINE LEARNING RESEARCH&#12299;,relevantClaims[1-12],relevantPassages[&#31532;643-669&#39029;]
      &#21556;&#35777;&#31561;: "&#32467;&#21512;&#20027;&#20803;&#25104;&#20998;&#20998;&#26512;&#30340;&#21463;&#38480;&#29627;&#32819;&#20857;&#26364;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#38477;&#32500;&#26041;&#27861;", &#12298;&#19978;&#28023;&#20132;&#36890;&#22823;&#23398;&#23398;&#25253;&#12299;,relevantClaims[1-12],relevantPassages[&#31532;559-563&#39029;]
      &#24352;&#26149;&#38686;&#31561;: "&#21463;&#38480;&#27874;&#23572;&#20857;&#26364;&#26426;", &#12298;&#24037;&#31243;&#25968;&#23398;&#23398;&#25253;&#12299;,relevantClaims[1-12],relevantPassages[&#31532;159-173&#39029;]
   CN106096638-B
      Hugo Larochelle and the like. the rule Algorithms for the Classification Restricted Boltzmann Machine. "Journal of Machine E-Learning Research". 2012, (13), page 643-669.
      , and the like. dimension reduction method combined with principal component analysis of restricted Boltzmann machine neural network. Shanghai Jiaotong University Journal ". 2008, volume 42 (4), page 559-563.
      tension spring and so on. restricted Boltzmann machine. "Journal of engineering mathematics". 2015, volume 32 (2), page 159-173.
UT DIIDW:201671629D
ER

PT P
PN CN106095735-A
TI Method for detecting academic literature plagiarism in fingerprint technology based learning resource library based on depth neural network, involves returning text number of fingerprint for indicating suspicion exists in document.
AU LIU Y
   LIU X
   LI L
   LIU W
   LI J
AE BEIJING ZHONGJIA NAT ROAD TECHNOLOGY CO (BEIJ-Non-standard)
GA 201671649B
AB    NOVELTY - The method involves collecting a mass academic literature with any length text. Arbitrary length text is encoded as a fixed number of binary data. The fingerprint is stored in a fingerprint database. Fingerprint of a to-be-detected document is extracted by utilizing a deep neural network and compared with the fingerprint in the fingerprint database. Text number of the fingerprint is returned for indicating copying suspicion exists in the to-be-detected document if the fingerprint of the to-be-detected document is same as the fingerprint in the fingerprint database.
   USE - Method for detecting academic literature plagiarism in a fingerprint technology based learning resource library based on a depth neural network.
   ADVANTAGE - The method enables comparing the fingerprint of the to-be-detected document with the fingerprint database so as to ensure effective identification of candidate sentence, paragraph or article of plagiarism if the fingerprint of the to-be-detected document is same as in the fingerprint database.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for detecting academic literature plagiarism in a fingerprint technology based learning resource library based on a depth neural network. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC S05-D01C5A; T01-J05B4P; T01-J11A; T01-J16C1; T01-N01B3; T01-N01D2; W04-W05A
IP G06F-017/22; G06N-003/08
PD CN106095735-A   09 Nov 2016   G06F-017/22   201682   Pages: 9   Chinese
AD CN106095735-A    CN10395603    06 Jun 2016
PI CN10395603    06 Jun 2016
CP CN106095735-A
      CN101398758-A   UNIV BEIJING AERONAUTICS & ASTRONAUTICS (UNBA)   XIONG H, YAN H
      CN103729459-A   UNIV BEIJING POSTS & TELECOM (UBPT)   ZHOU Y
      CN104598611-A   BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BAID-Non-standard)   LIU Z, XU X, WU X, ZHANG J, HE W, YU T
      CN104657350-A   CHINESE ACAD SCI AUTOMATION INST (CAZD)   HAO H, XU B, TIAN G, WANG F, XU J
      GB2483246-A   FUJITSU LTD (FUIT)   GOODMAN D
CR CN106095735-A
      &#35768;&#20271;&#26704;&#65292;&#27605;&#20940;&#29141;&#65292;&#31041;&#26126;&#32534;: "&#12298;&#29616;&#20195;&#20225;&#19994;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#12299;", 31 January 2007,relevantClaims[1-10],relevantPassages[&#20840;&#25991;]
      STEVE ENGELS&#31561;: "Plagiarism Detection Using Feature-Based Neural Networks", &#12298;PROCEEDINGS OF THE 38TH SIGCSE TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION&#12299;,relevantClaims[1-10],relevantPassages[&#31532;34-37&#39029;]
      &#31206;&#29577;&#24179;&#31561;: "&#22522;&#20110;&#23616;&#37096;&#35789;&#39057;&#25351;&#32441;&#30340;&#35770;&#25991;&#25220;&#34989;&#26816;&#27979;&#31639;&#27861;", &#12298;&#35745;&#31639;&#26426;&#24037;&#31243;&#12299;,relevantClaims[1-10],relevantPassages[&#31532;193-197&#39029;]
UT DIIDW:201671649B
ER

PT P
PN CN106096605-A; CN106096605-B
TI Deep learning based image blurred region detecting method, involves obtaining classification result and element values in RES set corresponding to identification image region, and determining fuzzy attributes of identification image.
AU SHI F
   QIAO B
   WANG B
AE SHI F (SHIF-Individual)
GA 2016716304
AB    NOVELTY - The method involves establishing fuzzy picture blocks. Clear image blocks are obtained. Convolution operation is performed by using a gabor filter to obtain a convolution image. A network optimizing model is obtained. A four-channel image is obtained. Four-channel image elements in ROI are passed through a convolution layer under forward propagation process. Classification result is obtained by classifying the four-channel image. Fuzzy attributes of an identification image is determined. Element values in RES set corresponding to an identification image region are obtained.
   USE - Deep learning based image blurred region detecting method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based image blurred region detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J04B2; T01-J05B2; T01-J10B1; T01-J10B2; T01-N01B3; W04-W05A
IP G06K-009/32; G06K-009/46
PD CN106096605-A   09 Nov 2016   G06K-009/32   201681   Pages: 28   Chinese
   CN106096605-B   19 Mar 2019   G06K-009/32   201922      Chinese
AD CN106096605-A    CN10390374    02 Jun 2016
   CN106096605-B    CN10390374    02 Jun 2016
FD  CN106096605-B Previous Publ. Patent CN106096605
PI CN10390374    02 Jun 2016
CP CN106096605-A
      CN101625721-A   AMBARELLA SEMICONDUCTOR TECHNOLOGY SHANGHAI CO LTD (AMBA-Non-standard)   LEI F
      CN104408469-A   UNIV WUHAN (UYWU)   WANG Y, WU X, ZHAO J, ZHANG D
      CN104794504-A   UNIV ZHEJIANG (UYZH)   LI T, YU H
      CN105184271-A   JIANGSU CHINA SCI INTELLIGENT ENG CO LTD (JIAN-Non-standard);  QINGDAO ACAD INTELLECTUAL IND (QING-Non-standard);  SUZHOU PAIRULEIER INTELLIGENT TECHNOLOGY (SUZH-Non-standard);  TIANWEN HUIKE BEIJING TECHNOLOGY CO LTD (TIAN-Non-standard)   TIAN B, WANG F, YANG L, YAO Y
      CN105335716-A   UNIV BEIJING TECHNOLOGY (UYBT)   WANG L, GE X, KONG D
      CN105631439-A   BEIJING MEGVII TECHNOLOGY CO LTD (BEIJ-Non-standard)   GU K, HE T, YIN Q
   CN106096605-B
      CN101625721-A   AMBARELLA SEMICONDUCTOR TECHNOLOGY SHANGHAI CO LTD (AMBA-Non-standard)   LEI F
      CN104408469-A   UNIV WUHAN (UYWU)   WANG Y, WU X, ZHAO J, ZHANG D
      CN104794504-A   UNIV ZHEJIANG (UYZH)   LI T, YU H
      CN105184271-A   JIANGSU CHINA SCI INTELLIGENT ENG CO LTD (JIAN-Non-standard);  QINGDAO ACAD INTELLECTUAL IND (QING-Non-standard);  SUZHOU PAIRULEIER INTELLIGENT TECHNOLOGY (SUZH-Non-standard);  TIANWEN HUIKE BEIJING TECHNOLOGY CO LTD (TIAN-Non-standard)   TIAN B, WANG F, YANG L, YAO Y
      CN105335716-A   UNIV BEIJING TECHNOLOGY (UYBT)   WANG L, GE X, KONG D
      CN105631439-A   BEIJING MEGVII TECHNOLOGY CO LTD (BEIJ-Non-standard)   GU K, HE T, YIN Q
CR CN106096605-A
      LIU R &#31561;: "Image partial blur detection and classification", &#12298;PROCEEDINGS OF THE IEEE CONFERENCE ON COMPUTER VISION AND PATTERN &#65330;ECOGNITION&#12299;,relevantClaims[1-10],relevantPassages[&#31532;1-8&#39029;]
      &#23609;&#23453;&#25165; &#31561;: "&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#32508;&#36848;", &#12298;&#21271;&#20140;&#24037;&#19994;&#22823;&#23398;&#23398;&#25253;&#12299;,relevantClaims[1-10],relevantPassages[&#31532;48-59&#39029;]
      &#32993;&#27491;&#24179; &#31561;: "&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#27169;&#22411;&#22312;&#27169;&#24335;&#35782;&#21035;&#20013;&#30340;&#26032;&#36827;&#23637;", &#12298;&#29141;&#23665;&#22823;&#23398;&#23398;&#25253;&#12299;,relevantClaims[1-10],relevantPassages[&#31532;283-291&#39029;]
   CN106096605-B
      Liu R and so on. CH2-C* of Image Blur detection and classification. Use of the IEEE Conference on Computer Vision and Pattern Recognition. 2008, page 1-8.
      Hu, positive level, and so on. new progress convolutional neural network classification model in pattern recognition. Yanshan University Journal. 2015, volume 39 (4), page 283-291.
      Yoon. depth learning research statement. Beijing Industry University Journal ". 2015, volume 41 (1), page 48-59.
UT DIIDW:2016716304
ER

PT P
PN CN106022287-A
TI Deep learning dictionary representation based across age face authentication method, involves obtaining human face image of personal face block when encoded vector of two images is calculated by using cosine.
AU GU J
   HU H
   LI H
   XIAO X
AE SYSU-CMU SHUNDE INT JOINT RES INST (SYSU-Non-standard)
GA 201667689C
AB    NOVELTY - The method involves locating a number of key points by face key point locating process. The key points are extracted corresponding to a local face block. Depth of the local face block is determined corresponding to a learning frame. High-level feature vector of local face block is extracted. Age of the local face block is determined as the of a human face image. M-dimensional vector is determined as final coding vector of the human face image. The human face image of a personal face block is obtained when encoded vector of two images is calculated by using cosine.
   USE - Deep learning dictionary representation based across age face authentication method.
   ADVANTAGE - The method enables realizing age, area and class feature extracting process by using outer data reference set.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning dictionary representation based across age face authentication method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-E03; T01-E04; T01-J04C; T01-J10B2; T01-J11A1; T01-J12C1; T01-J30A; T04-D07D3; T04-D07F1; W04-W05A
IP G06K-009/00
PD CN106022287-A   12 Oct 2016   G06K-009/00   201674   Pages: 7   Chinese
AD CN106022287-A    CN10369776    27 May 2016
PI CN10369776    27 May 2016
CP CN106022287-A
      CN102663413-A   ZHONGDUN XINAN TECHNOLOGY JIANGSU CO LTD (ZHON-Non-standard)   GAO Y, LIU G, YANG Y, SUN W
      CN104866829-A   UNIV SOOCHOW (USWZ)   GE R, GONG S, JI Y, LIU C, WANG C, DI H
CR CN106022287-A
      JIANQUAN GU : "Patch-based Sparse Dictionary Representation for Face Recognition with Single Sample per Person", CCBR 2015,relevantClaims[1-4],relevantPassages[120-126]
      BOR-CHUN CHEN : "Face Recognition and Retrieval Using Cross-Age Reference Coding With Cross-Age Celebrity Dataset", IEEE TRANSACTIONS ON MULTIMEDIA,relevantClaims[1-4],relevantPassages[I4IIIACD23]
      : "", ,relevantClaims[1-4],relevantPassages[1.11.32.12.23.1]
UT DIIDW:201667689C
ER

PT P
PN CN106022232-A
TI Deep learning based number plate detection method, involves obtaining quick-rcnn convolution neural network output vector, determining whether number plate rough area is detected as number plate area, and obtaining final number plate area.
AU ZOU G
   JIANG T
   LI H
AE CHENGDU XINZHOU RUISHI TECHNOLOGY CO LTD (CHEN-Non-standard)
GA 201666489F
AB    NOVELTY - The method involves constructing a picture library of a label. A training sample set is obtained by utilizing a faster-rcnn algorithm. Number plate detection is performed based a reference frame of a RPN convolution neural network (RPN CNN) and a quick-rcnn CNN. A convolution layer shared parameter is obtained. A rough area of a number plate is detected. An output vector of the quick-rcnn CNN is obtained. Determination is made to check whether a rough area of the number plate is detected as a number plate area. A final number plate area is obtained.
   USE - Deep learning based number plate detection method.
   ADVANTAGE - The method enables performing number plate detection process in simple manner with less calculation amount, and reducing omission factor.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a deep learning based number plate detection system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J07B; T01-J10B2; T01-J16C1; T01-N01B3; T04-D04
IP G06K-009/00; G06K-009/62; G06N-003/02
PD CN106022232-A   12 Oct 2016   G06K-009/00   201674   Pages: 10   Chinese
AD CN106022232-A    CN10312822    12 May 2016
PI CN10312822    12 May 2016
CP CN106022232-A
      CN105069413-A   UNIV ELECTRONIC SCI & TECHNOLOGY (UEST)   DONG L, ZHANG N
      US20140376819-A1      
CR CN106022232-A
      SHAOQING REN ET AL: "Faster R-CNN: Towards Real-Time Object", NIPS,relevantClaims[1-5],relevantPassages[1-3]
UT DIIDW:201666489F
ER

PT P
PN WO2016124103-A1; CN105989330-A
TI Picture detection method, involves acquiring picture detection threshold value corresponding to picture detection model, and determining picture type of first picture using picture detection model and picture detection threshold value.
AU CHEN Y
AE ALIBABA GROUP HOLDING LTD (ABAB-C)
   CHEN Y (CHEN-Individual)
GA 2016488081
AB    NOVELTY - The method involves acquiring training pictures to be already marked with picture types by a picture detection device (S1), where the picture types comprise normal types or prohibited types. A corresponding picture detection model is obtained (S2) based on the training pictures through training of a convolutional neural network. A picture detection threshold value is acquired (S3) corresponding to the picture detection model. The picture type of a first picture is determined (S4) using the picture detection model and the picture detection threshold value.
   USE - Picture detection method.
   ADVANTAGE - The method enables efficiently and accurately recognizing types of pictures to be detected, thus effectively shortening check time of pictures and improving user experience degree.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a picture detection device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a picture detection method. '(Drawing includes non-English language text)'
   Step for acquiring training pictures to be already marked with picture types by picture detection device (S1)
   Step for obtaining corresponding picture detection model based on training pictures through training of convolutional neural network (S2)
   Step for acquiring picture detection threshold value corresponding to picture detection model (S3)
   Step for determining picture type of first picture using picture detection model and picture detection threshold value (S4)
DC T01 (Digital Computers)
MC T01-J10B2A; T01-J16C1; T01-N01B3
IP G06K-009/00; G06K-009/62
PD WO2016124103-A1   11 Aug 2016   G06K-009/00   201655   Pages: 29   Chinese
   CN105989330-A   05 Oct 2016   G06K-009/00   201669      Chinese
AD WO2016124103-A1    WOCN072468    28 Jan 2016
   CN105989330-A    CN10055621    03 Feb 2015
PI CN10055621    03 Feb 2015
DS WO2016124103-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IR; IS; JP; KE; KG; KN; KP; KR; KZ; LA; LC; LK; LR; LS; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SA; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
      (Regional): BW; GH; GM; KE; LR; LS; MW; MZ; NA; RW; SD; SL; ST; SZ; TZ; UG; ZM; ZW; EA; AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; OA
CP WO2016124103-A1
      CN103336942-A   UNIV SUN YAT SEN (UYSY);  GUANGZHOU ART MUSEUM (GUAN-Non-standard)   BEI H, CHEN W, FANG H, LIN L, WANG J, WANG Y, YANG L, ZHU J
      CN103544506-A   TCL GROUP CO LTD (TCLC)   SHAO S, ZHOU L
      CN103971342-A   XIAMEN MEITUZHIJIA TECHNOLOGY CO LTD (XIAM-Non-standard)   WANG Z, ZHANG W, FU S, ZHANG Z
      CN104036323-A   YE M (YEMM-Individual)   FU M, LI T, LI X, WANG M, XIAO H, YE M
      CN104182735-A   XIAMEN MEITUZHIJIA TECHNOLOGY CO LTD (XIAM-Non-standard)   XU Q, ZHANG W, FU S, ZHANG Z
      US20100002920-A1      
   CN105989330-A
      CN102879401-A   UNIV XIAN POLYTECHNIC (UYXP)   LI P, JIAO Y, ZHANG H, LI J, LI H, JING J
      CN103336942-A   UNIV SUN YAT SEN (UYSY);  GUANGZHOU ART MUSEUM (GUAN-Non-standard)   BEI H, CHEN W, FANG H, LIN L, WANG J, WANG Y, YANG L, ZHU J
      CN104036323-A   YE M (YEMM-Individual)   FU M, LI T, LI X, WANG M, XIAO H, YE M
      CN104063719-A   SHENZHEN SUNWIN INTELLIGENT TECHNOLOGY C (SHEN-Non-standard)   LIU W, TONG Q, WANG Y, HU Z, MO Y
      CN104182735-A   XIAMEN MEITUZHIJIA TECHNOLOGY CO LTD (XIAM-Non-standard)   XU Q, ZHANG W, FU S, ZHANG Z
UT DIIDW:2016488081
ER

PT P
PN US2016217157-A1; WO2016118339-A1; KR2017107039-A; EP3248142-A1; CN107430691-A; EP3248142-A4
TI Computer system for identifying items depicted in images, comprises a memory having instructions, and the processor which is configured by the instructions to store a set of records for a set of corresponding items.
AU SHIH K
   DI W
   JAGADEESH V
   PIRAMUTHU R
AE EBAY INC (EBAY-C)
GA 2016460293
AB    NOVELTY - The system (110) comprises a memory having instructions. The processor is configured by the instructions to store a set of records for a set of corresponding items. The record of the set of records includes the text data and image data for the item corresponding to the record. The image is accessed which depicts an item. The first set of candidate matches is generated for the item from the set of items based on the image and the image data of the set of records. The text in the image is recognized. The second set of candidate matches is generated for the item.
   USE - Computer system for identifying items depicted in images, such as photo of a product, such as books or compact disks.
   ADVANTAGE - The trained convolutional neural network of the image identification module identifies a probability of a particular image matching the downloadable version of the compact disks, which enables efficient retrieval of data for an item from a media database.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a method for identifying items depicted in images; and
   (2) a machine-readable medium having instructions for executing the method for identifying items depicted in images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a computer system.
   Network environment (100)
   Computer system (110)
   Commerce server (120)
   Server (130)
   Network (170)
DC T01 (Digital Computers)
MC T01-F05B2; T01-J05B4F; T01-J10B2A; T01-J16C1; T01-N02A3C; T01-S03
IP G06F-017/30; G06K-009/46
PD US2016217157-A1   28 Jul 2016   G06F-017/30   201651   Pages: 22   English
   WO2016118339-A1   28 Jul 2016   G06K-009/46   201651      English
   KR2017107039-A   22 Sep 2017   G06F-017/30   201768      
   EP3248142-A1   29 Nov 2017   G06K-009/46   201779      English
   CN107430691-A   01 Dec 2017   G06K-009/46   201781      Chinese
   EP3248142-A4   13 Dec 2017   G06K-009/46   201782      English
AD US2016217157-A1    US973582    17 Dec 2015
   WO2016118339-A1    WOUS012691    08 Jan 2016
   KR2017107039-A    KR723364    08 Jan 2016
   EP3248142-A1    EP740502    08 Jan 2016
   CN107430691-A    CN80014377    08 Jan 2016
   EP3248142-A4    EP740502    08 Jan 2016
FD  US2016217157-A1 Provisional Application US107095P
   KR2017107039-A PCT application Application WOUS012691
   KR2017107039-A Based on Patent WO2016118339
   EP3248142-A1 PCT application Application WOUS012691
   EP3248142-A1 Based on Patent WO2016118339
   CN107430691-A PCT application Application WOUS012691
   CN107430691-A Based on Patent WO2016118339
PI US107095P    23 Jan 2015
   US973582    17 Dec 2015
   WOUS012691    08 Jan 2016
DS WO2016118339-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IR; IS; JP; KE; KG; KN; KP; KR; KZ; LA; LC; LK; LR; LS; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SA; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
      (Regional): BW; GH; GM; KE; LR; LS; MW; MZ; NA; RW; SD; SL; ST; SZ; TZ; UG; ZM; ZW; EA; AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; OA
EP3248142-A1: 
		      (Regional): AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LI; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; BA; ME; MA; MD
EP3248142-A4: 
		      (Regional): AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LI; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; BA; ME
CP US2016217157-A1
      US5404507-A   AT & T CORP (AMTT)   SIMON J J, NOWITZ D A, BOHM C P
      US20110153653-A1      
      US20110238659-A1      
      US20140046935-A1      
      US20140164406-A1      
      US7949191-B1   A9.COM INC (AMAZ)   RAMKUMAR G D, MANMATHA R, BHATTACHARYYA S, BHARGAVA G, RUZON M
      US8761512-B1   GOOGLE INC (GOOG)   BUDDEMEIER U, TAUBMAN G, ADAM H, ROSENBERG C, NEVEN H, PETROU D, BRUCHER F
   WO2016118339-A1
      US20060251292-A1      
      US20090304267-A1      
      US20130159920-A1      
      US20140046935-A1      
      US20140100991-A1      
      US8478052-B1   GOOGLE INC (GOOG)   YEE Y H, BENGIO S, ROSENBERG C, MURPHY-CHUTORIAN E
      US8775436-B1   GOOGLE INC (GOOG)   ZHOU H, MITROVIC S, BHARAT K, SCHMITT M, CURTISS M
   EP3248142-A4
      EP1703444-A2   RICOH KK (RICO)   OHGURO Y
      US20060251292-A1      
      US20080267504-A1      
      US8635124-B1   EBAY INC (EBAY)   HAMILTON S F, OLSON M, SINGH V, GATES S
CR US2016217157-A1
      Sam S. Tsai, David Chen, Huizhong Chen, Cheng-Hsin Hsu, Jatinder P. Singh, Bernd Girod, " Combining Image and Text Features: A Hybrid Approach to Mobile Book Spine Recognition", November 28 &#8211; December 1, 2011, MM &#8217;11 Proceedings of the 19th ACM international conference on Multimedia Pages 1029-1032
   EP3248142-A4
      "Progress in Computer Vision and Image Analysis", vol. 29, 1 April 1998, WORLD SCIENTIFIC, ISBN: 978-981-2834-46-1, ISSN: 1793-0839, article JONATHAN J HULL ET AL: "LANGUAGE IDENTIFICATION IN COMPLEX, UNORIENTED, AND DEGRADED DOCUMENT IMAGES", pages: 17 - 39, XP055417829, DOI: 10.1142/9789812797704_0002,relevantClaims[1-15],relevantPassages[&lt;pp&gt;A&lt;/pp&gt;]
      See also references of WO 2016118339A1
UT DIIDW:2016460293
ER

PT P
PN CN105654200-A
TI Depth learning based train advertising click prediction method, involves determining detected characteristic changing vector, and performing advertising click prediction process by using training click drilling depth learning model.
AU DONG Q
AE SHANGHAI ZHENDAO INFORMATION TECHNOLOGY (SHAN-Non-standard)
GA 2016361719
AB    NOVELTY - The method involves determining training characteristic transformation vector. A training click drilling depth learning model is established according to the training characteristic transformation vector. Detected characteristic changing vector is determined. Advertising click prediction process is performed by using the training click drilling depth learning model. Train advertising click time is determined according to train number. Judgment is made to check whether advertising page residence time is less than a preset value. A discrete training characteristic value is calculated.
   USE - Depth learning based train advertising click prediction method.
   ADVANTAGE - The method enables realizing train advertising click prediction process with high accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a depth learning based train advertising click prediction device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a depth learning based train advertising click prediction method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05A2; T01-J30A
IP G06Q-010/04; G06Q-030/02
PD CN105654200-A   08 Jun 2016   G06Q-010/04   201643   Pages: 13   Chinese
AD CN105654200-A    CN11022651    30 Dec 2015
PI CN11022651    30 Dec 2015
CP CN105654200-A
      CN102663617-A   IZP BEIJING TECHNOLOGY CO LTD (IZPB-Non-standard)   HUANG S, LI N, LUO F
      CN103310003-A   UNIV EAST CHINA NORMAL (UYEN)   ZHOU A, WANG X, HE X, JI W, WU S
CR CN105654200-A
      : "", ,relevantClaims[1-10],relevantPassages[2231]
UT DIIDW:2016361719
ER

PT P
PN US2016092766-A1; US9646634-B2
TI Method for training deep neural network that comprises low rank hidden input layer for speech recognition, involves adjusting one or more weights for low rank hidden input layer based on accuracy of confidence score.
AU SAINATH T N
   PARADA S M M C
AE GOOGLE INC (GOOG-C)
GA 201619055S
AB    NOVELTY - The method involves receiving (602) a feature vector comprising i values that represent features of an audio signal encoding an utterance. An output vector comprising o values using the feature vector is determined using the low rank hidden input layer. Another vector using the output vector is determined using the adjoining hidden layer. A confidence score that indicates whether the utterance includes the keyword is determined. The one or more weights for the low rank hidden input layer are adjusted based on an accuracy of the confidence score.
   USE - Method for training deep neural network that comprises low rank hidden input layer for speech recognition.
   ADVANTAGE - The deep neural network with a low rank hidden input layer allows the speech recognition to perform fast and low latency, power efficient, flexible, and speaker adaptive. The hidden layers are allowed to learn a better and more robust feature representation by exploiting larger amounts of data and avoiding bad local optima.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for training a deep neural network; and
   (2) a computer-readable medium storing program for training deep neural network that comprises low rank hidden input layer for speech recognition.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of an example process for detecting keyword utterances in an audio waveform.
   Step for receiving a feature vector comprising i values that represent features of an audio signal encoding an utterance (602)
   Step for generating a posterior probability vector for each of the feature vector (604)
   Step for determining that the first word was present in the audio waveform (606)
   Step for determining that a phrase was present in the audio waveform (608)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-C08A; T01-D02; T01-J16B; T01-J16C1; T01-N01B3; T01-S03; W04-V01; W04-V05
IP G06N-003/04; G06N-003/08; G06N-007/00; G10L-015/06; G10L-025/30
PD US2016092766-A1   31 Mar 2016   G06N-003/08   201625   Pages: 20   English
   US9646634-B2   09 May 2017   G06N-003/04   201732      English
AD US2016092766-A1    US616881    09 Feb 2015
   US9646634-B2    US616881    09 Feb 2015
FD  US2016092766-A1 Provisional Application US057599P
   US9646634-B2 Provisional Application US057599P
PI US057599P    30 Sep 2014
   US616881    09 Feb 2015
FS None/
CP US2016092766-A1
      US5842163-A   SRI INT (STRI)   WEINTRAUB M
      US20030236664-A1      
      US20140019388-A1      
      US20140156575-A1      
      US20150127594-A1      
      US20150161522-A1      
      US20150170020-A1      
      US20150310858-A1      
      US20150310862-A1      
      US20150317990-A1      
      US8918352-B2   MICROSOFT CORP (MICT)   DENG L, YU D
      US9099083-B2   MICROSOFT CORP (MICT)   DENG L, HE X, TUR G, HAKKANI-TUR D
      US9202462-B2   GOOGLE INC (GOOG)   PARADA S M M C, GRUENSTEIN A H, CHEN G
   US9646634-B2
      US5842163-A   SRI INT (STRI)   WEINTRAUB M
      US20030236664-A1      
      US20140019388-A1      
      US20140156575-A1      
      US20150127594-A1      
      US20150161522-A1      
      US20150170020-A1      
      US20150310858-A1      
      US20150310862-A1      
      US20150317990-A1      
      US8918352-B2   MICROSOFT CORP (MICT)   DENG L, YU D
      US9099083-B2   MICROSOFT CORP (MICT)   DENG L, HE X, TUR G, HAKKANI-TUR D
      US9202462-B2   GOOGLE INC (GOOG)   PARADA S M M C, GRUENSTEIN A H, CHEN G
CR US2016092766-A1
      LEE, H-y, et al. "Graph-based Re-ranking using Acoustic Feature Similarity between Search Results for Spoken Term Detection on Low-resource Languages". INTERSPEECH 2014, 14-18 September 2014. pp. 2479-2483.
      SONG, Y. et al. "i-vector representation based on bottleneck features for language identification". ELECTRONICS LETTERS 21st November 2013 Vol. 49 No. 24 pp. 1569&#226;&#128;&#147;1570.
      SONG, Y. et al. "i-vector representation based on bottleneck features for language identification". ELECTRONICS LETTERS 21st November 2013 Vol. 49 No. 24 pp. 1569a1570.
   US9646634-B2
      Lee, H-y, et al. &#x201c;Graph-based Re-ranking using Acoustic Feature Similarity between Search Results for Spoken Term Detection on Low-resource Languages&#x201d;. Interspeech 2014, Sep. 14-18, 2014. pp. 2479-2483.
      Song, Y. et al. &#x201c;i-vector representation based on bottleneck features for language identification&#x201d;. Electronics Letters Nov. 21, 2013 vol. 49 No. 24 pp. 1569-1570.
      Abdel-Hamid, Ossama et al., &#x201c;Applying Convolutional Neural Networks Concepts to Hybrid NN-HMM Model for Speech Recognition&#x201d;, 2012 IEEE Conference on Acoustic, Speech and Signal Processing (ICASSP), 4 pages.
      Davis, Andrew S. et al., &#x201c;Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks&#x201d;, Jan. 28, 2014, arXiv:1312.4661 [cs.LG], 10 pages.
      Denil, Misha et al., &#x201c;Predicting Parameters in Deep Learning&#x201d;, Advances in Neural Information Processing Systems 26 (NIPS 2013), 9 pages.
      Grezl, Frantisek et al., Optimizing Bottle-Neck Features for LVCSR, in 2008 IEEE Conference on Acoustic, Speech and Signal Processing (ICASSP), 4 pages.
      Hinton, Geoffrey et al., &#x201c;Deep Neural Networks for Acoustic Modeling in Speech Recognition&#x201d;, IEEE Signal Processing Magazine, Nov. 2012, 16 pages.
      Keshet, Joseph et al., &#x201c;Discriminative Keyword Spotting&#x201d;, preprint submitted to Elsevier, Oct. 6, 2008, 27 pages.
      Lamel, Lori F. et al., &#x201c;Speech Database Development: Design and Analysis of the Acoustic-Phonetic Corpus&#x201d;, Speech Input/Output Assessment and Speech Databases, Noordwijkerhout, the Netherlands, Sep. 20-23, 1989, 10 pages.
      Liao, Hank et al., &#x201c;Large Scale Deep Neural Network Acoustic Modeling with Semi-Supervised Training Data for Youtube Video Transcription&#x201d;, 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 6 pages.
      Nguyen, Patrick an Phu et al., &#x201c;Keyword Detection Based on Acoustic Alignment&#x201d;, U.S. Appl. No. 13/861,020, filed Apr. 11, 2013, 130 pages.
      Peddinti, Vijayaditya et al., &#x201c;Deep-Scattering Spectrum with Deep Neural Networks&#x201d;, 2014 IEEE Conference on Acoustic, Speech and Signal Processing (ICASSP), 5 pages.
      Sainath, Tara N. et al., &#x201c;Learning Filter Banks Within a Deep Neural Network Framework&#x201d;, 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 6 pages.
      Sainath, Tara N. et al., &#x201c;Low-Rank Matrix Factorization for Deep Neural Network Training with High-Dimensional Output Targets&#x201d;, 2013 IEEE Conference on Acoustic, Speech and Signal Processing (ICASSP), 5 pages.
      Sainath, Tara N., &#x201c;Deep Convolutional Neural Networks for LVCSR&#x201d;, 2013 IEEE Conference on Acoustic, Speech and Signal Processing (ICASSP), 5 pages.
      Senior, Andrew et al., &#x201c;Fine Context, Low-Rank, Softplus Deep Neural Networks for Mobile Speech Recognition&#x201d;, 2014 IEEE Conference on Acoustic, Speech and Signal Processing (ICASSP), 5 pages.
      Sensory Inc.&#x2014;Embedded Speech Technologies for Consumer Electronics, Downloaded from the internet on Sep. 17, 2014 at http://www.sensoryinc.com/, 2 pages.
      Soltau, Hagen et al., &#x201c;Joint Training of Convolutional and Non-Convolutional Neural Networks&#x201d;, 2014 IEEE Conference on Acoustic, Speech and Signal Processing (ICASSP), 5 pages.
      Vanhoucke, Vincent O., et al. &#x201c;Keyword Detection Without Decoding&#x201d;, U.S. Appl. No. 13/860,982, filed Apr. 11, 2013, 144 pages.
      Xue, Jian et al., &#x201c;Singular Value Decomposition Based Low-Footprint Speaker Adaptation and Personalization for Deep Neural Network&#x201d;, 2014 IEEE Conference on Acoustic, Speech and Signal Processing (ICASSP), 5 pages.
      Zhang, Yu et al., &#x201c;Extracting Deep Neural Network Bottleneck Features Using Low-Rank Matrix Factorization&#x201d;, 2014 IEEE Conference on Acoustic, Speech and Signal Processing (ICASSP), 5 pages.
UT DIIDW:201619055S
ER

PT P
PN IL236596-A; US2016196672-A1; US9418458-B2
TI Graph image representation for convolutional neural networks Graph image representation for convolutional neural networks.
AU CHERTOK M
   LORBERT A
AE SUPERFISH LTD (SUPE-Non-standard)
   SUPERFISH LTD (SUPE-Non-standard)
GA 2016133012
AB    NOVELTY - Graph image representation for convolutional neural networks.
   USE - Graph image representation for convolutional neural networks.
   ADVANTAGE - Graph image representation for convolutional neural networks.
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J16B; T01-J16C1; T04-D03; T04-D04; T04-E
IP G06K-009/00; G06K-009/46; G06K-009/62; G06T-001/40; G06K-009/66; G06T-011/20; G06T-007/00
PD IL236596-A   31 Jan 2016   G06K-009/00   201619   Pages: 1   English
   US2016196672-A1   07 Jul 2016   G06T-011/20   201646      English
   US9418458-B2   16 Aug 2016   G06K-009/00   201655      English
AD IL236596-A    IL236596    05 Jan 2015
   US2016196672-A1    US987479    04 Jan 2016
   US9418458-B2    US987479    04 Jan 2016
PI IL236596    05 Jan 2015
CP    US9418458-B2
      US20150278642-A1      
      US20160086078-A1      
      US20160098844-A1      
      US20160104053-A1      
      US8345921-B1   GOOGLE INC (GOOG)   FROME A, CHEUNG G, ABDULKADER A, ZENNARO M, WU B, BISSACCO A, NEVEN H, VINCENT L, ADAM H
      US20100183217-A1      
      US20100266200-A1      
CR    US9418458-B2
      Krizhevsky et al., &#x201c;ImageNet classification with deep convolutional neural networks&#x201d;, NIPS 2012.
      Strigl et al., &#x201c;Performance and scalability of GPU-based convolutional neural networks&#x201d;, PDP 2010.
      Taigman et al., &#x201c;DeepFace: closing the gap to human-level performance in face verification&#x201d;, CVPR 2014.
      Krizhevsky et al. &#x201c;ImageNet Classification with Deep Convolutional Neural Networks.&#x201d; Proceedings from the Conf. on Neural Information Processing Systems, 2012 (9 pages).
      Simonyan et al. &#x201c;Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps.&#x201d;http://arxiv.org/abs/1312.6034, 2013 (8 pages).
      Zeiler et al.: &#x201c;Visualizing and Understanding Networks&#x201d;; http://arxiv.org/abs/1311.2901v3, 2013 (11 pages).
      Li et al.: &#x201c;Heterogeneous Multi-task Learning for Human Pose Estimation with Deep Convolutional Neural Network&#x201d;; http://arxiv.org/abs/1406.3474v1, 2014 (8 pages).
      Turaga et al.: &#x201c;Convolutional Networks Can Learn to Generate Affinity Graphs for Image Segmentation&#x201d;; Neural computation, vol. 22, No. 2, Feb. 28, 2010, pp. 511-538.
      Office Action issued in IL 236596, mailed Jun. 23, 2015 (6 pages).
UT DIIDW:2016133012
ER

PT P
PN CN105244020-A; CN105244020-B
TI Method of training hierarchical model for speech synthesis, involves obtaining training data belonging to text features and annotation according to word vector and prosodic annotation data to train prosodic hierarchy model.
AU CHEN Z
   LI X
   XU Y
   FU X
AE BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BIDU-C)
   BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BIDU-C)
GA 2016054493
AB    NOVELTY - The method involves obtaining (S201) no words to corpus data according to the character vector and prosodic annotation data. The training data belonging to text features and annotation is obtained (S202) according to the word vector and prosodic annotation data for training the prosodic hierarchy model. The prosodic hierarchy model training is labeled (S203) based on deep neural network and bidirectional long short term memory (LSTM) neural network using text features of the training data.
   USE - Method of training hierarchical model for speech synthesis.
   ADVANTAGE - The process of labeling the prosodic hierarchy model based on the text features and two-way LSTM network structure improve the accuracy of the prediction of rhythm and this makes the rhythm of the pause more smooth and natural, enhance the user experience.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a hierarchical model training device for speech synthesis.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the process of training the class model for speech synthesis. (Drawing includes non-English language text)
   Step for obtaining no words to corpus data according to the character vector and prosodic annotation data (S201)
   Step for obtaining training data belonging to text features and annotation according to the word vector and prosodic annotation data to train the prosodic hierarchy model (S202)
   Step for labeling prosodic hierarchy model training based on deep neural network and bidirectional LSTM neural network using text features of the training data (S203)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-C08A; T01-J16C1; T01-N01B3; W04-V02; W04-V04C1; W04-X03E1
IP G10L-013/08; G10L-013/10
PD CN105244020-A   13 Jan 2016   G10L-013/08   201610   Pages: 17   English
   CN105244020-B   22 Mar 2017   G10L-013/08   201724      Chinese
AD CN105244020-A    CN10616919    24 Sep 2015
   CN105244020-B    CN10616919    24 Sep 2015
FD  CN105244020-B Previous Publ. Patent CN105244020
PI CN10616919    24 Sep 2015
CP CN105244020-A
      CN104538024-A   BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BAID-Non-standard)   JIA L, KANG Y, LI W, GAI Y, ZOU S
      US20110046958-A1      
   CN105244020-B
      CN104538024-A   BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BAID-Non-standard)   JIA L, KANG Y, LI W, GAI Y, ZOU S
      US20110046958-A1      
CR CN105244020-A
      YUCHEN FAN ET AL: "TTS Synthesis with Bidirectional LSTM based Recurrent Neural Networks", &#12298;INTERSPEECH 2014&#12299
      YUCHEN FAN ET AL: "TTS Synthesis with Bidirectional LSTM based Recurrent Neural Networks", &#12298;INTERSPEECH 2014&#12299;,relevantClaims[1-10|1,6]
   CN105244020-B
      Yuchen Fan et al.TTS Synthesis with hereafter LSTM Ben Hassen between Networks." "INTERSPEECH 2014". 2014
UT DIIDW:2016054493
ER

PT P
PN CN105224775-A
TI Clothes based image processing method, involves obtaining images of clothes by using electronic device, performing shooting process to determine wardrobe data, and generating depth learning training information by using matching module.
AU CHENG C
   SHI Y
   XU H
   ZHOU X
AE CHINESE ACAD SCI CHONGQING GREEN & INTEL (CAGI-C)
GA 201603693N
AB    NOVELTY - The method involves obtaining images of clothes by using an electronic device. Shooting process is performed to determine local wardrobe data. The clothes images are transmitted to two matching modules. Image information of the clothes is identified by using a local database unit. First set clothes image information is obtained by using one of the matching modules. Second set clothes image information is generated by using the former matching module in off-line mode. Depth learning training information is generated by using the latter matching module.
   USE - Clothes based image processing method.
   ADVANTAGE - The method enables performing image processing with better matching effect.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a cloth based image processing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a clothes based image processing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J05B4F; T01-J10B2; T01-J15X; T01-J30A; W04-W05A
IP G06F-017/50
PD CN105224775-A   06 Jan 2016   G06F-017/50   201606   Pages: 14   English
AD CN105224775-A    CN10779020    12 Nov 2015
PI CN10779020    12 Nov 2015
CP CN105224775-A
      CN101477560-A   LIN Z (LINZ-Individual)   LIN Z
      CN102567543-A   BEIJING SOGOU TECHNOLOGY DEV CO (SOGU);  BEIJING SOOGOU INFORMATION SERVICE CO (SOGU)   LU J
      CN104391971-A   CHANGZHOU FEIXUN SHIXUN INFORMATION TECHNOLOGY CO LTD (CHAN-Non-standard)   YAO Z
      CN104951966-A   BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BAID-Non-standard)   HAN S, LI H, LU G, LIU G, YAO H, FAN X, HUANG W, LI Y, MENG X, YAN C, ZU Z
UT DIIDW:201603693N
ER

PT P
PN US2015255061-A1; WO2015134294-A1; US9324321-B2; CN106104673-A; EP3114680-A1
TI Method of adapting and personalizing deep neural network (DNN) model for automatic speech recognition (ASR), involves adapting DNN model comprising reduction in number of parameters in model by updating added matrix.
AU XUE J
   LI J
   YU D
   SELTZER M L
   GONG Y
AE MICROSOFT CORP (MICT-C)
   MICROSOFT TECHNOLOGY LICENSING LLC (MICT-C)
GA 2015528318
AB    NOVELTY - The method (300) involves applying (310) a decomposition approach to an original matrix in the DNN model by a computing device. The original matrix is converted (315) into multiple new matrices in response to applying the decomposition approach. Each new matrix is smaller than the original matrix. A square matrix is added (320) to the multiple new matrices by the computing device. The DNN model is adapted (325) by updating the added matrix. The adapted DNN model comprises a reduction in a number of parameters in the DNN model.
   USE - Method of adapting and personalizing deep neural network model for automatic speech recognition in computing device such as smartphone, desktop, laptop, tablet and game consoles.
   ADVANTAGE - The small matrices are converted from the delta matrices have a low footprint and enable the personalization of smaller and less costly DNN models for use in automatic speech recognition.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for adapting and personalizing a deep neural network model for automatic speech recognition; and
   (2) a computer-readable medium storing program for adapting and personalizing a deep neural network model for automatic speech recognition.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the method for adapting and personalizing deep neural network model for automatic speech recognition.
   Method of adapting and personalizing deep neural network model for automatic speech recognition (300)
   Step for applying decomposition approach (310)
   Step for converting original matrix (315)
   Step for adding square matrix (320)
   Step for adapting square matrix (325)
DC T01 (Digital Computers); W01 (Telephone and Data Transmission Systems); W04 (Audio/Video Recording and Systems)
MC T01-C08A; T01-J04C; T01-J16B; T01-J16C1; T01-M06A1; T01-S03; W01-C01D3C; W01-C01G8; W01-C01P2; W04-V01; W04-X02C
IP G10L-015/16; G10L-015/07; G06N-003/08
PD US2015255061-A1   10 Sep 2015   G10L-015/16   201562   Pages: 16   English
   WO2015134294-A1   11 Sep 2015   G10L-015/07   201562      English
   US9324321-B2   26 Apr 2016   G10L-015/16   201630      English
   CN106104673-A   09 Nov 2016   G10L-015/07   201677      Chinese
   EP3114680-A1   11 Jan 2017   G10L-015/07   201705      English
AD US2015255061-A1    US201704    07 Mar 2014
   WO2015134294-A1    WOUS017872    27 Feb 2015
   US9324321-B2    US201704    07 Mar 2014
   CN106104673-A    CN80012496    27 Feb 2015
   EP3114680-A1    EP717284    27 Feb 2015
FD  CN106104673-A PCT application Application WOUS017872
   CN106104673-A Based on Patent WO2015134294
   EP3114680-A1 PCT application Application WOUS017872
   EP3114680-A1 Based on Patent WO2015134294
PI US201704    07 Mar 2014
DS WO2015134294-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IR; IS; JP; KE; KG; KN; KP; KR; KZ; LA; LC; LK; LR; LS; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SA; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
      (Regional): AL; AT; BE; BG; BW; CH; CY; CZ; DE; DK; EA; EE; ES; FI; FR; GB; GH; GM; GR; HR; HU; IE; IS; IT; KE; LR; LS; LT; LU; LV; MC; MK; MT; MW; MZ; NA; NL; NO; OA; PL; PT; RO; RS; RW; SD; SE; SI; SK; SL; SM; ST; SZ; TR; TZ; UG; ZM; ZW
EP3114680-A1: 
		      (Regional): AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LI; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; BA; ME
CP US2015255061-A1
      US20140257803-A1      
   US9324321-B2
      EP2575128-A2   APPLE INC (APPY)   BASTEA-FORTE M, BRIGHAM C D, CHEYER A J, GIULI R D, GRUBER T R, GUZZONI D R, KITTLAUS D, SADDLER H J
      EP553101-B1   LERNOUT & HAUSPIE SPEECHPRODUCTS (LERN-Non-standard)   BOURLARD H, MORGAN N
      JP2002091477-A   MITSUBISHI ELECTRIC CORP (MITQ)   OKATO Y, ISHII
      KR2013022513-A   ELECTRONICS&TELECOM RES INST (ETRI)   SONG H J
      US6167377-A   DRAGON SYSTEMS INC (NUAN)   GILLICK L S, GOULD J M, ROTH R, VAN MULBREGT P A, BIBEAULT M D
      US20020083041-A1      
      US20030125948-A1      
      US20040088726-A1      
      US20050065789-A1      
      US20070038436-A1      
      US20070156392-A1      
      US20070203863-A1      
      US20070226649-A1      
      US20080004877-A1      
      US20080235017-A1      
      US20090030697-A1      
      US20090292687-A1      
      US20100004930-A1      
      US20100128863-A1      
      US20100211695-A1      
      US20100312546-A1      
      US20110010171-A1      
      US20110093459-A1      
      US20110144999-A1      
      US20110153324-A1      
      US20120065976-A1      
      US20120084086-A1      
      US20120232885-A1      
      US20120253802-A1      
      US20120254086-A1      
      US20120254227-A1      
      US20120265531-A1      
      US20120271617-A1      
      US20120290293-A1      
      US20120303565-A1      
      US20130031476-A1      
      US20130085756-A1      
      US20130138436-A1      
      US20130152092-A1      
      US20130185065-A1      
      US20140025380-A1      
      US20140257803-A1      
      US20140358537-A1      
      US20140372112-A1      
      US20140379326-A1      
      US20140379353-A1      
      US20150100312-A1      
      US20150161993-A1      
      US20150161994-A1      
      US20150255069-A1      
      US20150278191-A1      
      US20150310858-A1      
      US20150325236-A1      
      US6185528-B1   CSELT CENT STUDI LAB TELECOM SPA (CSEL)   FISSORE L, GEMELLO R, RAVERA F
      US6263308-B1   MICROSOFT CORP (MICT)   HECKERMAN D E, ALLEVA F A, ROUNTHWAITE R L, ROSEN D, HWANG M, YAACOVI Y, MANFERDELLI J L
      US7835910-B1   AT&T INTELLECTUAL PROPERTY II LP (AMTT)   HAKKANI-TUR D Z, TUR G
      US8321220-B1   AT&T INTELLECTUAL PROPERTY II LP (AMTT)   CHOTIMONGKOL A, HAKKANI-TUR D Z, TUR G
      US8346563-B1   ARTIFICIAL SOLUTIONS LTD (ARTI-Non-standard)   CHEADLE M, GUELSDORFF B, GUSTAVII E B, HJELM D, KRUEGER R
      US6970947-B2   INT BUSINESS MACHINES CORP (IBMC)   EBLING M R, HUNT G D H, LEI H
      US8015006-B2   KENNEWICK R A (KENN-Individual);  LOCKE D (LOCK-Individual);  KENNEWICK M R (KENN-Individual);  KENNEWICK R (KENN-Individual);  FREEMAN T (FREE-Individual)   KENNEWICK R A, LOCKE D, KENNEWICK M R, KENNEWICK R, FREEMAN T
      US8229729-B2   SARIKAYA R (SARI-Individual);  DENG Y (DENG-Individual);  KINGSBURY B E D (KING-Individual);  GAO Y (GAOY-Individual)   DENG Y, GAO Y, KINGSBURY B E D, SARIKAYA R
      US8275615-B2   KOZAT S S (KOZA-Individual);  SARIKAYA R (SARI-Individual)   KOZAT S S, SARIKAYA R
      US8296107-B2   ASPEN TECHNOLOGY INC (ASPE-Non-standard)   TURNER P, GUIVER J P, LINES B, TREIBER S S
      US8326634-B2   VOICEBOX TECHNOLOGIES INC (VOIC-Non-standard)   CRISTO P D, KE M, KENNEWICK R A, ARMSTRONG L E
      US8400332-B2   FORD GLOBAL TECHNOLOGIES INC (FORD)   SZWABOWSKI S J, MACNEILLE P R
      US8412521-B2   MULTIMODAL TECHNOLOGIES INC (MULT-Non-standard);  YEGNANARAYANAN G (YEGN-Individual);  FRITSCH J (FRIT-Individual)   YEGNANARAYANAN G, FRITSCH J, MATHIAS L
      US8571866-B2   AT & T INTELLECTUAL PROPERTY I LP (AMTT)   MELAMED D, BANGALORE S, JOHNSTON M
      WO2005013262-A1   PHILIPS INTELLECTUAL PROPERTY GMBH (PHIG);  KONINK PHILIPS ELECTRONICS NV (PHIG)   PORTELE T, THIELE F
      WO2013171481-A2   TOUCHTYPE LTD (TOUC-Non-standard)   BELL M, FREEMAN J, HATEGAN E G, MEDLOCK B
   CN106104673-A
      CN103049792-A   MICROSOFT CORP (MICT)   YU D, DENG L, SEIDE F T B, LI G
      CN103400577-A   BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BAID-Non-standard)   YIN Z, SU D
      CN103456299-A   BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BAID-Non-standard)   CAO L
      EP2619713-A1   UNIV AVIGNON & PAYS VAUCLUSE (UYAV-Non-standard);  TELEFONICA (TEFO)   ANGUERA MIRO X, BONASTRE J
      JP2012118668-A   NAT INST INFORMATION & COMMUNICATIONS TE (NICT)   WATANABE H
      US8527276-B1   GOOGLE INC (GOOG)   ANDREW W, CHUN B, SCHUSTER M
      WO2013149123-A1   UNIV OHIO STATE (OHIS)   WANG Y, WANG D
CR    US9324321-B2
      U.S. Appl. No. 14/265,110, filed Apr. 29, 2014, entitled "Shared Hidden Layer Combination for Speech Recognition Systems".
      U.S. Appl. No. 14/273,100, filed May 8, 2014, entitled "Context Specific Language Model Scale Factors".
      Gruenstein, et al., "Context-Sensitive Language Modeling for Large Sets of Proper Nouns in Multimodal Dialogue Systems", In Proceedings of IEEE/ACL Workshop on Spoken Language Technology, Dec. 10, 2006, 4 pages.
      Heck et al.; "Robustness to Telephone Handset Distortion in Speaker Recognition by Discriminative Feature Design"; In Journal of Speech Communication_Speaker Recognition and its Commercial and Forensic Applications, vol. 31, Issue 2-3; Jun. 2000; http://rmcet.com/lib/E-Journals/Speech%20Communication/1-s2.0-S0167638399000771-main.pdf; 12 pgs.
      Konig et al., "Nonlinear Discriminant Feature Extraction for Robust Text-Independent Speaker Recognition"; In Proceeding of the RLA2C, ESCA workshop on Speaker Recognition and its Commercial and Forensic Applications; Apr. 1998; http://www.msr-waypoint.com/pubs/193653/konig_heck/DNN.pdf; 4 pgs.
      Lecouteux et al., "Dimensionality Reduction for Speech Recognition Using Neighborhood Components Analysis"; In Proceedings of 8th Annual Conference of the International Speech Communication Association, Antwerp; Dec. 27, 2007; http://www.cs.columbia.edu/"mcollins/papers/icslp07.pdf; 4 pgs.
      Lecouteux et al., "Dynamic Combination of Automatic Speech Recognition Systems by Driven Decoding"; In Journal of IEEE Transactions on Audio, Speech and Language Processing; Jan. 2013; http://hal.archives-ouvertes.fr/docs/00/75/86/26/PDF/SystemCombination.pdf; 10 pgs.
      Li et al., "Lattice Combination for Improved Speech Recognition", In Proceedings of the 7th International Conference of Spoken Language Processing; Sep. 16, 2002; http://www.cs.cmu.edu/afs/cs/user/robust/www/Papers/icslp02_xiang.pdf; 4 pgs.
      Liu, et al., "Use of Contexts in Language Model Interpolation and Adaptation", In Journal of Computer Speech and Language vol. 27 Issue 1, Feb. 2009, 23 pages.
      Meinedo et al., "Combination of Acoustic Models in Continuous Speech Recognition Hybrid Systems"; In Proceedings of Sixth International Conference on Spoken Language Processing: Oct. 2000: http://www.inesc-id.pt/pt/indicadores/Ficheiros/416.pdf; 4 pgs.
      Motlicek et al., "Feature and Score Level Combination of Subspace Gaussinasin LVCSR Task"; In IEEE International Conference on Acoustics, Speech and Signal Processing; May 26, 2013; http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6639142: 5 pgs.
      Saluja, et al., "Context-aware Langauage Modeling for Conversational Speech Translation", In Proceedings of Machine Translation Summit XIII, Sep. 19, 2011, 8 pages.
      Su et al., "Error Back Propagation for Sequence Training of Context-Dependent Deep Networks for Conversational Speech Transcription"; In IEEE International Conference on Acoustics, Speech, and Signal Processing; May 26, 2013: http://research.microsoft.com/pubs/194345/0006664.pdf; 5 pgs.
      Swietojanski et al., "Revisiting Hybrid and GMM-HMM System Combination Techniques"; In Proceeding of the IEEE International Conference on Acoustics, Speech and Signal Processing; May 26, 2013; http://homepages.inf.ed.ac.uk/s1136550/data/Swietojanski_ICASSP2013.pdf; 5 pgs.
      Yan et al., "A Scalable Approach to Using DSS-Derived Features in GMM-HMM Based Acoustic Modeling for LVCSR"; In Proceeding of the 14th Annual Conference of the International Speech Communication Assoctation; Aug. 25, 2013; http://homepages.inf.ed.ac.uk/s1136550/data/Swietojanski_ICASSP2013.pdf; 5 pgs.
      Yu, et al., "Roles of Pre-Training and Fine-Tuning is Context-Dependent DBN-HMMs for Real-Word Speech Recognition", In Proceeding of NIPS Workshop on Deep Learning and Unsupervised Feature Learning, Dec. 2010, 8 pages.
      Davis, et al., "Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks", In Proceedings of ArXiv preprint arXiv:1312.4461, Dec. 2013, 9 pages.
      Chen, Wei, "Building Language Model on Continuous Space using Gaussian Mixture Models", In Proceedings of Research in Language Modeling, Jan. 2007, 66 pages.
      Novak, et al., "Use of Non-Negative Matrix Factorization for Language Model Adaptation in a Lecture Transcription Task", In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 1, May 7, 2001, 4 pages.
      Barman, et al., "Nonnegative Matrix Factorization (NMF) Based Supervised Feature Selection and Adaptation", In Proceedings of the 9th International Conference on Intelligent Data Engineering and Automated Learning, Nov. 2, 2008, 2 pages.
      Trmal, et al., "Adaptation of a Feedforward Artificial Neural Network Using a Linear Transform", In Proceedings of in Text, Speech and Dialogue, Sep. 10, 2010, pp. 8.
      Liao, Hank, "Speaker Adaptation of Context Dependent Deep Neural Networks", In IEEE International Conference on Acoustics, Speech and Signal Processing, May 26, 2013, 5 pages.
      Dahl, et al., "Context-Dependent Pre-Trained Deep Neural Networks for Large Vocabulary Speech Recognition", In IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, Issue 1, Jan. 1, 2012, 13 pages.
      Jaitly, et al., "An Application of Pretrained Deep Neural Networks to Large Vocabulary Conversational Speech Recognition", In Proceedings of 13th Annual Conference of the International Speech Communication Association, Mar. 12, 2012, 11 pages.
      Sainath, et al., "Making Deep Belief Networks Effective for Large Vocabulary Continuous Speech Recognition", In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding, Dec. 11, 2011, 6 pages.
      Dahl, et al., "Large Vocabulary Continuous Speech Recognition with Context-Dependent DBN-HMMs", In IEEE International Conference on Acoustics, Speech and Signal Processing, May 22, 2011, 4 pages.
      Mohamed, et al., "Acoustic Modeling Using Deep Belief Networks", In IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, Issue 1, Jan. 2012, 10 pages.
      Yu, et al., "Improved Bottleneck Features Using Pretrained Deep Neural Networks", In Proceedings of 12th Annual Conference of the International Speech Communication Association, Aug. 28, 2011, 4 pages.
      Sainath, et al., "Auto-Encoder Bottleneck Features Using Deep Belief Networks", In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, Mar. 25, 2012, 4 pages.
      Yao, et al., "Adaptation of Context-Dependent Deep Neural Networks for Automatic Speech Recognition", In IEEE Spoken Language Technology Workshop, Dec. 2, 2012, 4 pages.
      Li, et al., "Comparison of Discriminative Input and Output Transformations for Speaker Adaptation in the Hybrid NN/HMM Systems", In Proceedings of 11th Annual Conference of the International Speech Communication Association, Sep. 26, 2010, 4 pages.
      Gemello, et al., "Adaptation of Hybrid ANN/HMM Models Using Linear Hidden Transformations and Conservative Training", In IEEE International Conference on Acoustics, Speech and Signal Processing, May 14, 2006, 4 pages.
      Siniscalchi, et al., "Hermitian Based Hidden Activation Functions for Adaptation of Hybrid HMM/ANN Models", In Proceedings of 13th Annual Conference of the International Speech Communication Association, Sep. 9, 2012, 4 pages.
      Yu, et al., "KL-Divergence Regularized Deep Neural Network Adaptation for Improved Large Vocabulary Speech Recognition", In IEEE International Conference on Acoustics, Speech and Signal Processing, May 26, 2013, 5 pages.
      Abdel-Hamid, et al., "Fast Speaker Adaptation of Hybrid NN/HMM Model for Speech Recognition Based on Discriminative Learning of Speaker Code", In IEEE International Conference on Acoustics, Speech and Signal Processing, May 26, 2013, 5 pages.
      Sainath, et al., "Low-Rank Matrix Factorization for Deep Neural Network Training with High-Dimensional Output Targets", In proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, May 26, 2013, 5 pages.
      Xue, et al., "Restructuring of Deep Neural Network Acoustic Models with Singular Value Decomposition", In Proceedings of 14th Annual Conference of the International Speech Communication Association, Aug. 25, 2013, 5 pages.
      U.S. Appl. No. 13/920,323, "Restructuring Deep Neural Network Acoustic Models", Filed Date: Jun. 18, 2013.
      Abid, et al., "A New Neural Network Pruning Method Based on the Singular Value Decomposition and the Weight Initialisation", In Proceedings of 11th European Signal Processing Conference, Sep. 3, 2002, 4 pages.
      Abad, et al., "Context Dependent Modelling Approaches for Hybrid Speech Recognizers", In Proceeding of Interspeech, Sep. 26, 2010, 4 pages.
      Hinton, et al., "Deep Neural Networks for Acoustic Modeling in Speech Recognition", In IEEE Signal Processing Magazine, vol. 29, Issue 6, Nov. 2012, 27 pages.
      Deng, et al., "Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-Word Speech Recognition", In Proceeding of NIPS Workshop on Deep Learning and Unsupervised Feature Learning, Dec. 2010, 8 pages.
      Seide, et al., "Conversational Speech Transcription using Context-Dependent Deep Neural Networks", In Proceeding of 12th Annual Conference of the International Speech Communication Association, Aug. 28, 2011, 4 pages.
      Yu, et al., "Exploiting Sparseness in Deep Neural Networks for Large Vocabulary Speech Recognition", In Proceeding of IEEE International Conference on Acoustics, Speech and Signal Processing, Mar. 25, 2012, 4 pages.
      Vanhoucke, et al., "Improving the Speed of Neural Networks on CPUs", In Proceedings of NIPS Workshop on Deep Learning and Unsupervised Feature Learning, Dec. 16, 2011, 8 pages.
      International Search Report and Written Opinion Issued in PCT Application No. PCT/US2014/041014, Mailed Date: Oct. 2, 2014, 9 Pages.
      International Search Report and Written Opinion Issued in PCT Application No. PCT/US2014/041023, Mailed Date: Jun. 3, 2015, 17 Pages.
      International Search Report and Written Opinion Issued in PCT Application No. PCT/US2015/017872, Mailed Date: Jun. 25, 2015, 11 Pages.
      International Search Report and Written Opinion Issued in PCT Application No. PCT/US2015/029334, Mailed Date: Jul. 7, 2015, 12 Pages.
      Bohus, et al., "Olympus: An Open-Source Framework for Conversational Spoken Language Interface Research", In Proceedings of the Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies, Apr. 2007, 8 pages.
      Chandrasekaran et al., "Sparse and Low-Rank Matrix Decompositions"; IFAC Symposium on System Identification, 2009; 6 pgs.
      Dos Reis Mota, Pedro Jose, "LUP: A Language Understanding Platform", A Dissertation for the Degree of Master of Information Systems and Computer Engineering, Jul. 2012, 128 pages.
      Eagle, et al., "Common Sense Conversations: Understanding Casual Conversation using a Common Sense Database", In Proceedings of the Artificial Intelligence, Information Access, and Mobile Computing Workshop, Aug. 2003, 6 pages.
      He et al; "What is Discriminative Learning"; Achorn International; Jun. 25, 2008; 25 pgs.
      Hoffmeister et al., "Log-Linear Model Combination with Word-Dependent Scaling Factors"; Human Language Technology and Pattern Recognition Computer Science Department; Sep. 6-10; Brighton UK; Copyright (c) 2009 ISCA; 4 pgs.
      Huang et al., "Unified Stochastic Engine (USE) for Speech Recognition"; School of Computer Science; 1993 IEEE; 4 pgs.
      "Integrated Development Environments for Natural Language Processing", Published on: Oct. 2001, Available at: http://www.textanalysis.com/TAI-IDE-WP.pdf; 13 pgs.
      Keshtkar et al., "A Corpus-based Method for Extracting Paraphrases of Emotion Terms"; Proceedings of the NAACL HLT 2010 Workshop on Computational Appraoches to Analysis and Generation of Emotion in Text; Jun. 2010; 10 pgs.
      Ko, et al., "Cammia_A Context-Aware Spoken Dialog System for Mobile Environments", In Automatic Speech Recognition and Understanding Workshop, Jul. 29, 2011, 2 pages.
      Lee, et al., "Intention-Based Corrective Feedback Generationusing Context-Aware Model", In Proceedings of the Second International Conference on Computer Supported Education, Apr. 7, 2010, 8 pages.
      Moreira et al., "Towards the Rapid Development of a Natural Language Understanding Module", In Proceedings of the 10th International Conference on Intelligent Virtual Agengts, Jan. 2011, 7 pages.
      Sarukkai, et al., "Improved Spontaneous Dialogue Recognition Using Dialogue and Utterance Triggers by Adaptive Probability Boosting", In Fourth International Conference on Spoken Language, vol. 1, Oct. 3, 1996, 4 pages.
      Sarukkai et al., "Word Set Probability Boosting for Improved Spontaneous Dialog Recognition"; IEEE Transactions on Speech and Audio Processing, vol. 5, No. 5, Sep. 1997; 13 pgs.
      Seneff, et al., "Galaxy-II: A Reference Architecture for Conversational System Development", In Proceedings of the 5th International Conference on Spoken Language Processing, Nov. 2008, 4 pages.
      Sing, et al., "Domain Metric Knowledge Model for Embodied Conversation Agents", In 5th International Conference on Research, Innovation & Vision for the Future, Mar. 5, 2007, 7 pages.
      Xue, et al., "Singular Value Decomposition Based Low-Footprint Speaker Adaptation and Personalization for Deep Neural Network", In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, May 4, 2014, pp. 6359-6363.
      U.S. Official Action dated May 6, 2015 in U.S. Appl. No. 13/923,969, 12 pgs.
      Response dated May 21, 2015 in U.S. Appl. No. 13/920,323, 15 pgs.
      U.S. Official Action dated May 28, 2015 in U.S. Appl. No. 13/923,917, 18 pgs.
      Response dated Aug. 6, 2015 in U.S. Appl. No. 13/923,969, 7 pgs.
      Response dated Aug. 7, 2015 in U.S. Appl. No. 13/923,917, 10 pgs.
      U.S. Official Action dated Sep. 24, 2015 in U.S. Appl. No. 13/920,323, 25 pgs.
      U.S. Appl. No. 14/227,492, filed Mar. 27, 2014, entitled "Flexible Schema for Language Model Customization".
      Liu, et al., "Language Model Combination and Adaptation using Weighted Finite State Transducers", In Proceedings of IEEE International Conference on Acoustics Speech and Signal Processing, Mar. 14, 2010, 4 pages.
      Preliminary Report on Patentability dated Sep. 15, 2015 in Appln No. PCT/US2014/041014, 6 pgs.
      U.S. Official Action dated Aug. 13, 2015 in U.S. Appl. No. 14/227,492, 41 pgs.
      U.S. Official Action dated Sep. 29, 2015 in U.S. Appl. No. 13/923,917, 9 pgs.
      U.S. Official Action dated Oct. 1, 2015 in U.S. Appl. No. 13/920,323, 34 pgs.
      Notice of Allowance dated Oct. 1, 2015 in U.S. Appl. No. 13/923,969, 7 pgs.
      Notice of Allowance dated Nov. 30, 2015 in U.S. Appl. No. 13/923,969, 12 pgs.
      Lilly, et al., "Robust Speech Recognition Using Singular Value Decomposition Based Speech Enhancement," IEEE Tencon, 1997, 4 pgs.
      U.S. Official Action dated Feb. 27, 2015 in U.S. Appl. No. 13/920,323, 22 pgs.
      Preliminary Report on Patentability dated Nov. 13, 2015 in Appln No. PCT/US2014/041023, 7 pgs.
      Preliminary Report on Patentability dated Dec. 14, 2015 in Appln No. PCT/US2015/017872, 7 pgs.
      Notice of Allowance dated Dec. 15, 2015 in U.S. Appl. No. 13/923,917, 11 pgs.
      U.S. Official Action dated Jan. 4, 2016 in U.S. Appl. No. 13/920,323, 49 pgs.
      U.S. Official Action dated Mar. 3, 2016 in U.S. Appl. No. 13/920,323, 24 pgs.
      Notice of Allowance dated Mar. 4, 2016 in U.S. Appl. No. 13/923,917, 28 pgs.
      PCT Written Opinion dated Feb. 11, 2016 in Appln No. PCT/US2015/021921, 7 pgs.
   CN106104673-A
      YU DONG ET AL: "KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition", 2013 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP);VANCOUCER, BC; 26-31 MAY 2013, INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS,PISCATAWAY,relevantClaims[1-10],relevantPassages[7893-7897]
      GEOFFREY HINTON ET AL: "Deep Neural Networks for Acoustic Modeling in Speech Recognition:The Shared Views of Four Research Groups", IEEE SIGNAL PROCESSING MAGAZINE, IEEE SERVICE CENTER, PISCATAWAY,relevantClaims[1-10],relevantPassages[82-97]
      JIAN XUE ET AL: "Restructuring of Deep Neural Network Acoustic Modela with Singular Value Decomposition", PROC.INTERSPEECH 2013,relevantClaims[1-10],relevantPassages[2365-2369]
UT DIIDW:2015528318
ER

PT P
PN US2015006443-A1; WO2014210368-A1; EP3014534-A1; CN105531725-A; JP2016531343-W; EP3014534-A4; US9727824-B2; CN105531725-B; CN108256651-A; JP6465876-B2
TI Method for analyzing electroencephalographic data to identify maximally repeating pattern in data, involves setting values for dictionary of objective function, and interacting with quantum processor to minimize function.
AU ROSE G
   GILDERT S
   MACREADY W G
   WALLIMAN D C
   MACREADY W
   WALLIMAN D
AE ROSE G (ROSE-Individual)
   GILDERT S (GILD-Individual)
   MACREADY W G (MACR-Individual)
   WALLIMAN D C (WALL-Individual)
   D-WAVE SYSTEMS INC (DWAV-Non-standard)
   DWAVE SYSTEM INC (DWAV-Non-standard)
   D-WAVE SYSTEMS INC (DWAV-Non-standard)
   DWAVE SYSTEM INC (DWAV-Non-standard)
   D-WAVE SYSTEMS INC (DWAV-Non-standard)
GA 201500530B
AB    NOVELTY - The method (500) involves formulating (503) an objective function based on a preprocessed data set through a non-quantum processor, where the function includes a regularization term to minimize complications in the function. A set of weights in the function is casted as variables by using the processor. A set of values for a dictionary of the function is set by using the processor, where the values are constrained such that the function matches a connectivity structure of a quantum processor, which is interacted through the non-quantum processor to minimize the function (504).
   USE - Method for analyzing electroencephalographic data using a quantum processor to identify a maximally repeating pattern in data through a HDL over a communication network. Uses include but are not limited to telecommunication network, cellular network, paging network, mobile network and wide area network.
   ADVANTAGE - The method enables utilizing the quantum processor to minimize sparse least squares objective transmitted to an appropriate system such as mechanical system or robotic system, so that the system is enabled to extract user-specified instructions from the data set while improving the performance by sparse coding and increasing overall optimization.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for identifying maximally repeating patterns in data through a hierarchical deep learning (HDL)
   (2) a processor-readable storage medium comprising a set of instructions to perform a method for analyzing electroencephalographic data by using a quantum processor to identify a maximally repeating pattern in data through a HDL
   (3) a method for automatically labeling data.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for analyzing electroencephalographic data using a quantum processor.
   Method for analyzing electroencephalographic data (500)
   Step for collecting data set (501)
   Step for transmitting data set to digital computer (502)
   Step for formulating objective function based on preprocessed data set through non-quantum processor (503)
   Step for interacting quantum processor with non-quantum processor (504)
DC T01 (Digital Computers)
MC T01-E05Q; T01-J04; T01-J11A1; T01-M06Q; T01-N01B3; T01-S03
IP G06F-017/10; G06N-099/00; G06K-009/00; G06K-009/62; G06N-010/00; G06N-020/00
PD US2015006443-A1   01 Jan 2015   G06N-099/00   201505   Pages: 76   English
   WO2014210368-A1   31 Dec 2014   G06N-099/00   201505      English
   EP3014534-A1   04 May 2016   G06N-099/00   201630      English
   CN105531725-A   27 Apr 2016   G06N-099/00   201634      English
   JP2016531343-W   06 Oct 2016   G06N-099/00   201666   Pages: 116   Japanese
   EP3014534-A4   22 Mar 2017   G06N-099/00   201722      English
   US9727824-B2   08 Aug 2017   G06N-099/00   201755      English
   CN105531725-B   13 Mar 2018   G06N-099/00   201820      Chinese
   CN108256651-A   06 Jul 2018   G06N-099/00   201846      Chinese
   JP6465876-B2   06 Feb 2019   G06N-099/00   201911   Pages: 102   Japanese
AD US2015006443-A1    US316372    26 Jun 2014
   WO2014210368-A1    WOUS044421    26 Jun 2014
   EP3014534-A1    EP817299    26 Jun 2014
   CN105531725-A    CN80047692    26 Jun 2014
   JP2016531343-W    JP524211    26 Jun 2014
   EP3014534-A4    EP817299    26 Jun 2014
   US9727824-B2    US316372    26 Jun 2014
   CN105531725-B    CN80047692    26 Jun 2014
   CN108256651-A    CN10128747    26 Jun 2014
   JP6465876-B2    JP524211    26 Jun 2014
FD  US2015006443-A1 Provisional Application US841129P
   US2015006443-A1 Provisional Application US873303P
   EP3014534-A1 PCT application Application WOUS044421
   EP3014534-A1 Based on Patent WO2014210368
   CN105531725-A PCT application Application WOUS044421
   CN105531725-A Based on Patent WO2014210368
   JP2016531343-W PCT application Application WOUS044421
   JP2016531343-W Based on Patent WO2014210368
   US9727824-B2 Provisional Application US841129P
   US9727824-B2 Provisional Application US873303P
   CN105531725-B PCT application Application WOUS044421
   CN105531725-B Based on Patent WO2014210368
   CN105531725-B Previous Publ. Patent CN105531725
   CN108256651-A Div ex Application CN80047692
   JP6465876-B2 PCT application Application WOUS044421
   JP6465876-B2 Based on Patent WO2014210368
   JP6465876-B2 Previous Publ. Patent JP2016531343
PI US841129P    28 Jun 2013
   US873303P    03 Sep 2013
   US316372    26 Jun 2014
   WOUS044421    26 Jun 2014
DS WO2014210368-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IR; IS; JP; KE; KG; KN; KP; KR; KZ; LA; LC; LK; LR; LS; LT; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SA; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
EP3014534-A1: 
		      (Regional): AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LI; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; BA; ME
EP3014534-A4: 
		      (Regional): AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LI; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR
FS None/
CP US2015006443-A1
      US20060047477-A1      
      US20070162406-A1      
      US20080313430-A1      
      US20090077001-A1      
      US20090278981-A1      
      US20110047201-A1      
      US20110142335-A1      
      US20110231462-A1      
      US20110238378-A1      
      US20110295845-A1      
      US20120149581-A1      
      US20120215821-A1      
      US20140025606-A1      
      US20140152849-A1      
      US20150242463-A1      
      US20150248586-A1      
      US20160307305-A1      
   WO2014210368-A1
      KR2013010181-A   UNIV HANYANG IUCF-HYU (UHYG)   SUH I, PARK Y
      US20080069438-A1      
      US20120084235-A1      
      US20130097103-A1      
      WO2010071997-A1   KIBBOKO INC (KIBB-Non-standard)   BATES K, SU J, WANG B, XU B
   CN105531725-A
      CN101473346-A   LET IT WAVE (LETI-Non-standard)   BRUNA J, MALLAT S
      CN101657827-A   ROSE G (ROSE-Individual)   ROSE G
      US20110231462-A1      
      US20110295845-A1      
      US20120149581-A1      
   US9727824-B2
      US20060047477-A1      
      US20070162406-A1      
      US20080313430-A1      
      US20090077001-A1      
      US20090278981-A1      
      US20110047201-A1      
      US20110142335-A1      
      US20110231462-A1      
      US20110238378-A1      
      US20110295845-A1      
      US20120149581-A1      
      US20120215821-A1      
      US20140025606-A1      
      US20140152849-A1      
      US20150242463-A1      
      US20150248586-A1      
      US20160307305-A1      
      KR2013010181-A   UNIV HANYANG IUCF-HYU (UHYG)   SUH I, PARK Y
      US20080069438-A1      
      US20080176750-A1      
      US20080215850-A1      
      US20090121215-A1      
      US20110022820-A1      
      US20120084235-A1      
      US20130097103-A1      
      US7135701-B2   D-WAVE SYSTEMS INC (DWAV-Non-standard)   AMIN M H S, STEININGER M F H
      US7418283-B2   D-WAVE SYSTEMS INC (DWAV-Non-standard)   AMIN M H S, STEININGER M F H
      US7533068-B2   D-WAVE SYSTEMS INC (DWAV-Non-standard)   VAN DEN BRINK A M, LOVE P, AMIN M H S, ROSE G, GRANT D, STEININGER M F H, BUNYK P
      US7876248-B2   D-WAVE SYSTEMS INC (DWAV-Non-standard)   BERKLEY A J, BUNYK P I, ROSE G
      US8008942-B2   D-WAVE SYSTEMS INC (DWAV-Non-standard)   VAN DEN BRINK A M, LOVE P, AMIN M H S, ROSE G, GRANT D, STEININGER M F H, BUNYK P
      US8035540-B2   D-WAVE SYSTEMS INC (DWAV-Non-standard)   BERKLEY A J, BUNYK P I, ROSE G
      WO2010071997-A1   KIBBOKO INC (KIBB-Non-standard)   BATES K, SU J, WANG B, XU B
      WO2009120638-A2   D-WAVE SYSTEMS INC (DWAV-Non-standard)   BUNYK P, MAIBAUM F, NEUFELD R D
   CN105531725-B
      CN101473346-A   LET IT WAVE (LETI-Non-standard)   BRUNA J, MALLAT S
      CN101657827-A   ROSE G (ROSE-Individual)   ROSE G
      US20110231462-A1      
      US20110295845-A1      
      US20120149581-A1      
CR    WO2014210368-A1
      See also references of EP 3014534A4
   US9727824-B2
      Amin, Effect of Local Minima on Adiabatic Quantum Optimization, Physical Review Letters 100, 130503, 2008, 4 pages.
      Bell et al., The Independent Components of Natural Scenes are Edge Filters, Vision Res. 37(23):3327-3338, 1997.
      Lee et al., Efficient sparse coding algorithms, NIPS, pp. 801-808, 2007.
      Lovasz et al., Orthogonal Representations and Connectivity of Graphs, Linear Algebra and its Applications 114/115:439-454, 1989.
      Lovasz et al., A correction: orthogonal representations and connectivity of graphs, Linear Algebra and its Applications 313:101-105, 2000.
      International Search Report and Written Opinion, mailed Oct. 13, 2014, for PCT/US2014/044421, 13 pages.
UT DIIDW:201500530B
ER

PT P
PN WO2014188940-A1; JP2014229124-A; CN105229676-A; EP3001358-A1; US2016110642-A1; US9691020-B2; JP6164639-B2; EP3001358-A4; CN105229676-B
TI Learning method of deep neural network used for audio recognition, involves isolating first sub-network from another sub-network, and making storage medium to store as category independent sub-network after completion of learning step.
AU MATSUDA S
   LU X
   HORI C
   KASHIOKA H
AE NAT INST INFORMATION & COMMUNICATIONS TE (NICT-C)
   NAT INST INFORMATION & COMMUNICATIONS TE (NICT-C)
GA 2014V19892
AB    NOVELTY - The method involves learning first deep neural network (DNN) who connected second sub-network to back stage of first sub-network, by learning data belongs to first category. The second DNN is learned and which connected and formed third sub-network in back stage of first sub-network by learning data belongs to second category. A computer isolates first sub-network from another sub-network, and makes storage medium to store as category independent sub-network (120) after completion of DNN learning step.
   USE - Learning method of deep neural network used for recognition techniques, such as audio or voice recognition and image recognition.
   ADVANTAGE - Since the computer isolates first sub-network from another sub-network, and makes storage medium to store as category independent sub-network after completion of DNN learning step, the DNN is learned by reducing time for learning DNN using data belonging to several categories.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a learning apparatus of deep neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of fundamental structure of DNN.
   Neuron (100)
   Independent sub-network (120)
   Japanese dependence sub-network (122)
   English dependence sub-network (124)
DC P86 (Musical instruments, acoustics (G10).); T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-C08A; T01-J10B2A; T01-J16B; T01-J16C1; T01-N01B3; W04-V
IP G06N-003/04; G06N-003/08; G10L-015/16; G06N-003/00
PD WO2014188940-A1   27 Nov 2014   G06N-003/04   201480   Pages: 42   Japanese
   JP2014229124-A   08 Dec 2014   G06N-003/04   201480   Pages: 19   Japanese
   CN105229676-A   06 Jan 2016   G06N-003/04   201605      English
   EP3001358-A1   30 Mar 2016   G06N-003/04   201624      English
   US2016110642-A1   21 Apr 2016   G06N-003/08   201628      English
   US9691020-B2   27 Jun 2017   G06N-003/04   201744      English
   JP6164639-B2   19 Jul 2017   G06N-003/04   201749   Pages: 18   Japanese
   EP3001358-A4   26 Jul 2017   G10L-015/16   201750      English
   CN105229676-B   23 Nov 2018   G06N-003/04   201881      Chinese
AD WO2014188940-A1    WOJP062911    15 May 2014
   JP2014229124-A    JP109061    23 May 2013
   CN105229676-A    CN80029326    15 May 2014
   EP3001358-A1    EP801131    15 May 2014
   US2016110642-A1    US14787903    29 Oct 2015
   US9691020-B2    US14787903    29 Oct 2015
   JP6164639-B2    JP109061    23 May 2013
   EP3001358-A4    EP801131    15 May 2014
   CN105229676-B    CN80029326    15 May 2014
FD  CN105229676-A PCT application Application WOJP062911
   CN105229676-A Based on Patent WO2014188940
   EP3001358-A1 PCT application Application WOJP062911
   EP3001358-A1 Based on Patent WO2014188940
   US2016110642-A1 PCT application Application WOJP062911
   US9691020-B2 PCT application Application WOJP062911
   US9691020-B2 Based on Patent WO2014188940
   CN105229676-B PCT application Application WOJP062911
   CN105229676-B Based on Patent WO2014188940
   CN105229676-B Previous Publ. Patent CN105229676
PI JP109061    23 May 2013
   WOJP062911    15 May 2014
   CN80029326    20 Nov 2015
DS WO2014188940-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IR; IS; KE; KG; KN; KP; KR; KZ; LA; LC; LK; LR; LS; LT; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SA; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
EP3001358-A1: 
		      (Regional): AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LI; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR; BA; ME
EP3001358-A4: 
		      (Regional): AL; AT; BE; BG; CH; CY; CZ; DE; DK; EE; ES; FI; FR; GB; GR; HR; HU; IE; IS; IT; LI; LT; LU; LV; MC; MK; MT; NL; NO; PL; PT; RO; RS; SE; SI; SK; SM; TR
CP WO2014188940-A1
      JP06309293-A      
      JP10063632-A   MITSUBISHI ELECTRIC CORP (MITQ)   ISHIZUKA Y
      JP2002520719-A   SIEMENS AG (SIEI)   STERZING V
      US20090204558-A1      
   CN105229676-A
      CN103049792-A   MICROSOFT CORP (MICT)   YU D, DENG L, SEIDE F T B, LI G
      CN103117060-A   CHINESE ACAD SCI ACOUSTICS INST (CAAI);  BEIJING KEXIN TECHNOLOGY CO LTD (BEIJ-Non-standard)   PAN J, YAN Y, XIAO Y
      EP2221805-A1   HARMAN BECKER AUTOMOTIVE SYSTEMS BECKER (HRMN)   VASQUEZ D, GUILLERMO A, GRUHN R
      US6421654-B1   COMMISSARIAT ENERGIE ATOMIQUE (COMS);  CENT NAT RECH SCI (CNRS)   GORDON M B
   US9691020-B2
      CN103049792-A   MICROSOFT CORP (MICT)   YU D, DENG L, SEIDE F T B, LI G
      CN103117060-A   CHINESE ACAD SCI ACOUSTICS INST (CAAI);  BEIJING KEXIN TECHNOLOGY CO LTD (BEIJ-Non-standard)   PAN J, YAN Y, XIAO Y
      EP2221805-A1   HARMAN BECKER AUTOMOTIVE SYSTEMS BECKER (HRMN)   VASQUEZ D, GUILLERMO A, GRUHN R
      JP10063632-A   MITSUBISHI ELECTRIC CORP (MITQ)   ISHIZUKA Y
      US20130138436-A1      
      US6421654-B1   COMMISSARIAT ENERGIE ATOMIQUE (COMS);  CENT NAT RECH SCI (CNRS)   GORDON M B
      WO2000003355-A2      
CR    US2016110642-A1
      T. Caelli et al., "Modularity in Neural Computing", Proc. IEEE, vol. 87, no. 9, sept. 1999, pp. 1497-1518.
   US9691020-B2
      T. Caelli et al., Modularity in Neural Computing, Proc. IEEE, vol. 87, No. 9, Sep. 1999, pp. 1497-1518.
      International Search report for corresponding International Application No. PCT/JP2014/062911 mailed Jul. 1, 2014.
      Y. Bengio, Learning Deep Architectures for Al, Foundations and Trends in Machine Learning, vol. 2, No. 1, pp. 1-127, 2009 (cited in the specification).
      Hinton et al., Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups, IEEE Signal Processing Magazine, vol. 29, No. 6, pp. 82-97, 2012 (cited in the specification).
      Mohamed et al., Acoustic Modeling using Deep Belief Networks, IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, No. 1, pp. 14-22, 2012 (cited in the specification).
      Le et al., Building High-level Features Using Large Scale Unsupervised Learning, Proc. ICML, 2012 (cited in the specification).
UT DIIDW:2014V19892
ER

PT P
PN CN104102929-A; CN104102929-B
TI Method for classifying high-spectral remote sensing data based on depth learning, involves extracting characteristics of hyperspectral data based on classification of deep learning, and classifying high-spectral remote sensing data.
AU CHEN Y
   WANG Q
   ZHAO X
   SHI C
AE HARBIN INST TECHNOLOGY (HAIT-C)
   HARBIN INST TECHNOLOGY (HAIT-C)
GA 2015012648
AB    NOVELTY - The method involves reading original high spectrum original data, obtaining high spectrum data of original characteristic value and characteristic vector, and achieving high spectrum data of original spatial signature information. The characteristics of hyperspectral data is extracted based on the classification of deep learning. Fine-tune into a multi-Boltzmann limit network is obtained using training samples for multi-agency into restricted Boltzmann network by supervised learning network, and high-spectral remote sensing data is classified.
   USE - Method for classifying high-spectral remote sensing data based on depth learning used in image processing application.
   ADVANTAGE - The more efficient extraction of the characteristics of hyperspectral data is obtained based on the classification of deep learning, and the non-linear data characteristics on data classification accuracy result is reduced, and hence accurate data classification result is obtained effectively so as to apply to subsequent image processing.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the process of classifying high-spectral remote sensing data based on depth learning.(Drawing includes non-English language text)
DC T01 (Digital Computers)
MC T01-J05B2; T01-J05B4P; T01-J10B2; T01-J16C2; T01-N01B3; T01-N02B1B
IP G06K-009/66
PD CN104102929-A   15 Oct 2014   G06K-009/66   201503   Pages: 10   Chinese
   CN104102929-B   03 May 2017   G06K-009/66   201731      Chinese
AD CN104102929-A    CN10359935    25 Jul 2014
   CN104102929-B    CN10359935    25 Jul 2014
FD  CN104102929-B Previous Publ. Patent CN104102929
PI CN10359935    25 Jul 2014
CP CN104102929-A
      CN101976361-A   UNIV CHINA MINING & TECHNOLOGY XUZHOU (UYMT)   TAN K, DU P
      CN102024153-A   UNIV XIDIAN (UYXN)   ZHANG X, WANG S, YU X, JIAO L, HOU B, MA W, DENG Q, SHANG R
      CN102096825-A   UNIV XIDIAN (UYXN)   YANG S, ZHANG X, LI Y, JIAO L, HOU B, MA W, LIU R, WEI Z
   CN104102929-B
      CN101976361-A   UNIV CHINA MINING & TECHNOLOGY XUZHOU (UYMT)   TAN K, DU P
      CN102024153-A   UNIV XIDIAN (UYXN)   ZHANG X, WANG S, YU X, JIAO L, HOU B, MA W, DENG Q, SHANG R
      CN102096825-A   UNIV XIDIAN (UYXN)   YANG S, ZHANG X, LI Y, JIAO L, HOU B, MA W, LIU R, WEI Z
UT DIIDW:2015012648
ER

PT P
PN CN103914841-A; CN103914841-B
TI Ultra-pixel depth study based vaginal bacteria segmentation and classification method, involves comparing standard classification process and depth learning classifier, and producing output result according to similarities.
AU CHEN S
   NI D
   WANG T
   ZENG Z
   LEI B
   SONG Y
AE UNIV SHENZHEN (UYSZ-C)
GA 2014R98465
AB    NOVELTY - The method involves containing a depth learning classifier in a test image. A GRB channel is selected as a median filter of comparison process. Noise present in an ultra-pixel is removed. A size value is determined for ultra-pixel area by a primary filter to determine candidate bacteria area. Characteristic extraction process is performed on the candidate bacteria area. Standard classification process and the depth learning classifier are compared according to the characteristic extraction process. An output result is produced according to similarities in the comparison process.
   USE - Ultra-pixel depth study based vaginal bacteria segmentation and classification method.
   ADVANTAGE - The method enables utilizing ultra-pixel calculation process and depth learning process so as to realize high visibility, low cost, simple operation and easy popularization of classification.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an ultra-pixel depth study based vaginal bacteria segmentation and classification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an ultra-pixel depth study based vaginal bacteria segmentation and classification method.'(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-E01C; T01-J05B2; T01-J10B2; T01-J30A; T04-D03A; T04-D04
IP G06K-009/62; G06T-007/00; G06T-007/12
PD CN103914841-A   09 Jul 2014   G06T-007/00   201465   Pages: 14   Chinese
   CN103914841-B   09 Mar 2018   G06T-007/12   201819      Chinese
AD CN103914841-A    CN10133805    03 Apr 2014
   CN103914841-B    CN10133805    03 Apr 2014
FD  CN103914841-B Previous Publ. Patent CN103914841
PI CN10133805    03 Apr 2014
CP CN103914841-A
      CN101900737-A   UNIV SHANGHAI SCI & TECHNOLOGY (USHS)   ZHENG J, FAN X, CHEN Z
      CN102750544-A   ZHEJIANG ICARE VISION TECH CO LTD (ZHEJ-Non-standard)   GAO Y, JIANG Z, SHANG L, WANG H, YU X, ZHENG X, ZHENG Y
      CN103093470-A   UNIV TIANJIN (UTIJ)   WAN L, ZHANG S, FENG W, ZHANG J, JIANG J
      CN103164858-A   UNIV ZHEJIANG (UYZH)   YU H, CAI D
      CN103353938-A   UNIV SHANDONG (USHA)   WANG L, WANG S, YANG G, YIN Y, ZHANG C
      CN103400368-A   UNIV XIDIAN (UYXN)   JIAO L, MA J, ZHANG X, MA W, WANG S, HOU B, GONG D
      CN103544697-A   UNIV NANJING INFORMATION SCI & TECHNOLOG (UNAI)   DENG J, LIU Q, SUN Y, WANG C
      CN103679719-A   UNIV HOHAI (UYHO)   WANG M
      US8526723-B2   LOS ALAMOS NAT SECURITY LLC (USGO)   PRASAD L, SWAMINARAYAN S
      WO2013037357-A1   EADS DEUT GMBH (EADS)   SCHERTLER K, LIEBELT J
   CN103914841-B
      CN101900737-A   UNIV SHANGHAI SCI & TECHNOLOGY (USHS)   ZHENG J, FAN X, CHEN Z
      CN102750544-A   ZHEJIANG ICARE VISION TECH CO LTD (ZHEJ-Non-standard)   GAO Y, JIANG Z, SHANG L, WANG H, YU X, ZHENG X, ZHENG Y
      CN103093470-A   UNIV TIANJIN (UTIJ)   WAN L, ZHANG S, FENG W, ZHANG J, JIANG J
      CN103164858-A   UNIV ZHEJIANG (UYZH)   YU H, CAI D
      CN103353938-A   UNIV SHANDONG (USHA)   WANG L, WANG S, YANG G, YIN Y, ZHANG C
      CN103400368-A   UNIV XIDIAN (UYXN)   JIAO L, MA J, ZHANG X, MA W, WANG S, HOU B, GONG D
      CN103544697-A   UNIV NANJING INFORMATION SCI & TECHNOLOG (UNAI)   DENG J, LIU Q, SUN Y, WANG C
      CN103679719-A   UNIV HOHAI (UYHO)   WANG M
      US8526723-B2   LOS ALAMOS NAT SECURITY LLC (USGO)   PRASAD L, SWAMINARAYAN S
      WO2013037357-A1   EADS DEUT GMBH (EADS)   SCHERTLER K, LIEBELT J
CR CN103914841-A
      &#36213;&#26126;&#29664;: "&#32454;&#32990;&#30149;&#29702;&#22270;&#20687;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#20998;&#31867;&#35782;&#21035;", &#12298;&#20013;&#22269;&#20248;&#31168;&#30805;&#22763;&#23398;&#20301;&#35770;&#25991;&#20840;&#25991;&#25968;&#25454;&#24211; &#20449;&#24687;&#31185;&#25216;&#36753;&#12299;,relevantClaims[1-8],relevantPassages[&#20840;&#25991;]
      &#26446;&#33862;&#38738;: "&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#24335;&#20998;&#31867;&#22120;", &#12298;&#22823;&#36830;&#22823;&#23398;&#23398;&#25253;&#12299;,relevantClaims[1-8],relevantPassages[&#20840;&#25991;]
   CN103914841-B
      Li, prolivon. based on the pattern classifier of a convolutional neural network. Dalian University Journal ". 2003, volume 24 (2), text.
      Zhao. feature analysis and classification and identification of cells pathological image. Quasi-degree thesis of Chinese excellent full-text database information Technology ". 2013, (7) second, text.
UT DIIDW:2014R98465
ER

PT P
PN CN103901298-A
TI Variable power station device based operation state detecting method, involves determining operation status of history data model, and determining high current transformer sub-station operation state according to history operation data.
AU CHEN H
   GAO Y
   FAN Y
   FENG K
   JIANG T
   WANG H
   ZHU W
   ZHANG H
AE BEIJING CHINA REAL-TIME TECHNOLOGY CO (BEIJ-Non-standard)
   GUANGDONG POWER GRID CORP ELECTRIC POWER (CSPG-C)
GA 2014R31802
AB    NOVELTY - The method involves receiving history operation data by a transformer sub-station device. Current operation data is transmitted to the transformer sub-station device. A current data model and a history data model are established according to depth learning algorithm. A correlation model is established according to a preset rule. An association degree value is calculated. Operation status of the history data model is determined. High current transformer sub-station operation state is determined according to the history operation data.
   USE - Variable power station device based operation state detecting method.
   ADVANTAGE - The method enables enhancing history state similarity value more than a shread value and sending early warning information so as to find early stage of transformer substation for a potential mal-function symptom early warning and malfunction type correlation analysis.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a transformer sub-station device based operation state detecting system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a variable power station device based operation state detecting method. '(Drawing includes non-English language text)'
DC S01 (Electrical Instruments)
MC S01-H09
IP G01R-031/00
PD CN103901298-A   02 Jul 2014   G01R-031/00   201460   Pages: 7   Chinese
AD CN103901298-A    CN10093140    13 Mar 2014
PI CN10093140    13 Mar 2014
CP CN103901298-A
      CN103455563-A   CHINA ELECTRIC POWER RES INST (SGCC);  STATE GRID CORP CHINA (SGCC)   XU X, YAO Z, DOU R, FAN C, REN H, GENG M
      CN103488802-A   STATE GRID CORP CHINA (SGCC);  MAINTENANCE CO STATE GRID HUNAN ELECTRIC (MAIN-Non-standard)   MA R, LIANG Y
      CN103544651-A   GUANGDONG POWER GRID CORP ELECTRIC POWER (CSPG)   FENG Z, MA C, WANG J, ZHANG X, ZHOU Y
UT DIIDW:2014R31802
ER

PT P
PN WO2013149123-A1; US2015066499-A1; US9524730-B2
TI System for separating monaural speech from background noise, has deep neural network which extracts and classifies features of time-frequency (T-F) unit and linear support vector machine which classifies each T-F unit.
AU WANG Y
   WANG D
AE UNIV OHIO STATE (OHIS-C)
   WANG D (WANG-Individual)
   WANG Y (WANG-Individual)
   OHIO STATE INNOVATION FOUND (OHIS-C)
GA 2013Q07684
AB    NOVELTY - The system (5) has a cochlear filter (12) which receives monaural sound (17) in an electronic format and divides received sound into time-frequency (T-F) units. The processing units (14) classify T-F units as speech or non-speech and output each T-F unit classified as speech. A deep neural network (15) extracts and classifies features of T-F unit. A linear support vector machine (16) classifies each T-F unit based on extracted features from deep neural network. The linear support vector machine and deep neural network include training with training noises.
   USE - System for separating monaural speech from background noise.
   ADVANTAGE - The efficient large-scale training can be provided with background noises. The speech can be separated in low complexity using linear binary classification. The effective acoustic features can be learned using pre-trained deep neural networks. The training of noises can be generalized. The computational overhead in filtering background interference from monaural sounds can be reduced.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method of separating monaural speech from background noise.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of the system for separating monaural speech from background noise.
   System for separating monaural speech from background noise (5)
   Cochlear filter (12)
   Processing units (14)
   Deep neural network (15)
   Linear support vector machine (16)
   Monaural sound (17)
DC P86 (Musical instruments, acoustics (G10).); T01 (Digital Computers)
MC T01-J05B2; T01-J16B; T01-J16C1; T01-N01B3
IP G10L-015/16; G10L-021/0208; G10L-021/0232; G10L-025/30
PD WO2013149123-A1   03 Oct 2013   G10L-015/16   201368   Pages: 21   English
   US2015066499-A1   05 Mar 2015   G10L-021/0208   201518      English
   US9524730-B2   20 Dec 2016   G10L-025/30   201701      English
AD WO2013149123-A1    WOUS034564    29 Mar 2013
   US2015066499-A1    US14388260    26 Sep 2014
   US9524730-B2    US14388260    26 Sep 2014
FD  US2015066499-A1 PCT application Application WOUS034564
   US2015066499-A1 Provisional Application US617695P
   US9524730-B2 PCT application Application WOUS034564
   US9524730-B2 Provisional Application US617695P
   US9524730-B2 Based on Patent WO2013149123
PI US617695P    30 Mar 2012
   US14388260    26 Sep 2014
DS WO2013149123-A1: 
		      (National): AE; AG; AL; AM; AO; AT; AU; AZ; BA; BB; BG; BH; BN; BR; BW; BY; BZ; CA; CH; CL; CN; CO; CR; CU; CZ; DE; DK; DM; DO; DZ; EC; EE; EG; ES; FI; GB; GD; GE; GH; GM; GT; HN; HR; HU; ID; IL; IN; IS; JP; KE; KG; KM; KN; KP; KR; KZ; LA; LC; LK; LR; LS; LT; LU; LY; MA; MD; ME; MG; MK; MN; MW; MX; MY; MZ; NA; NG; NI; NO; NZ; OM; PA; PE; PG; PH; PL; PT; QA; RO; RS; RU; RW; SC; SD; SE; SG; SK; SL; SM; ST; SV; SY; TH; TJ; TM; TN; TR; TT; TZ; UA; UG; US; UZ; VC; VN; ZA; ZM; ZW
FS 704233/
CP WO2013149123-A1
      US20030158830-A1      
      US20120072215-A1      
   US2015066499-A1
      US9008329-B1   AUDIENCE INC (AUDI-Non-standard)   MANDEL M, AVENDANO C
      US9378733-B1   GOOGLE INC (GOOG)   VANHOUCKE V O, VINYALS O, NGUYEN P A P, SAN MARTIN M C P, SCHALKWYK J
      US7454342-B2   NEFIAN A V (NEFI-Individual);  LIU X (LIUX-Individual);  PI X (PIXX-Individual);  LIANG L (LIAN-Individual);  ZHAO Y (ZHAO-Individual)   NEFIAN A V, LIU X, PI X, LIANG L, ZHAO Y
      US7472063-B2   NEFIAN A V (NEFI-Individual);  PI X (PIXX-Individual);  LIANG L (LIAN-Individual);  LIU X (LIUX-Individual);  ZHAO Y (ZHAO-Individual)   NEFIAN A V, PI X, LIANG L, LIU X, ZHAO Y
      US9190053-B2   PENN G B (PENN-Individual);  JIANG H (JIAN-Individual);  ABDELHAMID O A M (ABDE-Individual);  MOHAMED A S A (MOHA-Individual)   PENN G B, JIANG H, ABDELHAMID O A M, MOHAMED A S A
   US9524730-B2
      US9008329-B1   AUDIENCE INC (AUDI-Non-standard)   MANDEL M, AVENDANO C
      US9378733-B1   GOOGLE INC (GOOG)   VANHOUCKE V O, VINYALS O, NGUYEN P A P, SAN MARTIN M C P, SCHALKWYK J
      US7454342-B2   NEFIAN A V (NEFI-Individual);  LIU X (LIUX-Individual);  PI X (PIXX-Individual);  LIANG L (LIAN-Individual);  ZHAO Y (ZHAO-Individual)   NEFIAN A V, LIU X, PI X, LIANG L, ZHAO Y
      US7472063-B2   NEFIAN A V (NEFI-Individual);  PI X (PIXX-Individual);  LIANG L (LIAN-Individual);  LIU X (LIUX-Individual);  ZHAO Y (ZHAO-Individual)   NEFIAN A V, PI X, LIANG L, LIU X, ZHAO Y
      US9190053-B2   PENN G B (PENN-Individual);  JIANG H (JIAN-Individual);  ABDELHAMID O A M (ABDE-Individual);  MOHAMED A S A (MOHA-Individual)   PENN G B, JIANG H, ABDELHAMID O A M, MOHAMED A S A
      US20030158830-A1      
      US20120072215-A1      
CR WO2013149123-A1
      JIN ET AL.: 'A Supervised Learning Approach to Monaural Segregation of Reverberant Speech' IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, [Online] vol. 17, no. 4, May 2009, Retrieved from the Internet: URL:http://www.cse.ohio-state.edu/~dwang/papers/Jin-Wang.taslp09.pdf [retrieved on 2013-07-03]
      HAMEL ET AL.: 'Learning Features from Music Audio with Deep Belief Networks' ISMIR, [Online] 2010, Retrieved from the Internet: URL:http://musicweb.ucsd.edu/-sdubnov/Mu270d/DeepLearning/FeaturesAudioEck.pdf [retrieved on 2013-07-03]
   US9524730-B2
      International Preliminary Report on Patentability issued Oct. 1, 2014 for International application No. PCT/US2013/034564.
      International Search Report mailed Jul. 11, 2013 for International application No. PCT/US2013/034564.
      Written Opinion mailed Jul. 11, 2013 for International application No. PCT/US2013/034564.
      Hamel, Philippe et al., Learning Features from Music Audio with Deep Belief Networks, The International Society for Music Information Retrieval Conference, pp. 339-344, (ISMIR 2010).
      Jin, Zhaozhang et al., A Supervised Learning Approach to Monaural Segregation of Reverberant Speech, IEEE Transactions on Audio, Speech, and Language Processing, vol. 17, No. 4, May 2009.
UT DIIDW:2013Q07684
ER

PT P
PN US2013138436-A1; CN103049792-A; HK1183141-A0; US9235799-B2; CN103049792-B; HK1183141-A1
TI Computer-implemented process for pretraining deep neural network, involves repeating inputting data entry of set, and designating last produced revised multiple layer deep neural network to be pretrained deep neural network.
AU YU D
   DENG L
   SEIDE F T B
   LI G
   SED F
   SEIDE F
AE MICROSOFT CORP (MICT-C)
   MICROSOFT CORP (MICT-C)
   MICROSOFT TECHNOLOGY LICENSING LLC (MICT-C)
GA 2013J68870
AB    NOVELTY - The process involves inputting data entry of a set one by one into an input layer to produce a revised multiple hidden layer deep neural network (DNN) (300) such that weights associated with a new hidden layer and a previously trained hidden layer are set via error back-propagation (BP) procedure to produce an output from an output layer that matches a label associated with training data entry (300). Repeating inputting step until a prescribed number of hidden layers is added. A last produced revised multiple layer DNN is designated to be a pretrained DNN (304).
   USE - Computer-implemented process for pretraining multiple-hidden-layer DNN for directly modeling tied context-dependent (CD) states and CD phones combined with a hidden markov model (HMM) in speech recognition systems, handwriting recognition systems and human activity recognition/detection systems using computing devices. Uses include but are not limited to personal computers (PCs), server computers, hand-held computing devices, laptop or mobile computers, communication devices e.g. cell phones and personal digital assistants (PDAs), multiprocessor systems, microprocessor-based systems, set-top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers and audio or video media players.
   ADVANTAGE - The process enables bringing DNN layer weights close to better local optimum, while leaving the DNN layer weights in a range with high gradient, so that the DNN layer weights can be fine-tuned effectively at later stage of training.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for training a context-dependent DNN (CD-DNN)
   (2) a computer-readable storage medium comprising a set of instructions for pretraining DNN.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a process for performing iteration of multi-iteration process for fine-tuning a pretrained DNN.
   Step for inputting data entry of set one by one into input layer to produce revised multiple hidden layer DNN (300)
   Step for determining whether pretrained DNN with fine-tuned prescribed number of times or weights associated with hidden layer does not vary by more than prescribed training threshold (302)
   Step for designating last produced revised multiple layer DNN as pretrained DNN (304)
DC P86 (Musical instruments, acoustics (G10).); T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-C08A; T01-J16B; T01-J16C1; T01-N01B3; T01-N02A3C; T01-S03; W04-E30A1
IP G10L-015/16; G06N-003/08; G06N-000/00
PD US2013138436-A1   30 May 2013   G10L-015/16   201337   Pages: 11   English
   CN103049792-A   17 Apr 2013   G06N-003/08   201364      Chinese
   HK1183141-A0   13 Dec 2013   G06N-000/00   201403      Chinese
   US9235799-B2   12 Jan 2016   G10L-015/16   201605      English
   CN103049792-B   17 Aug 2016   G06N-003/08   201660      Chinese
   HK1183141-A1   06 Oct 2017   G06N-000/00   201769      Chinese
AD US2013138436-A1    US304643    26 Nov 2011
   CN103049792-A    CN10488501    26 Nov 2012
   HK1183141-A0    HK110250    03 Sep 2013
   US9235799-B2    US304643    26 Nov 2011
   CN103049792-B    CN10488501    26 Nov 2012
   HK1183141-A1    HK110250    03 Sep 2013
FD  HK1183141-A0 Related to Patent CN103049792
   HK1183141-A1 Related to Patent CN103049792
PI US304643    26 Nov 2011
CP US2013138436-A1
      US20040073096-A1      
      US20050149461-A1      
      US20100217589-A1      
      US6421654-B1   COMMISSARIAT ENERGIE ATOMIQUE (COMS);  CENT NAT RECH SCI (CNRS)   GORDON M B
   CN103049792-A
      US20040073096-A1      
      US20050149461-A1      
      US20100217589-A1      
      US6421654-B1   COMMISSARIAT ENERGIE ATOMIQUE (COMS);  CENT NAT RECH SCI (CNRS)   GORDON M B
      WO1998017989-A2      
   US9235799-B2
      US5150449-A   NEC CORP (NIDE)   YOSHIDA K, WATANABE T
      US5404422-A   SHARP KK (SHAF)   SAKAMOTO K, YAMAGUCHI K, AKABANE T, FUJIMOTO Y
      US20040073096-A1      
      US20040260550-A1      
      US20050149461-A1      
      US20090216528-A1      
      US20100057453-A1      
      US20100217589-A1      
      US6421654-B1   COMMISSARIAT ENERGIE ATOMIQUE (COMS);  CENT NAT RECH SCI (CNRS)   GORDON M B
      US7254538-B1   INT COMPUTER SCI INST (ITCO-Non-standard)   HERMANSKY H, SHARMA S, ELLIS D
      US7720683-B1   SENSORY INC (SENS-Non-standard)   MOZER F S, SAVOIE R E, SUTTON S, VERMEULEN P J
      WO1998017989-A2      
   CN103049792-B
      US20040073096-A1      
      US20050149461-A1      
      US20100217589-A1      
      US6421654-B1   COMMISSARIAT ENERGIE ATOMIQUE (COMS);  CENT NAT RECH SCI (CNRS)   GORDON M B
CR    US9235799-B2
      Kirchhoff, Katrin, "Combining Articulatory and Acoustic Information for Speech Recognition in Noisy and Reverberant Environments", Retrieved at http://citeseerx.ist.psu.edu/viewdoc/download? doi=10.1.1.52.637&rep=rep1&type=pdf, International Conference on Spoken Language Processing, Nov. 30, 1998, pp. 891-894.
      Yu, D., D. Li, G. E. Dahl, Roles of pre-training and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition, NIPS 2010 Workshop on Deep Learning and Unsupervised Feature Learning, Dec. 2010.
      Seide, et al., "Conversational Speech Transcription Using Context-Dependent Deep Neural Networks", Retrieved at http://research.microsoft.com/pubs/153169/CD-DNN-HMM-SWB-Interspeech2011-Pub.pdf, Interspeech, Aug. 28-31, 2011, pp. 437-440.
      Renals, et al., "Connectionist Probability Estimators in HMM Speech Recognition", Retrieved at http://www.cstr.ed.ac.uk/downloads/publications/1994/sap94.pdf, IEEE Transactions on Speech and Audio Processing, vol. 2, No. 1, Jan. 1994, pp. 1-13.
      Franco, et al., "Context-Dependent Connectionist Probabilty Estimatation in a Hybrid Hidden Markov Model_Neural Net Speech Recognition System", Retrieved at http://citeseerx.ist.psu.edu/viewdoc/download? doi=10.1.1.29.9046&rep=rep1&type=pdf, Computer Speech and Language, vol. 8, Jul. 1994, pp. 1-24.
      Fritsch, et al., "ACID/HNN: Clustering Hierarchies of Neural Networks for Context-Dependent Connectionist Acoustic Modeling", Retrieved at http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=674478, Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, May 12-15, 1998, pp. 505-508.
      Saon, et al., "Maximum Likelihood Discriminant Feature Spaces", Retrieved at http://citeseer.ist.psu.edu/viewdoc/download?doi=10.1.1.26.7481&rep=rep1&type=pdf, Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, Jun. 2000, vol. 2, pp. 1129-1132.
      Zhan, et al., "Vocal Tract Length Normalization for LVCSR", Retrieved at http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.2780&rep=rep1&type=pdf, Technical Report CMU-LTI-97-150, May 1997, pp. 20.
      Gales, M. "Maximum Likelihood Linear Transforms for HMM-Based Speech Recognition", Retreived at http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=98546C241F70D362559073269ABF24A3?doi=10.1.1.21.444&rep=rep1&type=pdf, Computer Speech & Language, vol. 12, Jan. 1998, pp. 19.
      Hinton, et al., "A Fast Learning Algorithm for Deep Belief Nets", Retrieved at http://www.cs.toronto.edu/"hinton/absps/ncfast.pdf, Neural Computation, vol. 18, pp. 1527-1554, Jul. 2006.
      Stolcke, et al., "Recent Innovations in Speech-to-Text Transcription at SRI-ICSI-UW", Retrieved at http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.2814&rep=rep1&type=pdf, IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, No. 5, Sep. 2006, pp. 1-16.
      Saul, et al., "Mean Field Theory for Sigmoid Belief Networks", Retrieved at http://www.cis.upenn.edu/"jshi/Teaching/cis680-04/saul96a.pdf, Computing Research Repository (CORR), Mar. 1996, pp. 61-76.
      Yu, et al., "Deep-Structured Hidden Conditional Random Fields for Phonetic Recognition", Retrieved at http://research.microsoft.com/pubs/135407/deepCFR-interspeech2010.pfd, Interspeech, Sep. 26-30, 2010, pp. 2986-2989.
      Rumelhart, et al., "Learning Representations by Back-Propagating Errors", Retrieved at http://www.iro.umontreal.ca/"vincentp/ift3395/lectures/backprop_old.pdf, Nature, vol. 323, Oct. 9, 1986, pp. 533-536.
      Abrash, et al., "Connectionist Speaker Normalization and Adaptation", Retrieved at http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=CB69F9C3E41B216E4EDCC5BCE29A2BD8?doi=10.1.1.48.6904&rep=rep1&type=pdf, Eurospeech, Sep. 1995, pp. 4.
      Hinton, Geoffrey, "A Practical Guide to Traning Restricted Boltzmann Machines", Retrieved at http://www.cs.toronto.edu/"hinton/absps/guideTR.pdf, Technical Report UTML TR 2010-003, Aug. 2, 2010, pp. 21.
      Mohamed, et al., "Deep Belief Networks for Phone Recognition", Retrieved at http://research.microsoft.com/en-us/um/people/dongyu/nips2009/papers/Mohamed-Deep_Belief_Networks_for_phone_recognition.pdf, Proceedings of the NIPS Workshop Deep Learning for Speech Recognition, Dec. 2009, pp. 1-9.
      Fiscus, et al., "2000 NIST Evaluation of Conversational Speech Recognition over the Telephone: English and Mandarin Performance Results", Retrieved at http://citeseerx.ist.psu.edu/viewdoc/download? doi=10.1.1.27.5417&rep=rep1&type=pdf, May 2000, pp. 9.
      Mohamed, et al., "Deep Belief Networks Using Discriminative Features for Phone Recognition", Retrieved at http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5947494, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 22-27, 2011, pp. 5060-5063.
      "First Office Action Issued in Chinese Patent Application No. 201210488501.X", Mailed Date: Feb. 25, 2015, 15 Pages.
UT DIIDW:2013J68870
ER

PT P
PN CN108121975-A
TI Method for training convolutional neural network through small-scale face data sets, involves generating tagged face sample set through plug-and-play generation network (PPGN).
AU XU H
AE ZHONGKE HUITONG INVESTMENT HOLDINGS CO (ZHON-Non-standard)
GA 2018470760
AB    NOVELTY - The method involves training (101) a convolutional neural network VGG face recognition model. A deep convolution is constructed (102) to generate an anti-network depth convolution generates against network (DCGAN) model. The depth convolution is trained to generate a confrontation network using the original labeled face sample set. A tag-free face sample set through DCGAN is generated (103). The face data set annotations is generated (104) for DCGAN. The PPGN is trained (105) using the original annotated face sample set. A tagged face sample set through PPGN is generated (106). The convolutional neural network is trained (107) with the sample set generated by the DCGAN, PPGN, and the original labeled sample set. The training is repeated (108) that is generated face data set annotations, trained PPGN, generated tagged face sample set and trained convolutional neural network process. The VGG network is fine tuned (109) using the original annotated face sample set.
   USE - Method for training convolutional neural network through small-scale face data sets.
   ADVANTAGE - The method can reduce the labeling amount of face data annotation.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for training convolutional neural network through small-scale face data sets. (Drawing includes non-English language text)
   Step for training a convolutional neural network VGG face recognition model (101)
   Step for constructing deep convolution (102)
   Step for generating tag-free face sample set through DCGAN (103)
   Step for generating face data set annotations (104)
   Step for training PPGN (105)
   Step for generating tagged face sample set through PPGN (106)
   Step for training convolutional neural network (107)
   Step for repeating training (108)
   Step for fine tuning VGG network using the original annotated face sample set (109)
DC T01 (Digital Computers)
MC T01-C07C; T01-J04B2; T01-J10B2A; T01-N01B3
IP G06K-009/00; G06N-003/04
PD CN108121975-A   05 Jun 2018   G06K-009/00   201842   Pages: 9   Chinese
AD CN108121975-A    CN10007534    04 Jan 2018
PI CN10007534    04 Jan 2018
UT DIIDW:2018470760
ER

PT P
PN US2018150721-A1; KR2018060149-A; US10296829-B2
TI Apparatus for processing convolution operation for deep learning based image and voice recognition in a terminal, has controller that is configured to load a pixel of an input image and filter is configured to extract one kernel element.
AU MOSTAFA H
   ALESSANDRO A
   AIMAR A
AE SAMSUNG ELECTRONICS CO LTD (SMSU-C)
   UNIV ZUERICH (UYZU-C)
   SAMSUNG ELECTRONICS CO LTD (SMSU-C)
   UNIV ZUERICH (UYZU-C)
GA 2018436842
AB    NOVELTY - The convolution processing apparatus (101) comprises a controller (102) that is configured to load a pixel of an input image. A filter bank (103) has a filter, which is configured to extract one kernel element corresponding to the pixel from the filter based on an index of the pixel and an input channel of the pixel. A multiplier accumulator (104) is configured to perform a convolution operation based on the value of pixel and a value of the kernel element. An operation result of convolution operation is accumulatively stored in the accumulator. Multiple input registers are provided corresponding to predefined pixel rows.
   USE - Apparatus for processing the convolution operation for deep learning-based image and voice recognition in a terminal, such as smartphone.
   ADVANTAGE - The apparatus is enabled to effectively process an operation associated with the convolution. The computational overload generated during a convolution operation is reduced.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a method for processing the convolution operation for deep learning-based image and voice recognition in a terminal; and
   (2) a non-transitory computer-readable storage medium for storing a program for executed a computer to perform a method for processing the convolution operation for deep learning-based image and voice recognition in a terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a convolution processing apparatus.
   Convolution processing apparatus (101)
   Controller (102)
   Filter bank (103)
   Multiplier accumulator (104)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W01 (Telephone and Data Transmission Systems); W04 (Audio/Video Recording and Systems)
MC T01-C08A; T01-J04B2; T01-J05A1; T01-J10B1; T01-J10B2A; T01-J30A; T01-N01A; T01-S03; T04-D04; W01-C01D3C; W01-C01G8S; W01-C01Q8C; W04-N05C3E; W04-V04A5
IP G06K-009/62; G06N-003/04; G06N-003/08; G06N-003/063
PD US2018150721-A1   31 May 2018   G06K-009/62   201840   Pages: 21   English
   KR2018060149-A   07 Jun 2018   G06N-003/08   201840      
   US10296829-B2   21 May 2019   G06N-003/08   201938      English
AD US2018150721-A1    US581872    28 Apr 2017
   KR2018060149-A    KR159293    28 Nov 2016
   US10296829-B2    US581872    28 Apr 2017
FD  US10296829-B2 Previous Publ. Patent US2018150721
PI KR159293    28 Nov 2016
UT DIIDW:2018436842
ER

PT P
PN CN106778657-A
TI Classifying neonatal pain expression useful for evaluating neonatal pain degree by classifying neonatal pain expression images, establishing expression image library, constructing e.g. data layer, and identifying and classifying.
AU LIU Y
   LU G
   LI X
   YAN J
   LI H
AE UNIV NANJING POSTS & TELECOM (UNPT-C)
GA 201738609E
AB    NOVELTY - Method for classifying neonatal pain expression involves collecting and classifying neonatal pain expression images, establishing neonatal pain expression image library, constructing e.g. data layer, and convolution layers, dividing the image of neonatal pain expression into training sample and testing sample, inputting test samples, identifying and classifying the test samples using trained convolution neural network and outputting the test sample belonging to each probability value, and classifying the categories corresponding to the maximum probability values.
   USE - The method is useful for classifying neonatal pain expression (claimed) which is used for evaluating neonatal pain degree.
   DETAILED DESCRIPTION - Method for classifying neonatal pain expression involves (a) collecting neonatal pain expression images, classifying the images based on the different states of the facial expressions of the newborns, and establishing of neonatal pain expression image library, (b) constructing a data layer, 3 convolution layers, 2 completely connected layers, and 1 classification layer convolution neural network, (c) dividing the image of each expression in the neonatal pain expression image database into training sample and testing sample, using training sample data as data input of the convolution neural network, and back propagation algorithm to iterate the convolution neural network, and training and optimizing global parameters softmax convolutional neural network output and decreased loss function value converges to obtain trained convolution neural network, and (d) inputting the test samples, identifying and classifying the test samples using trained convolution neural network and outputting the test sample belonging to each probability value, and classifying the categories corresponding to the maximum probability values as test sample.
TF TECHNOLOGY FOCUS - BIOTECHNOLOGY - Preferred Method: The different neonatal facial expressions are chosen from calmness, crying, mild pain, and severe pain. The step (b) involves constructing (i) data layer by using sample image in the neonatal pain expression library and its corresponding category label as data layer of the convolution neural network, (ii) convolution layer 1, where the first layer is neural network convolution layer using m1-k1x k1 convolution kernel, interval is 1l pixel, sample image is convoluted, the output is sampled and normalized as the next input, convolution layer 2 which is the second layer of the convolutional neural network convolution layer using m2-k2x k2 convolution kernel, and convolution layer 3 using m3-k3x k3 convolution kernel with l3 pixels, and k1, k2, and k3 are gradually decreased, (iii) 2 completely connected layers, where the first layer of the convolution neural network is connected to input of the convolution layer, and (iv) classification layer, where the layer is convolutional neural network classification layer, connecting n output nodes of all-connected layer, and output is the probability value Pj belonging to the jth class, where j is 1, and k is the total number of classes. The softmax loss function is calculated using specific equation involving a predetermined parameters such as F theta , m, k, 1(y(i)=j), x, theta 1- theta k, and T, where F theta is loss function, m is the total number of training samples, k is the total number of classes, 1(y(i)=j) is an instruction function, x is vector formed by the full connection layer output node, theta 1- theta k is model parameter, and T is transpose. The probability values are calculated using specific equation involving predetermined parameters such as identity (y) which is classification result of test sample y.
DC T01 (Digital Computers)
MC T01-F04; T01-J04B2; T01-J04C; T01-J05B1; T01-J05B2; T01-J05B4F; T01-J10B2; T01-N01B3A
IP G06K-009/00; G06K-009/62; G06N-003/08
PD CN106778657-A   31 May 2017   G06K-009/00   201749   Pages: 9   Chinese
AD CN106778657-A    CN11233381    28 Dec 2016
PI CN11233381    28 Dec 2016
UT DIIDW:201738609E
ER

PT P
PN CN106778481-A
TI Convolutional neural network based human body condition monitoring method, involves performing preliminary judgment performed to judge fall state when vector angle drops to specific degrees or less in time duration of minute.
AU YE W
AE SHANGHAI BAIZHILONG NETWORK TECHNOLOGY (SHAN-Non-standard)
GA 2017386136
AB    NOVELTY - The method involves pre-processing and extracting human profile edges based on a human body image obtained from an infrared camera. Geometric center of human body outline is identified between an upper end point and a lower end point along a vertical direction in a three-point form from top to bottom. Vector angle between a starting point and end point is calculated along a horizontal line. A preliminary judgment is performed to judge fall state when the angle drops to 15 degree or less within time duration of one minute.
   USE - Convolutional neural network based human body condition monitoring method.
   ADVANTAGE - The method enables determining human body condition based on convolutional neural network in an effective manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a convolutional neural network based human body condition monitoring system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-N02B2
IP G06K-009/00
PD CN106778481-A   31 May 2017   G06K-009/00   201749   Pages: 5   Chinese
AD CN106778481-A    CN11029620    15 Nov 2016
PI CN11029620    15 Nov 2016
UT DIIDW:2017386136
ER

PT P
PN CN108133202-A
TI Hierarchical mixed density network based hand pose estimating method, involves obtaining, training and testing hierarchical mixed density network, and degrading hierarchical mixed density network into mixed-density network.
AU XIA C
AE SHENZHEN VISION TECHNOLOGY CO LTD (SHEN-Non-standard)
GA 201846354C
AB    NOVELTY - The method involves obtaining hierarchical mixed density network (HMDN). The HMDN is trained and tested. The HMDN is degraded into a mixed-density network. A produced image is obtained for realizing hand depth controlled by two sets of parameters. A three-dimensional (3D) hand shape model is established according to posture parameter. A two-dimensional (2D) image is projected on a point 3D model. Visibility of the 2D image is determined by a projection pixel in the 2D image from the 3D hand shape model. The generated 2D image is controlled by a vector. A hand skeleton is identified to identify pose parameters and given configuration of the 2D image.
   USE - HMDN based hand pose estimating method.
   ADVANTAGE - The method enables utilizing convolutional neural network to learn single value output and multivalued mapping, and reducing hand self-occlusion, and avoiding gesture estimation of influence, and improving gesture estimation accuracy and sensitivity.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a HMDN based hand pose estimating method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-N01B3A
IP G06K-009/00
PD CN108133202-A   08 Jun 2018   G06K-009/00   201842   Pages: 12   Chinese
AD CN108133202-A    CN10045105    17 Jan 2018
PI CN10045105    17 Jan 2018
CP CN108133202-A
      CN106346485-A   UNIV DALIAN TECHNOLOGY (UYDA)   SUN Y, QU W, WEI S, XU F, YANG Q
      CN106778479-A   SHENZHEN VISION TECHNOLOGY CO LTD (SHEN-Non-standard)   XIA C
CR CN108133202-A
      QI YE, TAE-KYUN KIM: "Occlusion-aware Hand Pose Estimation Using Hierarchical Mixture Density Network", ARXIV,relevantClaims[1-10],relevantPassages[1-9]
UT DIIDW:201846354C
ER

PT P
PN CN108108676-A
TI Human face identifying method, involves obtaining to-be-detected video, which includes frame images of human face, inputting multi-channel image, and combining frame images to generate multi-channel image of non-living face.
AU CHEN Z
AE BEIJING XIAOMI MOBILE SOFTWARE CO LTD (XIAO-C)
GA 201844845K
AB    NOVELTY - The method involves obtaining to-be-detected video, which includes frame images of human face. The frame images are combined to generate a multi-channel image. The multi-channel image is inputted to a target convolutional neural network for detecting and determining whether a human face is in a living human face. A convolutional neural network is generated according to the detection result. The frame images are combined to generate multi-channel image of a non-living face.
   USE - Human face identifying method.
   ADVANTAGE - The method enables determining whether a human face is detected in a living face so as to improve accuracy of face recognition and improve user experience.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a human face identifying device
   (2) a convolution neural network generating method
   (3) a convolution neural network generating device
   (4) a computer-readable storage medium for storing set of instructions to identify human face.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face identifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B2A; T01-N02A2D; T01-S03; T04-D07F1
IP G06K-009/00; G06N-003/04
PD CN108108676-A   01 Jun 2018   G06K-009/00   201840   Pages: 25   Chinese
AD CN108108676-A    CN11319694    12 Dec 2017
PI CN11319694    12 Dec 2017
UT DIIDW:201844845K
ER

PT P
PN CN108106841-A
TI Planetary gear box intelligent fault diagnosis method, involves establishing deep convolutional neural network model, and inputting final test data to convolutional neural network model for performing fault classification process.
AU LIN J
   JIAO J
   ZHAO J
   ZHAO M
AE UNIV XIAN JIAOTONG (UYXJ-C)
GA 201844887J
AB    NOVELTY - The method involves obtaining built-in encoder signal and an output shaft position of a planetary gear box by a coder data collecting card. Angular position signal of a test shaft is obtained. Polynomial fitting transient motion information is obtained by utilizing a polynomial fitting of the planetary gear box. Instantaneous corner acceleration signal is obtained for segmenting a training set into a validation set and a testing set. A deep convolutional neural network model is established. Final test data is input to the convolutional neural network model for performing fault identification and classification processes.
   USE - Built-in encoder signal based planetary gear box intelligent fault diagnosis method.
   ADVANTAGE - The method enables simplifying execution of data collecting program, reduces test cost, realizing fault characteristic extraction, detection and diagnosis processes of automation by utilizing the convolutional neural network model.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a planetary gear box intelligent fault diagnosis method. '(Drawing includes non-English language text)'
DC S02 (Engineering Instrumentation); T01 (Digital Computers)
MC S02-J03; T01-J05B2; T01-N01B3A
IP G01M-013/02
PD CN108106841-A   01 Jun 2018   G01M-013/02   201840   Pages: 9   Chinese
AD CN108106841-A    CN11397302    21 Dec 2017
PI CN11397302    21 Dec 2017
CP CN108106841-A
      CN103353396-A   UNIV XIAN JIAOTONG (UYXJ)   LEI Y, LIN J, WANG X, XU X, ZHAO M
      CN106168539-A   CHANGZHOU COLLEGE INFORMATION TECHNOLOGY (CHZH)   WANG E, ZHU J, ZHAO L
      CN106874957-A   UNIV SOOCHOW (USWZ)   ZHU Z, CAO S, YOU W, SHEN C, LIU C, HUANG W
      EP2412494-A3   HAZET WERK ZERVER GMBH HERMANN (HAZE-Non-standard)   TIMM F, UNSELD R
      US20110093155-A1      
CR CN108106841-A
      LUYANG JING: "A convolutional neural network based feature learning and fault diagnosis method for the condition monitoring of gearbox", MEASUREMENT,relevantClaims[1-2],relevantPassages[1-10]
      MING ZHAO: "Instantaneous speed jitter detection via encoder signal and its application for the diagnosis of planetary gearbox", MECHANICAL SYSTEMS AND SIGNAL PROCESSING,relevantClaims[1-2],relevantPassages[16-31]
      : "", 1,relevantClaims[1-2],relevantPassages[1-7]
UT DIIDW:201844887J
ER

PT P
PN CN106919673-A
TI Deep learning based text emotion analyzing system for enterprise or government related department, has emotion analysis module for processing comment information, and information displaying module for presenting mood analysis result.
AU SHI H
   LI X
   CHEN N
AE UNIV ZHEJIANG GONGSHANG (UYZG-C)
GA 201746995T
AB    NOVELTY - The system has a main body provided with an information collecting module, an information pre-processing module, an emotion analysis module and an information display module. The information collecting module collects comment information of multiple network resource websites. The information preprocessing module classifies collected comment information into word, marking part of speech and emotion information. The emotion analysis module process the comment information. The information displaying module presents an mood analysis result.
   USE - Deep learning based text emotion analyzing system for an enterprise or government related department.
   ADVANTAGE - The system reviews information for carrying out emotion tendency analysis, and visually educates a user so as to provide sentiment analysis result or pre-warning to an enterprise or government related department.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a deep learning based text emotion analyzing system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J05B4P; T01-J11A1; T01-N01A3; T01-N01B3; W04-W05A
IP G06F-017/27; G06F-017/30; G06N-003/08
PD CN106919673-A   04 Jul 2017   G06F-017/30   201749   Pages: 9   Chinese
AD CN106919673-A    CN10093688    21 Feb 2017
PI CN10093688    21 Feb 2017
CP CN106919673-A
      CN103425777-A   UNIV PEKING (UYPK)   YANG L, WANG G, WANG H, TENG J, YIN Z
      CN103488782-A   UNIV NORTH CHINA ELECTRIC POWER (UYHD)   HE H
      CN104965822-A   UNIV CENT SOUTH (UYCS)   GAO Y, CHAO X
      CN105512687-A   BEIJING RUN TECHNOLOGIES CO LTD (BEIJ-Non-standard)   LIU P, ZHANG J
      US20110087483-A1      
CR CN106919673-A
      : "", ,relevantClaims[1-10],relevantPassages[105-112]
      : "word embeddingCNN", ,relevantClaims[1-10],relevantPassages[2902-2909]
UT DIIDW:201746995T
ER

PT P
PN CN108111335-A
TI Method of scheduling and linking virtual network functions, involves obtaining virtual network function placement node and service chain requested by network according to network information, and Markov training model.
AU LIU F
   JIN H
   XIAO Y
AE UNIV HUAZHONG SCI & TECHNOLOGY (UYHZ-C)
GA 201847132W
AB    NOVELTY - The method involves building a Markov initial model according to network environment information and network request information. The Markov training model is obtained by using Markov initial model and random propagation action and backpropagation neural network to perform deep learning training. The network environment information and network request information are obtained in real time, and the virtual network function placement node and service chain requested by the network are obtained according to the network environment information, the network requested information, and the Markov training model.
   USE - Method of scheduling and linking virtual network functions.
   ADVANTAGE - A Markov training model is obtained by adopting a random generation action and a back-propagation neural network method to perform depth-enhancement learning training, which maximizes the optimization of network functions and service chains, reduces the overall delay of network requests, and improves the network.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system based on the method for scheduling and linking virtual network functions.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the process of scheduling and linking virtual network functions. (Drawing includes non-English language text)
DC T01 (Digital Computers); W01 (Telephone and Data Transmission Systems)
MC T01-N01B3; W01-A06A3; W01-A06B7G; W01-A06D; W01-A06E
IP H04L-012/24; H04L-029/08
PD CN108111335-A   01 Jun 2018   H04L-012/24   201841   Pages: 20   Chinese
AD CN108111335-A    CN11262639    04 Dec 2017
PI CN11262639    04 Dec 2017
CP CN108111335-A
      CN103117060-A   CHINESE ACAD SCI ACOUSTICS INST (CAAI);  BEIJING KEXIN TECHNOLOGY CO LTD (BEIJ-Non-standard)   PAN J, YAN Y, XIAO Y
      CN103747059-A   UNIV HUAZHONG SCI & TECHNOLOGY (UYHZ)   WU S, WU X, JIN H
      CN107666448-A   UNIV CHONGQING POSTS & TELECOM (UCPT)   CHEN Q, YANG H, GANG P, WANG Y, ZHAO G, TANG L
      US20020059376-A1      
      US6705869-B2   SCHWARTZ D (SCHW-Individual)   SCHWARTZ D
UT DIIDW:201847132W
ER

PT P
PN CN108108993-A
TI Deep neural network based virtual currency optimization method for e.g. mobile game, involves creating growth rate deceleration strategies for game currency distribution, according to deep learning results.
AU QIN Q
   WANG H
AE JIANGSU MINGTONG INFORMATION TECHNOLOGY (JIAN-Non-standard)
GA 201845408F
AB    NOVELTY - The method involves performing the data collection and mining of game currency system. The intensive learning is used for deep learning of the user's purchase behavior corresponding to high-frequency data. The growth rate deceleration strategies for game currency distribution are created, according to the deep learning results. The data mining is performed to obtain the history data purchased by all users first, and the statistics are performed on each user's purchase behavior, so as to rank the users according to the purchase frequency to obtain users corresponding to high-frequency data.
   USE - Deep neural network based virtual currency optimization method for online games and mobile games.
   ADVANTAGE - The money supply amount is increased and reduced flexibly, according to the desire of the user, and the spreading force of game and entertainment is increased.
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J03; T01-N01A1; T01-N01B1; T01-N01B3; T01-N01D; W04-X02C
IP A63F-013/70; G06N-003/08; G06Q-030/02
PD CN108108993-A   01 Jun 2018   G06Q-030/02   201842   Pages: 5   Chinese
AD CN108108993-A    CN11090337    08 Nov 2017
PI CN11090337    08 Nov 2017
CP CN108108993-A
      CN103810625-A   TENCENT TECHNOLOGY SHENZHEN CO LTD (TNCT)   HU T, LUO B, MO W, BAI J, CHEN J, SHEN L, LI J, LI S, LIU J, XIE J, YU Y, RAN X, TAN Y, CHEN Z, LI L, LEI G, WANG C
      CN105119733-A   BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BIDU)   HE J, SHI L, TIAN H, WANG F, ZHOU F
      CN106845817-A   UNIV TSINGHUA (UYQI)   LIU R, WANG D, WANG Y
      US20140358651-A1      
CR CN108108993-A
      : "Q", ,relevantClaims[5],relevantPassages[15-1623-28]
UT DIIDW:201845408F
ER

PT P
PN CN108108757-A
TI Method for classifying convolutional neural network-based diabetic foot ulcer, involves creating diabetic foot ulcers data set and region of interest marker, followed by pre-processing of training patches and conventional machine learning.
AU XIA C
AE SHENZHEN VISION TECHNOLOGY CO LTD (SHEN-Non-standard)
GA 201844843L
AB    NOVELTY - A convolutional neural network-based diabetic foot ulcer classifying method involves creating a diabetic foot ulcers data set and a region of interest marker, adding the diabetic foot ulcers data set and the region of interest marker data, followed by pre-processing of training patches and conventional machine learning, and constructing diabetic foot ulcer network.
   USE - Method for classifying convolutional neural network-based diabetic foot ulcer.
   ADVANTAGE - The method enables classifying the convolutional neural network-based diabetic foot ulcer with high efficiency and high sensitivity, in a rapid manner.
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-N01B3; T04-D02; T04-D03; T04-D04
IP G06K-009/32; G06K-009/46; G06K-009/62
PD CN108108757-A   01 Jun 2018   G06K-009/62   201841   Pages: 12   Chinese
AD CN108108757-A    CN11368840    18 Dec 2017
PI CN11368840    18 Dec 2017
CP CN108108757-A
      CN106599939-A   SHENZHEN VISION TECHNOLOGY CO LTD (SHEN-Non-standard)   XIA C
      CN107392314-A   UNIV TIANJIN (UTIJ)   LI H, PANG Y
CR CN108108757-A
      MANU GOYAL: "DFUNet: Convolutional Neural Networks for Diabetic Foot Ulcer Classification", ARXIV:1711.10448V1,relevantClaims[1-10],relevantPassages[1-5]
UT DIIDW:201844843L
ER

PT P
PN CN108109121-A
TI Convolutional neural network based rapid human face image blur removing method, involves establishing blur eliminating model, performing convolution kernel generating process, and constructing convolutional network structure.
AU XIA C
AE SHENZHEN VISION TECHNOLOGY CO LTD (SHEN-Non-standard)
GA 201844835V
AB    NOVELTY - The method involves establishing a blur eliminating model. Convolution kernel generating process is performed. A convolutional network structure is constructed to perform human face smooth operation. Image obfuscation and de-obfuscation operations are performed. A human face image is input. The human face image is stored by using nonlinear operation function. An original input image is obtained. A convolution kernel generating parameter is obtained by using two-dimensional kernel Gaussian function. Total deviation loss function and human face distortion function are determined to control face fitting degree and distortion degree. A final reconstructed human face identification result is sent.
   USE - Convolutional neural network based rapid human face image blur removing method.
   ADVANTAGE - The method enables rapidly estimating an original sharpening image and the convolution kernel under non-priori knowledge conditions and providing a learning model based on multi-scale structure, thus improving high degree image restoring accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based rapid human face image blur removing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B1; T01-J10E; T04-D07F1
IP G06N-003/04; G06N-003/08; G06T-005/00
PD CN108109121-A   01 Jun 2018   G06T-005/00   201840   Pages: 9   Chinese
AD CN108109121-A    CN11368815    18 Dec 2017
PI CN11368815    18 Dec 2017
CP CN108109121-A
      CN107145546-A   BEIJING ENVIRONMENTAL FEATURES INST (BEIJ-Non-standard)   GU Y, WANG Y, XIE X, ZHAI J
      US20170294010-A1      
CR CN108109121-A
      LINGXIAO WANG ET AL: "DeepDeblurFast one-step blurryface images restoration", HTTPS://ARXIV.ORG/PDF/1711.09515.PDF,relevantClaims[1-10],relevantPassages[1-4]
UT DIIDW:201844835V
ER

PT P
PN CN108089425-A
TI Method for eliminating optical scanning hologram defocus noise based deep learning, involves analyzing reconstructed image through neural network model passing reconstructed image with defocus noise after defocusing noise cancelation.
AU OU H
   WU Y
   SHAO W
   WANG B
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 201845130A
AB    NOVELTY - The method involves acquiring an encrypted hologram by scanning an object with an optical holographic scanning device. A reconstructed image with defocus noise is obtained to decrypt reconstruction hologram. A deep convolutional neural network is trained in advance using a sample image to establish a neural network model that is eliminated for defocusing noise. The reconstructed image is analyzed through the neural network model passing the reconstructed image with defocus noise after defocusing noise cancelation. An object to be scanned is placed between an X-Y scanning mirror and a convex lens of a scanning optical path.
   USE - Method for eliminating optical scanning hologram defocus noise based deep learning.
   ADVANTAGE - The method enables eliminating defocus noise of the reconstructed image utilizing deep neural network learning process after random encryption processing, and eliminating the defocus noise of a test image by training the image with defocus noise and the image without defocus noise and effectively realizing large defocus noise in optical scanning holography.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of an optical holographic scanning device. '(Drawing includes non-English language text)'
DC P84 (Other photographic (G03D-H).); T01 (Digital Computers); V07 (Fibre-optics and Light Control); W01 (Telephone and Data Transmission Systems)
MC T01-D01; T01-J10B1; T01-J10B2; T01-N01B3A; V07-F02A; V07-F02C; V07-K05; V07-M; W01-A05A
IP G03H-001/04; G03H-001/10
PD CN108089425-A   29 May 2018   G03H-001/04   201841   Pages: 14   Chinese
AD CN108089425-A    CN10038955    16 Jan 2018
PI CN10038955    16 Jan 2018
CP CN108089425-A
      CN104159094-A   UNIV SICHUAN (USCU)   WANG Y, ZHOU X, HU Y
      CN105204311-A   UNIV ELECTRONIC SCI & TECHNOLOGY (UEST)   WANG B, WU Y, OU H, SHAO W
      CN107085838-A   UNIV XINJIANG (UYXI-Non-standard)   FAN H, HUANG X, JIA Z
      CN107240074-A   UNIV ELECTRONIC SCI & TECHNOLOGY (UEST)   OU H, CHEN X, SHAO W, WANG B
      EP467577-A3   SONY CORP (SONY);  CALIFORNIA INST OF TECHN (CALY)   KOBAYASHI S, PSALTIS D
CR CN108089425-A
      : "", ,relevantClaims[1-6],relevantPassages[23-61]
UT DIIDW:201845130A
ER

PT P
PN CN109063908-A
TI Depth multi-task learning based urban spatial granularity AQI prediction and level estimating method, involves obtaining fine grain AQI level estimation training set to define spatial fine-grained AQI level estimation objective function.
AU WANG J
   CHEN L
   DING Y
   ZHANG S
   ZHENG Y
AE ZHEJIANG HONGCHENG COMPUTER SYSTEM CO LT (ZHEJ-Non-standard)
GA 2019006506
AB    NOVELTY - The method involves dividing a city geospatial space into multiple disjoint grid areas to obtain a grid area set, where the grid area set includes network POI distribution data and road network distribution data of the grid area. Three bipartite graphs are drawn according to the POI distribution data and the road network distribution data. An AQI prediction sample is obtained according to the road network distribution data. A spatial fine-grained AQI level estimation sample is obtained to obtain short-term weather information. A labeled space fine grain AQI level estimation training set is obtained to define spatial fine-grained AQI level estimation objective function.
   USE - Depth multi-task learning based urban spatial granularity AQI prediction and level estimating method.
   ADVANTAGE - The method enables combining deep learning and multi-task learning to perform AQI prediction and level estimation without air quality monitoring station so as to realize wide range of application.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a depth multi-task learning based urban spatial granularity AQI prediction and level estimating method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-F02; T01-J21; T01-N01A2; T01-N01B3; T01-N01D; T01-N02B2
IP G06Q-010/04; G06Q-050/26; G01W-001/00; G01N-033/00
PD CN109063908-A   21 Dec 2018   G06Q-010/04   201930   Pages: 22   Chinese
AD CN109063908-A    CN10852241    30 Jul 2018
PI CN10852241    30 Jul 2018
UT DIIDW:2019006506
ER

PT P
PN CN109039534-A
TI Deep neural network based sparse code division multiple access signal detecting algorithm, has set of instructions for determining neural network parameters, and performing SCMA signal detection process by using MPA algorithm.
AU XU W
   LU C
AE UNIV SOUTHEAST (UYSE-C)
GA 2018A5519Y
AB    NOVELTY - The algorithm has set of instructions for obtaining a node of factor graph. A connection side of a neural network is obtained. An information value on a graph is expressed. An output value of a neural network node is obtained. MPA algorithm is determined. A multi-layer depth neural network is constructed. Parameters of a connecting edge are obtained to enhance MPA algorithm performance. Neural network parameters are determined by using gradient descent optimization algorithm. A SCMA signal detection process is performed by using traditional MPA algorithm. Modulation symbols of each user are preset.
   USE - Deep neural network based sparse code division multiple access signal detecting algorithm.
   ADVANTAGE - The method enables obtaining network coefficient by using gradient descent operation, enhancing multiple access signal detecting performance, reducing time delay caused by SCMA signal detection in an effective manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a sectional view of a deep neural network based sparse code division multiple access signal detecting algorithm. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W01 (Telephone and Data Transmission Systems); W02 (Broadcasting, Radio and Line Transmission Systems)
MC T01-J16C1; W01-A01; W01-A06A3; W02-K08
IP H04L-001/00; G06N-003/04; G06N-003/08
PD CN109039534-A   18 Dec 2018   H04L-001/00   201930   Pages: 12   Chinese
AD CN109039534-A    CN10635535    20 Jun 2018
PI CN10635535    20 Jun 2018
UT DIIDW:2018A5519Y
ER

PT P
PN TW201839665-A; TW643137-B1
TI Object recognition method and object recognition system includes obtaining a multi-media data, inputting the multi-media data into a deep learning model.
AU PAN P
AE PAN P (PANP-Individual)
GA 201911368N
AB    NOVELTY - An object recognition method and an object recognition system are provided. The method includes: obtaining a multi-media data; inputting the multi-media data into a deep learning model to identify an object of the multi-media data; and outputting output information corresponding to the object according to the object of the multi-media data.
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2A; T01-J30A; T04-D04
IP G06K-009/78; G06N-003/08; G06F-003/01
PD TW201839665-A   01 Nov 2018   G06K-009/78   201931   Pages: 56   Chinese
   TW643137-B1   01 Dec 2018   G06K-009/78   201932      Chinese
AD TW201839665-A    TW113453    21 Apr 2017
   TW643137-B1    TW113453    21 Apr 2017
PI TW113453    21 Apr 2017
UT DIIDW:201911368N
ER

PT P
PN CN108108354-A
TI Deep learning based microblog user gender prediction method, involves generating feature vector of sentence of microblog text, and performing gender prediction or classification of microblog user by using network model process.
AU ZHANG C
   RAN S
   WU J
   FENG L
   NIU Z
   HUANG D
AE BEIJING INST TECHNOLOGY (BEIT-C)
GA 201844853B
AB    NOVELTY - The method involves collecting blog information (step1). A microblog text of a user is collected on an IM platform by using a network crawler. The microblog text of the user is stored in a computer. The microblog text is pre-processed (step2) to extract a text of the microblog text. Word filtering and punctuation filtering are performed. A word vector is constructed (step3) for words of the microblog text. Words in a sentence of the microblog text are mapped into the word vector. A feature vector of the sentence of the microblog text is generated (step4) by using convolutional neural network based IM text representation process. Gender prediction or classification of a microblog user is performed (step5) by using long term memory based network model process.
   USE - Deep learning based microblog user gender prediction method.
   ADVANTAGE - The method enables avoiding need of artificial constructed microblog text features to realize semantic modeling of the microblog text, and extracting semantic sequence dependency relationship in microblog text features, and microblog text features in an accurate manner, and improving identification performance of a microblog user gender, and realizing wide application prospect in information recommendation and product marketing.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based microblog user gender prediction method. '(Drawing includes non-English language text)'
   Step for collecting blog information (step1)
   Step for pre-processing the microblog text (step2)
   Step for constructing a word vector for words of the microblog text (step3)
   Step for generating a feature vector of the sentence of the microblog text (step4)
   Step for performing Gender prediction or classification of a microblog user (step5)
DC T01 (Digital Computers)
MC T01-J05B2; T01-J05B4P; T01-J11A1; T01-J16C3; T01-L02; T01-N01A2D; T01-N01B3
IP G06F-017/27; G06F-017/30; G06N-003/08; G06Q-050/00
PD CN108108354-A   01 Jun 2018   G06F-017/27   201840   Pages: 14   Chinese
AD CN108108354-A    CN11380014    20 Dec 2017
PI CN11380014    20 Dec 2017
UT DIIDW:201844853B
ER

PT P
PN CN109035316-A
TI Nuclear magnetic resonance image sequence registration method involves performing image registration between sequence of images to be registered and sequence of reference images, and outputting registered sequence of nuclear magnetic images.
AU XU Y
   WU Z
AE BEIJING ANDE YIZHI TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 201900049E
AB    NOVELTY - The method involves using (S102) a sequence of a nuclear magnetic resonance image of the patient as a sequence of images to be registered in response to a user operation. The image registration is performed (S104) between the sequence of images to be registered and a sequence of reference images generated in advance using a convolutional neural network. The specific area of multiple images in the reference image sequence is aligned with each other, where the convolutional neural network includes discriminative features for image registration. The discriminative feature is used to discriminate image translation, image scaling, image rotation, and/or image miscuting of the image in the sequence of images to be registered relative to corresponding images in the sequence of reference images. The registered sequence of nuclear magnetic resonance images is outputted (S106).
   USE - Nuclear magnetic resonance image sequence registration method.
   ADVANTAGE - The image registration is performed at one time by utilizing the discriminative features for image registration in the convolutional neural network. Thus, the image registration time is greatly reduced.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a nuclear magnetic resonance image sequence registration device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the nuclear magnetic resonance image sequence registration method. (Drawing includes non-English language text)
   Step for using patient nuclear magnetic resonance image sequence as a sequence of images to be registered (S102)
   Step for performing image registration between sequence of images to be registered and sequence of reference images (S104)
   Step for outputting registered sequence of nuclear magnetic images (S106)
DC S01 (Electrical Instruments); S03 (Scientific Instrumentation); S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S01-E02A2; S03-E07A; S05-D02B2; T01-J10B2; T01-J10B3A; T01-N01E
IP G06T-007/38; G06N-003/04
PD CN109035316-A   18 Dec 2018   G06T-007/38   201928   Pages: 15   Chinese
AD CN109035316-A    CN10988485    28 Aug 2018
PI CN10988485    28 Aug 2018
UT DIIDW:201900049E
ER

PT P
PN CN106780569-A
TI Human body pose estimation behavior analysis method, involves pre-processing data on three-dimensional convolutional neural network, and expanding convolutional neural network process to three-dimensional convolution process.
AU XIA C
AE SHENZHEN VISION TECHNOLOGY CO LTD (SHEN-Non-standard)
GA 201738566B
AB    NOVELTY - The method involves pre-processing data on a three-dimensional (3D) convolutional neural network (CNN). A CNN process is expanded to a 3D convolution process. Depth information is transmitted to a CNN based on a two-dimensional (2D) video frame stream. A video is captured by the depth 3D CNN to estimate a human pose, where the video is captured by using a single 2D monocular camera. A 3D convolution operation of a z-dimension is performed by a time dimension to adjust image height and width. High resolution video sequence is determined.
   USE - Human body pose estimation behavior analysis method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human body pose estimation behavior analysis method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B2; T01-J10D; T01-N01D1B; T04-D07D3
IP G06T-007/00; G06T-007/285; G06T-007/292
PD CN106780569-A   31 May 2017   G06T-007/292   201749   Pages: 9   Chinese
AD CN106780569-A    CN11016790    18 Nov 2016
PI CN11016790    18 Nov 2016
CP CN106780569-A
      CN101989326-A   SAMSUNG ELECTRONICS CO LTD (SMSU)   CHEN M, CHU R
      CN105069423-A   BEIJING DEEPGLINT INFORMATION TECHNOLOGY (BEIJ-Non-standard)   WANG X, ZHAO Y, PAN Z, SHEN H, CAI Y
      CN105160310-A   UNIV XIDIAN (UYXN)   HAN H, JIAO L, LI Y, MA W, WANG S, WANG W, ZHANG D, YE X
      CN105787439-A   GUANGZHOU NEW RHYTHM INTELLIGENT TECHNOL (GUAN-Non-standard)   CHEN Y, LIN L, WANG Q, WANG K
CR CN106780569-A
      AGNE GRINCIUNAITE: "Human Pose Estimation in Space and Time Using 3D CNN", ECCV 2016 WORKSHOPSPART LNCS 991,relevantClaims[1-10],relevantPassages[32-361-4]
UT DIIDW:201738566B
ER

PT P
PN CN109102502-A
TI Three-dimensional convolutional neural network pulmonary nodules detecting method, involves predicting trained network parameters, determining segmenting data, and transmitting group of segmenting data into small cube blocks.
AU LI Y
   CAO Y
   LIU L
   WANG Y
   WANG P
AE UNIV NORTHWESTERN POLYTECHNICAL (UNWP-C)
GA 201902975G
AB    NOVELTY - The method involves pre-treating lung CT data. A tuber is arranged into a patient coordinate system from an image coordinate system. A set of optimal parameters is determined for training training data using a defined network structure. The training data is sampled and cut. Data enhancement and horizontal turnover and rotation operation is performed. Trained network parameters are predicted for obtaining testing case data. Segmenting data is determined. A group of segmenting data is transmitted into small cube blocks.
   USE - Scintillation substance based three-dimensional convolutional neural network pulmonary nodules detecting method.
   ADVANTAGE - The method enables reducing time cost and improving nodule detection recall rate and average accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a three-dimensional convolutional neural network pulmonary nodules detecting method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D02A1; S05-D02A5E; T01-E01A; T01-J05B2B; T01-J10B2; T01-N01B3A; T01-N01D1B; T01-N01E1
IP G06T-007/00; G06T-007/13; G06T-007/155
PD CN109102502-A   28 Dec 2018   G06T-007/00   201927   Pages: 12   Chinese
AD CN109102502-A    CN10891156    03 Aug 2018
PI CN10891156    03 Aug 2018
UT DIIDW:201902975G
ER

PT P
PN CN109087313-A
TI Deep learning based intelligent tongue image dividing method, involves merging high-low layer characteristics by using decoder of tongue-splitting network, and performing pixel classification process on tongue image to obtain tongue region.
AU LI S
   SHAO Y
   LUO Z
   SU S
   CAO D
   LIN W
AE UNIV XIAMEN (UYXI-C)
GA 201901458B
AB    NOVELTY - The method involves developing tongue image collection standards to collect tongue image data in a standard environment, where the tongue image collection standards include aspects of equipment, light, location and patient requirements. The tongue image data is segmented into a training set and a test set. Low-level characteristics of a tongue image are obtained. Pixel positioning process is performed on the tongue image by using an encoder of a tongue-splitting network (TS-Net). High-low layer characteristics are merged by using a decoder of the TS-Net. Pixel classification process is performed on the tongue image to obtain a complete tongue region.
   USE - Deep learning based intelligent tongue image dividing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning based intelligent tongue image dividing method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-E03; S05-G02G9; T01-E01C; T01-J05B2; T01-J10B2; T01-N01B3A; T01-N01E1
IP G06T-007/11; G06T-007/194
PD CN109087313-A   25 Dec 2018   G06T-007/11   201927   Pages: 11   Chinese
AD CN109087313-A    CN10877382    03 Aug 2018
PI CN10877382    03 Aug 2018
UT DIIDW:201901458B
ER

PT P
PN CN109087296-A
TI Method for extracting human body region in computed tomography image, involves mapping original computed tomography image to complete single-pixel human body segmentation according to classification result.
AU XU M
   YANG Y
   QI S
   MA H
   QIAN W
AE UNIV CHINESE NORTHEASTERN (UYDB-C)
GA 201901458T
AB    NOVELTY - The method involves clamping a computed tomography image (CT) according to prior knowledge of the CT image. Binarization operation is performed on the clamped CT image according to prior knowledge of fatty tissue. Series of morphological operation is performed for first time on CT image data of a patient to obtain complete human body region and background region. Corresponding series of morphological operation is performed for second time on CT image data of the patient to obtain complete human body region and background region. Over-parameter of a convolutional neural network is optimized by utilizing Bayesian optimization algorithm. An original CT image is mapped to complete single-pixel human body segmentation according to classification result.
   USE - Method for extracting human body region in a CT image.
   ADVANTAGE - The method enables achieving high accuracy classification effect of human body area, and realizing automatic extraction of the patient CT image, and estimating lung analysis in the human body, and efficiently and accurately locating lung extracts.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method extracting human body region in a computed tomography image. '(Drawing includes non-English language text)'
DC S03 (Scientific Instrumentation); S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S03-E06B3A; S05-D02A1; S05-D02A5E; T01-J05B2; T01-J10B2; T01-J16A; T01-N01E1
IP G06N-003/04; G06T-007/00; G06T-007/11
PD CN109087296-A   25 Dec 2018   G06T-007/00   201927   Pages: 18   Chinese
AD CN109087296-A    CN10889991    07 Aug 2018
PI CN10889991    07 Aug 2018
UT DIIDW:201901458T
ER

PT P
PN CN109064521-A
TI Deep learning based CBCT de-artifact method, involves estimating residual distribution location between CBCT image and CT image to map of CBCT image, and obtaining artifact removed CBCT image by removing residual map from CBCT image.
AU YANG C
   XIE S
AE UNIV NANJING POSTS & TELECOM (UNPT-C)
GA 201900635G
AB    NOVELTY - The method involves obtaining a CBCT image from an IGRT system to generate a CBCT-CT image data set. Mutual information-based registration process is performed on the CBCT-CT image data set to increase matching degree of the CBCT image and a CT image in a part of a patient. The matched CBCT-CT image data set is divided into multiple sub-image blocks. Multiple sub-image blocks are subjected to secondary registration to obtain a CBCT-CT image sub-block group. Residual learning is performed on multiple sub-image blocks through a deep neural network. Residual distribution location between the CBCT image and the CT image is estimated to calculate a residual map of the to-be-tested CBCT image according to a model. Artifact removed CBCT image is obtained by removing the residual map from the CBCT image.
   USE - Deep learning based CBCT de-artifact method.
   ADVANTAGE - The method enables learning residual artifact frame distribution in pre-processed data by using a deep convolutional neural network structure, reducing training time so as to accelerate speed of training process, thus effectively removing bright and dark striped artifacts of the CBCT image without uneven artifact.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based CBCT de-artifact method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D02A1; S05-D02A5E; T01-J04D; T01-J10B2; T01-J10C5; T01-N01B3; T01-N01D1B; T01-N01E
IP G06T-011/00; G06T-007/30; G06N-003/08; G06N-003/04
PD CN109064521-A   21 Dec 2018   G06T-011/00   201927   Pages: 11   Chinese
AD CN109064521-A    CN10825059    25 Jul 2018
PI CN10825059    25 Jul 2018
UT DIIDW:201900635G
ER

PT P
PN CN109031440-A
TI Deep learning based gamma radiation imaging method, involves performing optical image registration process, obtaining composite image, and performing radioactive substance tracing and monitoring process based on composite image.
AU TANG X
   GONG P
   WANG P
   ZHU X
   ZHANG R
AE UNIV NANJING AERONAUTICS & ASTRONAUTICS (UNUA-C)
GA 2018A4915H
AB    NOVELTY - The method involves performing a deep learning network model training and testing process by using a coded image as a sample. A gamma radiation coded image of a detection target area is obtained by using a coding hole gamma camera. A gamma radiation coded image decoding process is performed by a deep learning network model to obtain a radiation hot spot distribution image of the target area. A depth image and an optical image of the target area are obtained by using a depth detecting vision system. A radiation hot spot distribution image, depth image and optical image registration process is performed. A composite image is obtained. A radioactive substance tracing and monitoring process is performed based on the composite image of an image display device.
   USE - Deep learning based gamma radiation imaging method.
   ADVANTAGE - The method enables shortening gamma radiation imaging time, improving image quality and realizing accuracy in obtaining the radiation hot spot distribution image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based gamma radiation imaging method. '(Drawing includes non-English language text)'
DC S03 (Scientific Instrumentation); W04 (Audio/Video Recording and Systems)
MC S03-G02B3; W04-W05A
IP G01V-005/00
PD CN109031440-A   18 Dec 2018   G01V-005/00   201927   Pages: 14   Chinese
AD CN109031440-A    CN10561908    04 Jun 2018
PI CN10561908    04 Jun 2018
UT DIIDW:2018A4915H
ER

PT P
PN CN109035172-A
TI Non-local mean method based on deep learning for medical ultrasound image removing method, involves constructing feature vector corresponding to each pixel in noise image and removing medical ultrasound image by non-local mean method.
AU ZHANG X
   YU H
   HUANG L
AE UNIV HUAZHONG SCI & TECHNOLOGY (UYHZ-C)
GA 2019000502
AB    NOVELTY - The non-local mean method involves using medical ultrasound image data set offline training principal component analysis network deep learning model (PCANet) to obtain a convolution template. The noise image is pre-filtered and the pre-filtered image is input into the trained convolution template to obtain the depth feature. The feature vector corresponding to each pixel in the noise image is constructed by using the depth feature and similarity between the pixels are calculated by using the Euclidean distance of the vector. The medical ultrasound image is removed by a non-local mean method based on the obtained similarity.
   USE - Non-local mean method based on deep learning for medical ultrasound image removing method.
   ADVANTAGE - The method adopts a deep learning model to mine the high-order features of the ultrasonic image, and uses the feature to characterize the similarity of the image structure, which overcomes the shortcomings of the traditional non-local mean using the gray information to characterize the similarity of the image structure, effectively inhibit speckle noise in the image, and protects the image detail information.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of removing medical ultrasound images by implementing a combined PCANet and non-local mean method. (Drawing includes non-English language text)
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D03E; T01-J04B2; T01-J10B1; T01-N01B3; T01-N01E
IP G06T-005/00; G06N-003/04
PD CN109035172-A   18 Dec 2018   G06T-005/00   201927   Pages: 14   Chinese
AD CN109035172-A    CN10898888    08 Aug 2018
PI CN10898888    08 Aug 2018
UT DIIDW:2019000502
ER

PT P
PN CN106778584-A
TI Deep and shallow feature superficial fusion based human facial age estimating method, involves obtaining and outputting human face image corresponding to age tag value after carrying out human face image age regression estimation.
AU SUN N
   GU Z
   LI X
AE UNIV NANJING POSTS & TELECOM (UNPT-C)
GA 2017386113
AB    NOVELTY - The method involves pre-processing face sample image data set. Initial convolutional neural network training is carried out to obtain multi-scale human face sample image regions. Human face sample multi-scale picture construction is carried out by the multi-scale human face sample image regions to obtain a convolutional neural network with high precision for performing human face identification process. A human face image is obtained and output corresponding to an age tag value after carrying out human face image age regression estimation.
   USE - Deep and shallow feature superficial fusion based human facial age estimating method.
   ADVANTAGE - The method enables improving human facial age estimating precision and capability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep and shallow feature superficial fusion based human facial age estimating method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-N01B3; T04-D02; T04-D03; T04-D04; T04-D07F1; T04-K03B
IP G06K-009/00; G06K-009/32; G06K-009/42; G06K-009/62
PD CN106778584-A   31 May 2017   G06K-009/00   201749   Pages: 7   Chinese
AD CN106778584-A    CN11120293    08 Dec 2016
PI CN11120293    08 Dec 2016
CP CN106778584-A
      CN104090972-A   UNIV BEIJING NORMAL (UYBN)   ZHANG L, WANG Y
      CN104504376-A   XIAMEN MEITUZHIJIA TECHNOLOGY CO LTD (XIAM-Non-standard)   ZENG Z, ZHANG W, FU S, LI P
      CN105512661-A   UNIV PLA INFORMATION ENG (UYPC)   LI K, LI Q, YOU X
      CN105975916-A   UNIV XIDIAN (UYXN)   NIU Z, WEI X, ZHOU M, YUAN B, GAO X, HUA G
      US8582807-B2   NEC LAB AMERICA INC (NIDE)   YANG M, ZHU S, LV F, YU K
UT DIIDW:2017386113
ER

PT P
PN CN106792507-A
TI Network data based wireless fidelity positioning method, involves obtaining position of network data by detecting client terminal based on positioning model, and determining position of detecting client terminal according to output result.
AU WANG B
AE SHANGHAI PHICOMM COMMUNICATION CO LTD (SHFX-C)
GA 201740813K
AB    NOVELTY - The method involves obtaining network data in wireless access point detection area when a signal sent by a detecting client terminal is received (S100), where the network data is multidimensional data. The network data is sent (S200) to a data layer for training a positioning model. A position of the network data is obtained (S300) by the detecting client terminal based on the positioning model of a network layer. A position of the detecting client terminal is determined according to an output result of an output layer. Signal strength data is received by a wireless access point.
   USE - Network data based wireless fidelity (Wi-Fi) positioning method.
   ADVANTAGE - The method enables training a deep neural network through training sample data so as to improve positioning accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a network data based wireless fidelity positioning server.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a network data based wireless fidelity positioning method. '(Drawing includes non-English language text)'
   Step for obtaining network data in wireless access point detection area when signal sent by detecting client terminal is received (S100)
   Step for sending network data to data layer for training positioning model (S200)
   Step for obtaining position of network data by detecting client terminal based on positioning model of network layer (S300)
DC T01 (Digital Computers); W01 (Telephone and Data Transmission Systems); W02 (Broadcasting, Radio and Line Transmission Systems)
MC T01-C03C; T01-J05B4P; T01-N01B3; T01-N02A3C; W01-A06C4E; W01-A06D; W01-A06G5C; W02-C03C1E
IP G01S-001/08; H04W-004/02; H04W-064/00
PD CN106792507-A   31 May 2017   H04W-004/02   201749   Pages: 12   Chinese
AD CN106792507-A    CN11046266    22 Nov 2016
PI CN11046266    22 Nov 2016
CP CN106792507-A
      CN101267374-A   UNIV QINGHUA (UYQI)   LI F, LI H, DING X, WU J
      CN101815308-A   HARBIN INST TECHNOLOGY (HAIT)   MA L, SUN Y, MENG W, SHA X, XU Y, TAN X
      CN103313387-A   WANG D (WANG-Individual)   WANG D
      CN103945332-A   UNIV TSINGHUA (UYQI)   ZHANG Y, XIAO L, XU X, ZHOU S, CHEN G
      CN105101408-A   CHANGSHU INST TECHNOLOGY (CHGS)   DAI H, LI K, SHEN T, GE L
      CN105872981-A   UNIV HOHAI CHANGZHOU CAMPUS (UYHO)   CHEN P, FAN X, NI J, ZHU J, WU L, LUO C
      CN105992156-A   PUTIAN INFORMATION TECHNOLOGY INST (POTV)   YAO Y, LV Z
UT DIIDW:201740813K
ER

PT P
PN CN109034371-A
TI Method for optimizing inference time calculation process of advanced learning model in electronic device, involves performing to-be-processed data processing operation and performing data outputting process after data processing operation.
AU CAO S
   WEI H
   LIN Y
   TAO H
AE BEIJING VION TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 2018A4844F
AB    NOVELTY - The method involves obtaining a data-to-be processed optimized depth learning model. The optimized depth learning model is provided with an optimization parameter. To-be-processed data processing operation is performed by parameter optimization with an optimization combination depth learning model. Data outputting process is performed after the data processing operation. A to-be-optimized convolution neural network model is established by a batch normalization structure according to training data.
   USE - Method for optimizing inference time calculation process of an advanced learning model in an electronic device (claimed).
   ADVANTAGE - The method enables saving additional computational overhead of a depth inference learning model, so that inference time calculation period of the advanced learning model is reduced, thus reducing power consumption.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for optimizing inference time calculation process of an advanced learning model in an electronic device
   (2) a system for optimizing inference time calculation process of an advanced learning model in an electronic device
   (3) a computer readable storage medium comprises set of instructions for optimizing inference time calculation process of an advanced learning model in an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for optimizing inference time calculation process of an advanced learning model in an electronic device. '(Drawing includes non-English language text)'
DC S04 (Clocks and Timers); T01 (Digital Computers)
MC S04-C03C; T01-D03; T01-J04A; T01-J04B2; T01-J05B2B; T01-N01B3; T01-S03
IP G06N-003/04; G06N-003/08; G06N-005/04
PD CN109034371-A   18 Dec 2018   G06N-003/04   201926   Pages: 14   Chinese
AD CN109034371-A    CN10685004    27 Jun 2018
PI CN10685004    27 Jun 2018
UT DIIDW:2018A4844F
ER

PT P
PN CN108122173-A
TI Deep belief network based multi-field load forecasting method, involves obtaining training data set to train load predicting initial model, obtaining load forecasting model, and forecasting load in different industries by forecasting model.
AU ZHANG J
   LIU H
   FANG R
   XIE D
   CHEN F
   GAO X
   ZHAO X
   YAN J
   WANG Y
   ZHOU Y
   CHEN Z
   XUE R
   CHEN Y
AE STATE GRID CORP CHINA (SGCC-C)
   STATE GRID SHANDONG ELECTRIC POWER CO (SGCC-C)
   UNIV NORTH CHINA ELECTRIC POWER (UYHD-C)
GA 201845642S
AB    NOVELTY - The method involves analyzing load influencing factor of an industry. Different prediction types of the industry are determined according to an analysis result. A load predicting initial model is established corresponding to the different prediction types of the industry. A training data set is obtained to train the load predicting initial model. A load forecasting model is obtained. A load in different industries is forecasted by the load forecasting model. A different predictive input variables and output layer variables are determined. A hidden layer number and a node number are set by the load predicting initial model. Training parameters of the load predicting initial model are obtained.
   USE - Deep belief network based multi-field load forecasting method.
   ADVANTAGE - The method enables providing decision gist for a power supply enterprise and improving accuracy and efficiency in a distribution network load prediction process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep belief network based multi-field load forecasting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-N01A2; T01-N01B3
IP G06Q-010/04; G06Q-050/06
PD CN108122173-A   05 Jun 2018   G06Q-050/06   201841   Pages: 9   Chinese
AD CN108122173-A    CN11385664    20 Dec 2017
PI CN11385664    20 Dec 2017
UT DIIDW:201845642S
ER

PT P
PN CN109003299-A
TI Method for calculating hemorrhage amount in clinic based on depth learning, involves calculating cerebral hemorrhage volume according to dividing result, and generating structured report of cerebral hemorrhage site and bleeding volume.
AU ZHANG R
   GONG Q
   XIA C
   CHEN K
AE BEIJING INFERVISION TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 2018A37548
AB    NOVELTY - The method involves obtaining cerebral CT image data (S1). A segmentation model of a hemorrhage area is established based on deep learning design (S2). The cerebral CT image data is inputted to a trained model to obtain a dividing result (S3). Cerebral hemorrhage volume is calculated according to the dividing result (S4). Structured report of a cerebral hemorrhage site and bleeding volume are generated (S5). Hemorrhage CT image data of pixel level is collected from the cerebral CT image data to perform a marking process of a bleeding area.
   USE - Method for calculating hemorrhage amount in a clinic based on depth learning.
   ADVANTAGE - The method enables accurately and rapidly calculating hemorrhage amount to provide decision information of clinic in an effective manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for calculating hemorrhage amount in a clinic based on depth learning. '(Drawing includes non-English language text)'
   Step for obtaining cerebral CT image data (S1)
   Step for establishing segmentation model of hemorrhage area based on deep learning design (S2)
   Step for inputting cerebral CT image data to trained model to obtain dividing result (S3)
   Step for calculating cerebral hemorrhage volume according to dividing result (S4)
   Step for generating structured report of cerebral hemorrhage site and bleeding volume (S5)
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-G02G9; T01-J04A; T01-J10B2; T01-J30A
IP G06T-007/62; G06N-003/04; G06T-007/11
PD CN109003299-A   14 Dec 2018   G06T-007/62   201925   Pages: 10   Chinese
AD CN109003299-A    CN10730121    05 Jul 2018
PI CN10730121    05 Jul 2018
UT DIIDW:2018A37548
ER

PT P
PN CN108982405-A
TI Depth learning based oil water content measuring instrument, has distance between lens and reflector fixed with each other, and signal processing module is provided with infrared spectrum collection module and embedded computer module.
AU DAI Z
   WANG Y
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 2018A2031D
AB    NOVELTY - The instrument has an optical sensitive structure comprising a Y-type optical fiber, a transmission optical fiber and an optical fiber probe. Input and output ends of the optical fiber probe are connected with the transmission optical fiber. The optical fiber probe is provided with an incidence optical fiber, a collecting optical fiber and lens and mirror. A distance between the lens and a reflector are fixed with each other. A signal processing module is provided with an infrared spectrum collection module and an embedded computer module. The optical fiber probe transmits a light sensor in the light source module for converging an oil product by the incidence optical fiber.
   USE - Depth learning based oil water content measuring instrument.
   ADVANTAGE - The instrument transmits absorption light by the signal processing module, obtains infrared spectrum characteristic curve through an infrared spectrum acquisition module and the signal processing module, analyzes infrared spectrum analysis characteristic curve by an artificial intelligent analysis algorithm according to deep learning to obtain water content of oil.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning based oil water content measuring.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a depth learning based oil water content measuring instrument. '(Drawing includes non-English language text)'
DC S03 (Scientific Instrumentation); T01 (Digital Computers); V07 (Fibre-optics and Light Control)
MC S03-E04A5B; S03-F09A; T01-J30A; V07-F01A1; V07-F02A; V07-N01
IP G01N-021/3577
PD CN108982405-A   11 Dec 2018   G01N-021/3577   201925   Pages: 11   Chinese
AD CN108982405-A    CN11004953    30 Aug 2018
PI CN11004953    30 Aug 2018
UT DIIDW:2018A2031D
ER

PT P
PN CN108986085-A
TI CT image pulmonary nodules detecting method, involves performing adjustment image nodule analysis process, obtaining and outputting determined nodule area in adjustment image, and obtaining analysis information of determined nodule area.
AU DOU Q
   LIU Q
   CHEN H
AE SHENZHEN IMSIGHT MEDICAL TECHNOLOGY CO (SHEN-Non-standard)
GA 2018A1947M
AB    NOVELTY - The method involves obtaining medical treatment instruction corresponding to a target CT image when record processing instruction is received. Display parameter of the target CT image is obtained and adjusted. A three-dimensional convolutional neural pixel pre-dividing network is obtained corresponding to the target CT image. An adjustment image nodule analysis process is performed by using a three-dimensional convolutional neural network classifier. A determined nodule area in the adjustment image are obtained and output. Analysis information of the determined nodule area is obtained.
   USE - CT image pulmonary nodules detecting method.
   ADVANTAGE - The method enables improving current artificial lung nodule detection efficiency and precision.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a CT image pulmonary nodules detecting device
   (2) a readable storage medium for storing set of instruction for detecting CT image pulmonary nodules.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a CT image pulmonary nodules detecting method. '(Drawing includes non-English language text)'
DC S03 (Scientific Instrumentation); S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S03-E06B3A; S05-D02A1; S05-D02A5E; T01-J10B2; T01-N01E
IP G06T-007/00; G06N-003/04
PD CN108986085-A   11 Dec 2018   G06T-007/00   201925   Pages: 16   Chinese
AD CN108986085-A    CN10692741    28 Jun 2018
PI CN10692741    28 Jun 2018
UT DIIDW:2018A1947M
ER

PT P
PN CN108986912-A
TI Deep learning based intelligent traditional Chinese medicine stomach disease tongue image information processing method, involves outputting probability of judging results and syndrome type as result of disease syndromes score and severity.
AU LIU Y
AE BEIJING SANYI WISDOM TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 2018A3344X
AB    NOVELTY - The method involves performing hierarchical quantization on collected patient symptom data to obtain quantization information of the symptoms. A convolutional neural network is designed based on collected tongue image symptom data for extracting tongue image features. The tongue image features are combined with the quantization information of the symptoms in a connecting layer. A multi-level neural network is trained to obtain an analyzing and judging result of a patient. A model is trained based on the judging result of the patient and a judging result of an expert symptom. A network is obtained according to the trained model. A probability of the judging results and a syndrome type is output by the network as a result of disease syndromes score and severity.
   USE - Deep learning based intelligent traditional Chinese medicine stomach disease tongue image information processing method.
   ADVANTAGE - The method enables determining symptoms and severity of the patient after a tongue image of patient and an answer result of the symptoms of the patient are obtained so as to improve diagnostic efficiency and increase diagnosis result accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning based intelligent traditional Chinese medicine stomach disease tongue image information processing method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D08A; T01-J10B2; T01-N01B3; T01-N01E1
IP G16H-050/20; G06N-003/04; G06N-003/08
PD CN108986912-A   11 Dec 2018   G16H-050/20   201925   Pages: 11   Chinese
AD CN108986912-A    CN10763009    12 Jul 2018
PI CN10763009    12 Jul 2018
UT DIIDW:2018A3344X
ER

PT P
PN CN106778668-A
TI Combined random sample consensus and convolutional neural network robust lane line detecting method, involves obtaining original image by acquisition device, and obtaining lane line of user by processing de-noising image by neural network.
AU CHEN H
   CHEN C
   XIE C
   YE D
   REN F
   WANG Z
AE MINGJIAN XIAMEN TECHNOLOGY CO LTD (MING-Non-standard)
GA 2017386093
AB    NOVELTY - The method involves obtaining an original image by an image acquisition device. Gaussian smoothing process is performed to obtain a smooth image. Smooth image edge extraction process is performed to obtain an edge feature image. Image de-noising process is performed to obtain a de-noising image. Maximum iteration time is determined to obtain a final lane line. Vanishing point difference threshold is detected to detect an interrupt lane by random sample consensus (RANSAC) algorithm. A lane line of a user is obtained by processing the de-noising image by a convolutional neural network (CNN).
   USE - Combined RANSAC and CNN robust lane line detecting method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a combined RANSAC and CNN robust lane line detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10A; T01-J10B2
IP G06K-009/00; G06K-009/54
PD CN106778668-A   31 May 2017   G06K-009/00   201749   Pages: 11   Chinese
AD CN106778668-A    CN11254172    30 Dec 2016
PI CN11254172    30 Dec 2016
CP CN106778668-A
      CN102592114-A   UNIV HENAN TECHNOLOGY (UYHY)   FAN C, FU H, DI S, ZHANG R, ZHANG D, YANG T, LI Y, MA H, HOU L
      CN102722705-A   UNIV WUHAN (UYWU)   LI B, LI M, LI Q, SONG X, XIAO J, YI B, SHEN S, ZHU S
      CN103902985-A   UNIV ANHUI POLYTECHNIC (UYAP)   CHEN M, LANG L, CHAI C
      CN105261020-A   UNIV GUILIN ELECTRONIC TECHNOLOGY (UYGE)   DING J, DING Y, SUN K, ZHANG Y, ZHAO R, GUAN X
      KR2014080105-A   UNIV ULSAN FOUND IND COOP (UYUL-Non-standard)   CHO S B, JANG Y M, LIM D H, KIM S H
CR CN106778668-A
      MOHAMED ALY: "Real time Detection of Lane Markers in Urban Streets", IEEE,relevantClaims[1-9],relevantPassages[]
      JIHUN KIM: "Lane Detection System using CNN", IEMEK J. EMBED. SYS. APPL,relevantClaims[1-9],relevantPassages[]
      JIHUN KIM: "Robust Lane Detection Based On Convolutional Neural Network and Random Sample Consensus", SPRINGER,relevantClaims[1-9],relevantPassages[]
      : "", ,relevantClaims[1-9],relevantPassages[]
UT DIIDW:2017386093
ER

PT P
PN CN106780608-A
TI Method for estimating pose information in mobile device, involves extracting sample data based on multi-layer depth learning mode to estimate mobile device pose information and output position orientation information.
AU YANG D
AE BEIJING HORIZON ROBOTICS TECHNOLOGY RES (BEIJ-Non-standard)
GA 2017385659
AB    NOVELTY - The method involves obtaining sample data of working environment by an environment sensor. The sample data is extracted based on multi-layer depth learning mode to estimate mobile device pose information and output position orientation information. Characteristic data of the sample is extracted by using a depth learning feature model. The mobile device pose information is detected in a working environment. The mobile device pose information is corrected according to the detection result. Depth characteristic of the depth learning feature model is determined.
   USE - Method for estimating pose information in a mobile device (claimed).
   ADVANTAGE - The method enables improving processing efficiency.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for estimating pose information in a mobile device
   (2) a computer program product comprising set of instruction for estimating pose information in a mobile device
   (3) a computer-readable storage medium comprising set of instruction for estimating pose information in a mobile device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for estimating pose information in a mobile device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J05B4P; T01-J30A; T01-M06A1; T01-S03; T04-D07D3; W04-W05A
IP G06T-007/73
PD CN106780608-A   31 May 2017   G06T-007/73   201749   Pages: 19   Chinese
AD CN106780608-A    CN11050896    23 Nov 2016
PI CN11050896    23 Nov 2016
CP CN106780608-A
      CN104236548-A   UNIV TSINGHUA (UYQI)   LI D, LI Q, TANG L, CHENG N, YANG S
      CN104850120-A   UNIV WUHAN SCI & TECHNOLOGY (UWSC)   CHEN Y, WU H, ZHONG R, ZHANG Q, LI W, NIU H
      CN105783913-A   UNIV SUN YAT-SEN (UYSY)   CHENG H, LIN L, ZHU Q, WU Y
      CN105856230-A   JIAN Y (JIAN-Individual)   AI Q, JIAN Y, LIU S, YU J, ZHENG K
CR CN106780608-A
      XIANG GAO: "Unsupervised learning to detect loops using deep neural networks", SPRINGER SCIENCE,relevantClaims[1-14,16],relevantPassages[1-4]
UT DIIDW:2017385659
ER

PT P
PN CN109104301-A
TI Method for predicting network popularity for drama program based on deep learning model, involves selecting output information of network popularity prediction model as network popularity prediction result of to-be-predicted drama program.
AU LI S
   LI L
   SUN L
   CHEN F
AE GZT TECHNOLOGY CO LTD (GZTT-Non-standard)
GA 201902938A
AB    NOVELTY - The method involves detecting a historical search amount of a sample object. A network popularity prediction model is constructed by using the historical search amount of the sample object. Characteristic information of to-be-predicted drama program is obtained by receiving external input information, where the characteristic information of the to-be-predicted drama program includes drama program name, director information, actor information and manufacturing enterprise. The characteristic information of a preset object is obtained. Output information of the network popularity prediction model is selected as a network popularity prediction result of the to-be-predicted drama program within a preset time period to obtain a drama program issuing information parameter.
   USE - Method for predicting network popularity for a drama program based on a deep learning model.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for predicting network popularity for a drama program based on deep learning model
   (2) a computer readable medium for storing a set of instructions for predicting network popularity for a drama program based on deep learning model.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for predicting network popularity for a drama program based on a deep learning model. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W01 (Telephone and Data Transmission Systems); W03 (TV and Broadcast Radio Receivers); W04 (Audio/Video Recording and Systems)
MC T01-J05B2; T01-L02; T01-N01B3; T01-N03A2; T01-S03; W01-A06A; W01-A06D; W03-A16C5A; W04-W05A
IP H04L-012/24; H04N-021/44; H04N-021/466; H04N-021/81
PD CN109104301-A   28 Dec 2018   H04L-012/24   201924   Pages: 12   Chinese
AD CN109104301-A    CN10794254    19 Jul 2018
PI CN10794254    19 Jul 2018
UT DIIDW:201902938A
ER

PT P
PN CN108968916-A
TI Method for correcting respiratory motion by utilizing computer device, involves obtaining multiple groups of sample image, and correcting respiratory breathing movement of to-be-corrected image by using trained convolutional neural network.
AU YANG F
   FU T
AE ARIEMEDI MEDICAL TECHNOLOGY BEIJING CO (ARIE-Non-standard)
GA 2018A17919
AB    NOVELTY - The method involves obtaining multiple groups of sample images. A composite expiration deformation field set and a composite suction deformation field set are obtained according to multiple groups of the sample image. A breath simulation image set is obtained according to the composite-expiration deformation field set. A suction simulation image set is obtained according to the composite absorbing deformation field set. A convolutional neural network is obtained by training a preset convolutional neural network. A respiratory breathing movement of a to-be-corrected image is corrected by using the trained convolutional neural network.
   USE - Method for correcting respiratory motion by utilizing a computer device (claimed).
   ADVANTAGE - The method enables performing training function of a convolutional neural network so as to correct a respiratory motion of respiratory organs in an image to-be-corrected by using the trained convolutional neural network, thus improving correction accuracy of respiratory motion.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for correcting respiratory motion by utilizing a computer device
   (2) a storage medium for storing a set of instructions for performing a method for correcting respiratory motion by utilizing a computer device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for correcting respiratory motion by utilizing a computer device. '(Drawing includes non-English language text)'
DC P31 (Diagnosis, surgery (A61B).); S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-A05A; T01-J06A; T01-J16C1; T01-S03
IP A61B-005/00
PD CN108968916-A   11 Dec 2018   A61B-005/00   201924   Pages: 34   Chinese
AD CN108968916-A    CN10626980    19 Jun 2018
PI CN10626980    19 Jun 2018
UT DIIDW:2018A17919
ER

PT P
PN US2018166066-A1; US10249292-B2
TI Computing system for performing automatic speech recognition on music, has storage device for storing computer-executable code such that processor segments audio data using neural network to identify set of change points of audio data.
AU DIMITRIADIS D B
   HAWS D C
   PICHENY M
   SAON G
   THOMAS S
AE INT BUSINESS MACHINES CORP (IBMC-C)
GA 201846643W
AB    NOVELTY - The system has a storage device for storing audio data including speech of first speaker type, speech of second speaker type, and silence. The storage device stores a computer-executable code such that a processor segments the audio data using a long short-term memory (LSTM) recurrent neural network (RNN) (100) to identify a set of change points of the audio data that divide the audio data into a set of segments, where each change point is a transition from a set of the first speaker type, the second speaker type, and the silence to another set of the first speaker type, the second speaker type, and the silence. The processor performs speech recognition on the segments that correspond to the first speaker type and the second speaker type.
   USE - Computing system for performing automatic speech recognition on audio data i.e. music (claimed). Uses include but are not limited to server computing device, desktop computer, laptop computer, and mobile computing device such as smartphone and tablet computing device.
   ADVANTAGE - The system allows an LSTM RNN model in speaker diarization to concurrently segment the audio data and assign labels to the partitioned segments, thus improving accuracy of the speech recognition.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a computer program product comprising a set of instructions for performing automatic speech recognition on audio data
   (2) a method for performing automatic speech recognition on audio data.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a neural network.
   LSTM RNN (100)
   Input node (102)
   Output node (104)
   Hidden node (106)
   Connections (108, 110)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-C08A; T01-J16B; T01-J18; T01-N02A3C; T01-S03; W04-V01; W04-V04A1; W04-V05
IP G10L-015/04; G10L-015/06; G10L-015/16; G10L-025/81; G10L-021/00; G10L-015/00; G10L-025/30; G10L-025/78; G10L-017/18
PD US2018166066-A1   14 Jun 2018   G10L-015/04   201841   Pages: 21   English
   US10249292-B2   02 Apr 2019   G10L-021/00   201924      English
AD US2018166066-A1    US379010    14 Dec 2016
   US10249292-B2    US379010    14 Dec 2016
FD  US10249292-B2 Previous Publ. Patent US2018166066
PI US379010    14 Dec 2016
UT DIIDW:201846643W
ER

PT P
PN CN106776553-A
TI Deep learning based non-symmetrical text hash method, involves extracting training set of text semantic tags, and training neural network parameters according to back propagation algorithm, determining neural network hash code.
AU CHEN Z
   PAN R
AE UNIV SUN YAT-SEN (UYSY-C)
GA 201738659L
AB    NOVELTY - The method involves extracting a training set of text semantic tags. A semantic similarity between the samples is calculated. A training set text code is calculated based on the training set of the text semantic tags. A desired semantic hash code is determined based on calculated semantic similarity samples. A training set text input is transmitted into a neural network. A neural network hash code and a deviation code are determined based on back propagation algorithm. Neural network parameters are trained according to the back propagation algorithm.
   USE - Deep learning based non-symmetrical text hash method.
   ADVANTAGE - The method enables improving the efficiency of text hash study.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a deep learning based non-symmetrical text hash method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-E04; T01-J05B2B; T01-J05B4P; T01-J11A1; T01-J16C3; T01-N01B3; T01-N01D; T01-N02B1A
IP G06F-017/27; G06F-017/30
PD CN106776553-A   31 May 2017   G06F-017/27   201749   Pages: 6   Chinese
AD CN106776553-A    CN11117022    07 Dec 2016
PI CN11117022    07 Dec 2016
UT DIIDW:201738659L
ER

PT P
PN CN106777011-A
TI Multi-task learning based text classifying method, involves extracting characteristics of task from document by using convolutional neural network, and obtaining learning classifiers of another task by using document representation.
AU ZHANG Z
   PAN R
AE UNIV SUN YAT-SEN (UYSY-C)
GA 201738648W
AB    NOVELTY - The method involves learning a current task representation of document by using word vector and a bi-directional circulation network. Characteristics of a task from the document are extracted by using a convolutional neural network. Feature learning classifiers of another task is obtained by using current task document representation. A normal distribution random initialization word vector matrix is formed. Word vector and bi-directional circulation network learning process is performed to represent a current task. Normal distribution random initialization process is performed.
   USE - Depth multi-task learning based text classifying method.
   ADVANTAGE - The method enables improving learning capability of the convolutional neural network, expanding document semantic representation and effectively avoiding insufficient training data. The method enables extracting assistant task feature by using a bottom layer characteristic to effectively transfer the task and improving performance of text classification by using characteristic of another task.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-task learning based text classifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-F02; T01-J04C; T01-J05B2; T01-J05B4P; T01-J16C3; T01-N01B3; T01-N01D2; W04-W05A
IP G06F-017/30; G06N-003/08
PD CN106777011-A   31 May 2017   G06F-017/30   201749   Pages: 8   Chinese
AD CN106777011-A    CN11117038    07 Dec 2016
PI CN11117038    07 Dec 2016
UT DIIDW:201738648W
ER

PT P
PN CN109085752-A
TI Aluminum electrolysis preference algorithm optimization involves selecting control parameters to affect current efficiency, cell voltage, and perfluorinated emissions constitute decision variable.
AU YI J
   BAI J
   CHEN X
   WU L
   ZHOU W
   CHEN S
AE UNIV CHONGQING SCI & TECHNOLOGY (UYNQ-C)
GA 201901495W
AB    NOVELTY - Aluminum electrolysis preference algorithm optimization involves selecting control parameters to affect current efficiency, cell voltage, and perfluorinated emissions constitute decision variable. The aluminum electrolysis industry site is selected for collecting set of decision variables. The corresponding current efficiency, tank voltage and perfluorinated emissions, quantity and the aluminum energy consumption are used as outputs, where the sample is trained and tested by recurrent neural network to establish a production process model of four aluminum electrolysis cells. A-dominated preference multi-objective differential evolution algorithm is combined with the algorithm based on the pre-set expected value of the decision maker as reference point. The aluminum electrolysis industrial site is controlled according to the control parameter in the optimal decision variable to achieve the purpose of energy saving, emission reduction and consumption reduction.
   USE - Method for optimizing algorithm for aluminum electrolysis preference based on angle dominance.
   ADVANTAGE - The multi-objective optimization algorithm effectively improves current efficiency, reduces cell voltage, greenhouse gas emissions and energy consumption per ton of aluminum to meet preferences of policy makers, and thus achieves goal of energy saving and emission reduction.
DC M28 (Electrolytic and electrothermic production and refining of non-ferrous metals - excluding heat treatment (C25).); T01 (Digital Computers); T06 (Process and Machine Control); X25 (Industrial Electric Equipment)
MC M28-C02; T01-J16C4; T06-A05C; T06-A07B; X25-R02
IP G05B-013/04; G06N-003/08; C25C-003/20
PD CN109085752-A   25 Dec 2018   G05B-013/04   201923   Pages: 18   Chinese
AD CN109085752-A    CN10193063    09 Mar 2018
PI CN10193063    09 Mar 2018
UT DIIDW:201901495W
ER

PT P
PN CN106778740-A
TI Depth learning based freight car detection system fault image detection method, involves detecting non-fault image by detection module to remove fault image, and observing suspected fault image by indoor train examination staff person.
AU SUN J
   XIAO Z
   XIE Y
AE UNIV BEIHANG (UNBA-C)
GA 201738607D
AB    NOVELTY - The method involves establishing an image characteristic extracting module. Image weight parameter is obtained by a parameter initialization network model. Fault image of a test sample is determined by using a network training model. The fault image is detected by a detection network. Characteristic expression of the image is obtained by the image feature extracting module. Image key area, non-fault image and suspected fault image are detected by a detection module to remove the fault image. The suspected fault image is observed by an indoor train examination staff person.
   USE - Depth learning based TFDS fault image detection method.
   ADVANTAGE - The method enables filtering the fault image, removing suspected error image according to human eyes viewing image discrimination, and greatly reducing working quantity of indoor train examination staff and working intensity of detecting staff.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a depth learning based TFDS fault image detection method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J10B1; T01-J10B2; T01-J10B3A; T01-N01B3A; W04-W05A
IP G06K-009/32; G06K-009/46; G06K-009/62; G06N-003/04; G06N-099/00
PD CN106778740-A   31 May 2017   G06K-009/32   201749   Pages: 11   Chinese
AD CN106778740-A    CN11110940    06 Dec 2016
PI CN11110940    06 Dec 2016
UT DIIDW:201738607D
ER

PT P
PN CN106227792-A
TI Information pushing method, involves clustering coded data with history information, judging whether number of history information is less than pre-determined threshold value, and transmitting history information to user.
AU WANG W
   SHI P
AE BAIDU ON-LINE NETWORK TECHNOLOGY CO LTD (BIDU-C)
GA 201680123D
AB    NOVELTY - The method involves obtaining history information. Input data and output data of a depth learning model are obtained. The output data is encoded to obtain coded data. The coded data is clustered with the history information. A judgment is made to check whether a number of the history information is less than a pre-determined threshold value. The history information is transmitted to a user. The output data is encoded primarily to obtain primary encoded data. The output data is encoded secondarily to obtain secondary encoded data. A history information cluster is obtained.
   USE - Information pushing method.
   ADVANTAGE - The method enables improving information clustering efficiency and information pushing efficiency and pertinence rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an information pushing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an information pushing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B3; T01-J30A
IP G06F-017/30; G06N-003/08
PD CN106227792-A   14 Dec 2016   G06F-017/30   201705   Pages: 19   Chinese
AD CN106227792-A    CN10576501    20 Jul 2016
PI CN10576501    20 Jul 2016
CP CN106227792-A
      CN103353872-A   UNIV DALIAN TECHNOLOGY (UYDA)   KONG X, NIU Y, XIA F
      CN103995823-A   UNIV NANJING POSTS & TELECOM (UNPT)   CAO J, CHEN D, LI L, LI S, XU X, ZHOU Y, MA R
      CN105205046-A   ZHANG Q (ZHAN-Individual);  ZHEJIANG MINGTAI INFORMATION TECHNOLOGY (ZHEJ-Non-standard)   SHANG S, WANG Z, ZHANG Q
      CN105279288-A   UNIV SHENZHEN (UYSZ)   CHEN L, LI X, WANG N
      CN105357232-A   STARGIS TIANJIN TECHNOLOGY DEV CO LTD (STAR-Non-standard)   JIANG Y, WANG H
      US20150339570-A1      
UT DIIDW:201680123D
ER

PT P
PN CN108960260-A
TI Medical image classification model generating method, involves obtaining training data according to initial depth learning, and forming medical image classification model by base classification model, which comprises specific values.
AU WANG X
   LUAN X
   MENG J
   HE G
AE NEUSOFT CORP (NEUS-C)
GA 2018A0241Q
AB    NOVELTY - The method involves obtaining original medical image by image characteristics. The medical image is extracted. Image feature and classification label are obtained corresponding to the medical image. Transformed image of the medical image is generated. First training data of group is obtained for forming an initial classification model. A base classification model is generated. Second training data is obtained according to initial depth learning. Medical image classification model is formed by the base classification model, which comprises specific values. The classification label is used for training a support vector machine model.
   USE - Medical image classification model generating method.
   ADVANTAGE - The method enables determining weighing value of a medical image classification model by the medical image so as to eliminate subjectivity of influence in an accurate manner and improving commonality of medical image classification model.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a medical image classification model generating device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a medical image classification model generating method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC S05-D08A; S05-P; T01-E01A; T01-J05B2; T01-J06A1; T01-J10B1; T01-J10B2; T01-J30A; W04-W05A
IP G06K-009/46; G06K-009/62; G06N-003/04
PD CN108960260-A   07 Dec 2018   G06K-009/46   201922   Pages: 22   Chinese
AD CN108960260-A    CN10765946    12 Jul 2018
PI CN10765946    12 Jul 2018
UT DIIDW:2018A0241Q
ER

PT P
PN CN108921208-A
TI Deep learning based unbalanced data sampling and modeling method, involves adding abstract characteristic to concentrate category balance, constructing optimal DBN model, and training optimal DBN model.
AU YU M
   DENG R
   XU T
   ZHAO M
   GAO J
   ZHAO Y
AE UNIV TIANJIN (UTIJ-C)
GA 201898191T
AB    NOVELTY - The method involves constructing a cluster association matrix. Multiple samples are obtained by performing clustering combination through fusion algorithm according to the cluster association matrix. Sampling process is performed on the samples by using smote algorithm to obtain minority sample set. A model of the sample set is constructed. Data set type balance is performed. Abstract characteristic is extracted by performing balance auto-encoder process. The abstract characteristic is added to a concentrate category balance. An optimal DBN model is constructed. The optimal DBN model is trained.
   USE - Deep learning based unbalanced data sampling and modeling method.
   ADVANTAGE - The method enables increasing processing capability and unbalanced data sampling accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based unbalanced data sampling and modeling method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B3; T01-J06A1; T01-J10B2; T01-J30A
IP G06K-009/62; G16H-050/20
PD CN108921208-A   30 Nov 2018   G06K-009/62   201922   Pages: 7   Chinese
AD CN108921208-A    CN10637767    20 Jun 2018
PI CN10637767    20 Jun 2018
UT DIIDW:201898191T
ER

PT P
PN CN108898595-A
TI Chest disease detection model establishing method, involves inputting pre-processed image data to convolutional neural network model for realizing train process to obtain training model for realizing breast disease detection process.
AU CHAI X
   GUO N
   ZUO P
   MENG B
   WANG C
   LI A
AE HUIYING MEDICAL TECHNOLOGY BEIJING CO (HUIY-Non-standard)
GA 201897028R
AB    NOVELTY - The method involves obtaining chest X-ray images of patients with breast disease. A segmentation process is performed to overall chest X-ray images by using network training breast segmentation model to obtain a segmented chest region X-ray image. An image preprocessing operation is performed to the segmented chest region X-ray image to obtain pre-processed image data. The pre-processed image data is inputted to a convolutional neural network model for realizing a train process to obtain a training model for realizing breast disease detection process.
   USE - Chest disease detection model establishing method.
   ADVANTAGE - The method accurately predicting disease type and area of disease in chest so as to effectively reduce diagnosis time of doctor, thus reducing misdiagnosis rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a chest disease detection method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a chest disease detection model establishing method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D02A; S05-P; T01-J10B2; T01-N01B3
IP G06T-007/00; G06N-003/04
PD CN108898595-A   27 Nov 2018   G06T-007/00   201922   Pages: 14   Chinese
AD CN108898595-A    CN10682166    27 Jun 2018
PI CN10682166    27 Jun 2018
UT DIIDW:201897028R
ER

PT P
PN CN108898606-A
TI Three-dimensional medical image automatic segmentation method, involves establishing statistical shape model based on initial segmentation result, and optimizing medical image segmentation for obtaining outline of medical image.
AU HU H
   LIU H
   PAN N
   LI X
   GAO Z
AE UNIV SOUTH CENT NATIONALITIES (USOC-C)
GA 201897028G
AB    NOVELTY - The method involves obtaining a saliency map of a three-dimensional medical image by using a visual attention model. The saliency map of the medical image is inputted to a depth learning neural network for training parameters of depth in the depth learning neural network. The saliency map is divided by using the visual attention model. A medical image segmentation training process is performed for obtaining initial segmentation result. A statistical shape model is established based on the initial segmentation result. A medical image segmentation optimizing process is performed by the statistical shape model for obtaining outline of the medical image.
   USE - Three-dimensional medical image automatic segmentation method.
   ADVANTAGE - The method enables reducing calculated amount of statistical shape model matching operation so as to realize statistical shape model using processing for dividing three-dimensional medical image in quickly and accurately manners.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a three-dimensional medical image automatic segmentation method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a three-dimensional medical image automatic segmentation method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D08A; S05-G02G9; T01-J03; T01-J10B2; T01-J10B3A; T01-N01B3; T01-N01E
IP G06T-007/12; G06T-007/00
PD CN108898606-A   27 Nov 2018   G06T-007/12   201922   Pages: 21   Chinese
AD CN108898606-A    CN10634693    20 Jun 2018
PI CN10634693    20 Jun 2018
UT DIIDW:201897028G
ER

PT P
PN CN108898642-A
TI Convolutional neural network-based sparse-angle CT imaging method, involves obtaining network needed training data set, and reconstructing CT projection data to obtain clinical sparse angle with high quality.
AU KANG Y
   LIU J
   LIU T
   ZHANG P
   WANG J
   DOU Y
   XIU Y
   ZHANG K
AE UNIV ANHUI POLYTECHNIC (UYAP-C)
GA 201897027M
AB    NOVELTY - The method involves obtaining a network needed training data set. A restoration convolutional neural network is constructed. CT projection data is stored in the restoration convolutional neural network. Depth of the CT projection data is obtained. Depth of a trained convolutional neural network is established. The CT projection data is reconstructed to obtain a clinical sparse angle with high quality. The CT projection data is obtained through a TV iteration reconstruction algorithm. A ReLU activation function is realized. A loss function of the needed training data set is realized.
   USE - Convolutional neural network-based sparse-angle CT imaging method.
   ADVANTAGE - The method enables effectively performing a present reconstruction process on projection data, and solving height down-sampling details easy t loss and serious artifacts problem, and improving imaging quality.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a convolutional neural network-based sparse-angle CT imaging method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D02A1; S05-G02G3; T01-J10C; T01-N01B3
IP G06T-011/00; G06N-003/04; G06N-003/08
PD CN108898642-A   27 Nov 2018   G06T-011/00   201922   Pages: 18   Chinese
AD CN108898642-A    CN10557240    01 Jun 2018
PI CN10557240    01 Jun 2018
UT DIIDW:201897027M
ER

PT P
PN CN108108716-A
TI Deep belief network based loop detecting method, involves obtaining image characteristics, determining similarity of historical key frame, determining output characteristics of current frame, and completing loop detecting process.
AU XU T
   ZHANG D
   ZHANG F
AE CETC ACAD INFORMATION SCI INNOVATION (CETC-C)
GA 201844844H
AB    NOVELTY - The method involves selecting rich detail information of an image block. Computation amount is reduced from video frame data. A gray value of the image block is calculated. Input deep belief network training process is performed. Depth vector of a DBN belief network is obtained by a last hidden layer. Image characteristics are obtained. Similarity of a historical key frame is determined. Output characteristics of a current frame are determined. Loop detecting process is completed. Image key points are extracted based on characteristic point calculating process.
   USE - Deep belief network based loop detecting method.
   ADVANTAGE - The method enables realizing loop detecting operation in a simple manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep belief network based loop detecting method. '(Drawing includes non-English language text)'
DC S01 (Electrical Instruments); T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC S01-H07A; T01-J10B2; T01-J10B3A; T01-N01B3; T04-D03; T04-D04
IP G06K-009/00; G06K-009/46; G06K-009/62; G06N-003/08
PD CN108108716-A   01 Jun 2018   G06K-009/00   201840   Pages: 9   Chinese
AD CN108108716-A    CN11498176    29 Dec 2017
PI CN11498176    29 Dec 2017
UT DIIDW:201844844H
ER

PT P
PN CN108899087-A
TI Deep learning based chest X-ray film intelligent diagnosing method, involves generating heat map according to CNN feature map, and calculating integration result according to model training prediction result of classification model.
AU ZHOU Z
   MA L
AE ZHONGSHAN YANGSHI TECHNOLOGY CO LTD (ZHON-Non-standard)
GA 201897017F
AB    NOVELTY - The method involves obtaining image data of a chest X-ray film stored in a preset memory. The image data of the chest X-ray film is amplified in a preset manner. The augmented image data is pre-processed for satisfying data characteristics standards. The pre-processed image data is classified into classification models i.e. DenseNet121 model, DenseNet169 model, DenseNet201 model, ResNet50 model, InceptinV3 model, InceptinResNetV2 model and NASNetMobile model. A design model is trained for each classification model to generate model training prediction results. A final CNN feature map is obtained. A heat map is generated according to the CNN feature map. An integration result is calculated according to the model training prediction result of the classification model.
   USE - Deep learning based chest X-ray film intelligent diagnosing method.
   ADVANTAGE - The method enables accurately predicting pulmonary diseases, providing diagnostic basis when prediction time is less than manual diagnosis time, and reducing doctor time.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a chest X-ray film intelligent diagnosing method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D08A; S05-G02G3; S05-P; T01-E03; T01-J04A; T01-J05B2; T01-J06A1; T01-J07B; T01-J30A
IP G16H-050/20; G06N-003/02
PD CN108899087-A   27 Nov 2018   G16H-050/20   201921   Pages: 8   Chinese
AD CN108899087-A    CN10648064    22 Jun 2018
PI CN10648064    22 Jun 2018
UT DIIDW:201897017F
ER

PT P
PN CN109086826-A
TI Identifying wheat drought comprises dividing greenhouse into appropriate-, light-, moderate-, heavy- and extreme-drought areas, using camera to obtain images of wheat, using drought stress identification- and classification-accuracy rate.
AU AN J
   LI M
AE INST AGRIC RESOURCES & REGIONAL PLANNING (CAGS-C)
GA 2019014704
AB    NOVELTY - Identifying wheat drought comprises using a pot experiment method in a greenhouse to carry out wheat drought control experiments, dividing the drought grade into appropriate, light drought, moderate drought, heavy drought and extreme drought, using the weighing method to control drought, using a camera to obtain images of wheat with varying degrees of drought, labeling and randomly dividing the image data into a training set and a test set, where the test set does not participate in the training, pre-training on ImageNet, deconvolution operations of deep convolutional neural networks Inception V3, Resnet 50, and Resnet 152 extract image features, and training the final classification layer, testing the trained deep learning network with the test set, obtaining a confusion matrix of the model classification, using drought stress identification (DSI) accuracy and drought stress classification (DSC) accuracy rate to indicate drought stress identification and classification accuracy.
   USE - The method is useful for identifying wheat drought.
   ADVANTAGE - The method provides: a digital image of wheat by using deep learning methods to extract wheat phenotypic characteristics, which is based on the identification and classification of drought stress by the disaster-tolerant body; and is detected in a timely manner with high accuracy and non-destructive manner.
   DETAILED DESCRIPTION - Identifying wheat drought comprises (a) using a pot experiment method in a greenhouse to carry out wheat drought control experiments, dividing the drought grade into appropriate, light drought, moderate drought, heavy drought and extreme drought, using the weighing method to control drought, where the process of setting the drought degree refers to the agricultural industry standard of the People's Republic of China, Technical Specifications for Field Investigation and Grading of Winter Wheat Disasters, (b) using a camera to obtain images of wheat with varying degrees of drought, labeling and randomly dividing the image data into a training set and a test set, where the test set does not participate in the training, (c) pre-training on ImageNet, deconvolution operations of deep convolutional neural networks Inception V3, Resnet 50, and Resnet 152 extract image features, and training the final classification layer, where the learning rate of model training is 0.01, the batch size is 100, the number of iterations is 5000, printing the accuracy every 100 times, and determining whether the model converges by changing the cross entropy curve, and (d) testing the trained deep learning network with the test set, obtaining a confusion matrix of the model classification, using drought stress identification (DSI) accuracy and drought stress classification (DSC) accuracy rate to indicate drought stress identification and classification accuracy and calculating using the formula: (DSI is equal to (TC+TD)/Totalx 100%) and (DSC is equal to Ttrue/Totalx 100%), where Total is the total number of test samples, TC classifies the correct number of suitable control samples, TD is the number of drought stress samples with the correct classification and Ttrue classifies all samples correctly.
TF TECHNOLOGY FOCUS - INSTRUMENTATION AND TESTING - Preferred Components: The allocation ratio of the training set and the test set is 8:2. The camera is a single-lens reflex camera EOS700D. The photo is captured and automatically stored in the SD card in JPG format. A color 282 signal receiver is installed on the camera. Wheat images are automatically taken every 5 minutes between 6:00 am and 18:00 pm. All the experiments are conducted on the Google open source framework TensorFlow.
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-H01B3A; T01-J05B2; T01-J10B2; T01-J10B3B; T01-N01B3A; T01-N03A2; T04-D04; T04-D08; T04-K03D
IP G06K-009/62; G06K-009/00
PD CN109086826-A   25 Dec 2018   G06K-009/62   201919   Pages: 8   Chinese
AD CN109086826-A    CN10883530    06 Aug 2018
PI CN10883530    06 Aug 2018
UT DIIDW:2019014704
ER

PT P
PN CN108875913-A
TI Convolutional neural network based tricholoma matsutake rapid non-destructive testing system, has consumer terminal for obtaining tricholoma matsutake detection data by accessing control end, and sample collecting part for selecting samples.
AU PAN T
   LI Y
   LI H
   CHEN S
   ZOU X
AE UNIV JIANGSU (UYJS-C)
GA 2018958623
AB    NOVELTY - The system has a learning depth convolutional neural network model provided with a sample-collecting part, a data collecting part, a depth learning convolutional neural network modeling and optimization part. The sample collecting part selects samples of a detection object to create a sample set. The sample set is converted into a training set, validation set and testing set. The data collecting part is provided with a sample measuring chemical content and spectrum data collection part. A consumer terminal obtains tricholoma matsutake detection data by accessing a control end.
   USE - Convolutional neural network based tricholoma matsutake rapid non-destructive testing system.
   ADVANTAGE - The method enables effectively reducing detecting cost and monitoring market supervision department in an effective manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a convolutional neural network based tricholoma matsutake rapid non-destructive testing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a convolutional neural network based tricholoma matsutake rapid non-destructive testing system.
DC T01 (Digital Computers)
MC T01-N01B3A; T01-N01D2; T01-N02B2
IP G06N-003/04; G06N-003/08; G01N-021/359
PD CN108875913-A   23 Nov 2018   G06N-003/04   201919   Pages: 12   Chinese
AD CN108875913-A    CN10539012    30 May 2018
PI CN10539012    30 May 2018
UT DIIDW:2018958623
ER

PT P
PN CN108810838-A
TI Indoor background sound perception room level location method, involves comparing room information with real environment to obtain room sound perception recognition rate, and performing indoor background sound perception in room.
AU WANG M
   ANG C
   QIU H
   SONG X
   LUO L
AE UNIV GUILIN ELECTRONIC TECHNOLOGY (UYGE-C)
GA 201892387P
AB    NOVELTY - The method involves collecting amount of background sound data in a room by using a mobile phone. A locating model is trained for background acoustic positioning scene by using RNN-LSTM deep learning algorithm after constructing a background sound fingerprint database. Background sound fingerprint data is obtained as test set data. The test set data is input into a background sound localization model. Room information is compared with real environment to obtain room indoor background sound perception recognition rate. Indoor background sound perception is performed in a room.
   USE - Intelligent mobile phone based indoor background sound perception room level location method.
   ADVANTAGE - The method enables satisfying room level location requirements, improving identification rate of the room.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a background sound localization model. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W01 (Telephone and Data Transmission Systems); W04 (Audio/Video Recording and Systems); W06 (Aviation, Marine and Radar Systems)
MC T01-J05B4P; T01-J16C2; T01-J30A; W01-A06C4; W01-C01D3C; W04-V01; W04-V05; W06-A03
IP H04W-004/33; G01S-005/02; G10L-025/21; G10L-025/27; G10L-025/45
PD CN108810838-A   13 Nov 2018   H04W-004/33   201919   Pages: 10   Chinese
AD CN108810838-A    CN10560130    03 Jun 2018
PI CN10560130    03 Jun 2018
UT DIIDW:201892387P
ER

PT P
PN KR1864986-B1
TI Predicting disease susceptibility and causative agent, by receiving vector comprising genome information, determining major single nucleotide polymorphism (SNP) positions and genotypes of gene, and predicting disease based on major SNP.
AU JUN M K
   SI H S
   SUNG H K
AE KOREA ADVANCED SCI & TECHNOLOGY INST (KOAD-C)
GA 201847160E
AB    NOVELTY - Method (M1) for predicting disease susceptibility involves receiving vector comprising genome information in receiving unit, determining single nucleotide polymorphism (SNP) position and genotypes in genome information using mutual information between received vector and vector comprising predetermined disease information, and determining major SNP positions and genotypes of gene in determination unit, and predicting the disease based on major SNP position and genotypes in the predicting unit, where the mutual information is information representing a relationship between received vector and vector containing predetermined disease information, and the vector is received by transforming the genomic information comprising SNP position and genotypes into one-hot vector.
   USE - The method (M1) and device are useful for predicting disease susceptibility (claimed) and causative agent.
   ADVANTAGE - The method (M1) predicts disease susceptibility and causative agent in early stage in a simple and accurate manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) device comprising receiving unit, determination unit and predicting unit; and
   (2) method (M2) for executing classifier involves selecting a combination of pre-selected SNP position or genotype stored in a storage medium as input data.
TF TECHNOLOGY FOCUS - BIOTECHNOLOGY - Preferred Method: The method (M1) involves determining SNP position and genotypes by determining weight value for each disease based on the number of samples for each disease, and determining predetermined number of major SNP positions and genotypes using mutual information reflecting determined weight value. The method (M1) involves predicting the disease (i) through deep neural network learning using major SNP positions and genotype as input, or (ii) through multiple assortment network usinglogical sum and logical product of major SNP positions and genotype as input. The method (M1) involves predicting the disease using classifier having the major SNP position and genotype as input.
DC B04 (Natural products and polymers. Including testing of body fluids (other than blood typing or cell counting), pharmaceuticals or veterinary compounds of unknown structure, testing of microorganisms for pathogenicity, testing of chemicals for mutagenicity or human toxicity and fermentative production of DNA or RNA. General compositions.); D16 (Fermentation industry - including fermentation equipment, brewing, yeast production, production of pharmaceuticals and other chemicals by fermentation, microbiology, production of vaccines and antibodies, cell and tissue culture and genetic engineering.); T01 (Digital Computers)
MC B04-E01; B04-E09; B11-C08F1; B12-K04F; D05-H09; T01-J04; T01-S03
IP G06F-019/00; G06F-019/20
PD KR1864986-B1   05 Jun 2018   G06F-019/00   201842   Pages: 16   
AD KR1864986-B1    KR025459    27 Feb 2017
PI KR025459    27 Feb 2017
CP KR1864986-B1
      KR2014090544-A   UNIV YONSEI IND ACADEMIC COOP FOUND (UYIA)   KIM S, KIM M
      KR2014098561-A   KOREA ADVANCED INST SCI & TECHNOLOGY (KOAD)   KANG C, YI G S
      KR1565005-B1   KOREA ADVANCED INST SCI & TECHNOLOGY (KOAD)   JEONG C S, KIM D
      US20160364522-A1      
UT DIIDW:201847160E
ER

PT P
PN CN106778684-A
TI Deep neural network model establishing method, involves obtaining human face information and classification information, collecting human face information, and establishing independent depth neural network models according to loss function.
AU HUANG F
   TIAN Z
   HOU L
   DENG H
   XIE J
AE YSTEN TECHNOLOGY CO LTD (YSTE-Non-standard)
GA 201738608L
AB    NOVELTY - The method involves obtaining human face information and classification information. Human face information is classified according to the classification information. The human face information is collected by normalization process. Information in a deep neural network model is obtained. The deep neural network model is established according to loss function and training parameter to obtain an optimized depth neural network model. The depth neural network model is independent depth neural network models. Each independent depth neural network models are established according to loss function.
   USE - Deep neural network model establishing method.
   ADVANTAGE - The method enables improving face posture identifying clarity to improve identification success rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a multiple-depth fusion neural network based human face recognizing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep neural network model establishing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J10B2A; T01-N01B3; T04-D04; T04-D07F1
IP G06K-009/00; G06K-009/62
PD CN106778684-A   31 May 2017   G06K-009/00   201749   Pages: 16   Chinese
AD CN106778684-A    CN10022621    12 Jan 2017
PI CN10022621    12 Jan 2017
UT DIIDW:201738608L
ER

PT P
PN CN109005431-A
TI Video evaluation recommendation system for advertiser, has video recommendation module that calculates and obtains self-made content meeting screening condition, and sends screening recommendation result to user end.
AU LIU G
AE BEIJING TENSYN DIGITAL MARKETING TECHNOL (BEIJ-Non-standard)
GA 2018A3744A
AB    NOVELTY - The system has mode selection module that is configured to obtain an evaluation mode instruction or a recommendation mode instruction sent by the user end. A video evaluation module is configured to invoke the self-made content evaluation recommendation database data and the standard data of the self-made content history database, to perform evaluation of the evaluation object, and to send the evaluation result to the user end, according to the evaluation mode instruction. The video recommendation module is configured to invoke the self-made content evaluation recommendation database data and the standard data of the self-made content history database, to calculate and obtain the self-made content that meets the screening condition, and to send the screening recommendation result to the user end, according to the recommendation mode instruction.
   USE - Video evaluation recommendation system for evaluating brand advertisement video content for advertiser.
   ADVANTAGE - The system realizes the integration and application of multidimensional big data. The video content evaluation system integrates video media big data, enterprise innovation big data and industry third-party big data organically into a comprehensive set of big data systems. The video content evaluation system is the industry's first big data system to conduct multi-dimensional comprehensive quantitative prediction and evaluation of upcoming home-made content. The same homemade content produces different results to meet the customer's comprehensive consideration of homemade content in different situations, in different dimensions or different weights. The video evaluation system has designed hundreds of standard speech techniques, and makes smart matching through the results of comprehensive evaluation, and selects the most reasonable description method to present the evaluation report. The system completes self-renewal and verification of data evaluation standards through artificial intelligence (AI) machine learning system, uses big data as the basis of learning, obtains data through the Internet, and continuously upgrades and evolves through neural network algorithm, deep learning, and reinforcement learning.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of the video evaluation recommendation system. (Drawing includes non-English language text)
DC T01 (Digital Computers); W02 (Broadcasting, Radio and Line Transmission Systems); W03 (TV and Broadcast Radio Receivers)
MC T01-J05B4F; T01-N01A2C; T01-N01D1B; W02-F10Q3; W03-A16C5A; W03-A16C5K
IP H04N-021/25; H04N-021/258; H04N-021/262; H04N-021/466; H04N-021/482
PD CN109005431-A   14 Dec 2018   H04N-021/25   201918   Pages: 11   Chinese
AD CN109005431-A    CN11090258    18 Sep 2018
PI CN11090258    18 Sep 2018
UT DIIDW:2018A3744A
ER

PT P
PN CN109005451-A
TI Deep learning based video item dividing method, involves initializing video data, and refining fragment time of candidate item with sound characteristics by using sound identification technology to obtain final demolition time point.
AU NI P
   JIANG Z
   PENG M
   LIU R
   LIU Y
AE HANGZHOU XINGXI TECHNOLOGY CO LTD (HANG-Non-standard)
GA 2018A33814
AB    NOVELTY - The method involves initializing video data. A face detection process is performed by using face recognition technology. Continuous occurrence time fragment of similar face is considered as a candidate item fragment. Sound characteristics in the candidate item segment are extracted. Fragment time of a candidate item is refined with the sound characteristics by using sound identification technology to obtain the final demolition time point. Audio waveform data and image data are acquired based on the video data. A face coding process is performed by using deep learning algorithm. An image frame in the video data is compared. Certain range with the extracted sound characteristics is found.
   USE - Deep learning based video item dividing method.
   ADVANTAGE - The method enables improving item obtaining accuracy, performing face and voice recognition process for multiple video segments with rapid speed and reducing investment of manpower.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning based video item dividing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W03 (TV and Broadcast Radio Receivers)
MC T01-C08A; T01-E01C; T01-J10B2A; T01-J10B3A; T01-J16C2; T01-J30A; W03-A02C5L; W03-A11D; W03-A16C5A; W03-A16C5C
IP H04N-021/439; H04N-021/44; H04N-021/845; G06K-009/62; G06K-009/00
PD CN109005451-A   14 Dec 2018   H04N-021/439   201918   Pages: 6   Chinese
AD CN109005451-A    CN10701351    29 Jun 2018
PI CN10701351    29 Jun 2018
UT DIIDW:2018A33814
ER

PT P
PN CN109101537-A
TI Method for classifying multi-wheel dialog data using electronic device based on deep learning, involves obtaining current and preset sentence word vectors, and obtaining probability distribution labels of data regression model.
AU YANG P
AE BEIJING HUIWEN TECHNOLOGY DEV CO LTD (BEIJ-Non-standard)
GA 201902998R
AB    NOVELTY - The method involves obtaining current sentence word vector and a preset sentence word vector for obtaining current wheel sub-sentence. The current sentence word vector and the preset sentence word vectors are processed by a long term memory layer for obtaining a current wheel sentence vector and preset wheel sentence vectors. The preset wheel sentence vector is sent to an attention mechanism layer corresponding to a preset background vector. Judgment is made to check whether the current wheel sentence vector is compared to the preset background vector. Probability distribution labels of a multi-wheel dialog data regression model are obtained.
   USE - Method for classifying multi-wheel dialog data using an electronic device (claimed) based on deep learning.
   ADVANTAGE - The method enables generating background vectors with current wheel data so as to improve multi-wheel dialog scenario classification effect.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for classifying multi-wheel dialog data using an electronic device based on deep learning
   (2) a computer-readable storage medium for storing set of instructions for classifying multi-wheel dialog data using an electronic device based on deep learning.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for classifying multi-wheel dialog data using an electronic device based on deep learning. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J05B2; T01-J05B4P; T01-J10B2; T01-J30A; T01-S03; W04-W05A
IP G06F-017/30; G06K-009/62
PD CN109101537-A   28 Dec 2018   G06F-017/30   201917   Pages: 21   Chinese
AD CN109101537-A    CN10680888    27 Jun 2018
PI CN10680888    27 Jun 2018
UT DIIDW:201902998R
ER

PT P
PN CN108984811-A
TI Virtual desiging and evaluating pharmaceutical preparation prescriptions by establishing pharmaceuticals prescription in vivo prediction and evaluation module to predict and evaluate pharmacokinetic behavior of pharmaceuticals formulation.
AU OUYANG D
   ZHAO Q
AE OUYANG D (OUYA-Individual)
GA 2018A19775
AB    NOVELTY - Method and system for virtual design and evaluation of pharmaceutical preparation prescriptions are claimed. The method for virtual designing and evaluating pharmaceutical preparation prescriptions involves establishing pre-pharmaceutical pre-research module to predict basic parameters of pre-pharmaceutical study, establishing pharmaceuticals prescription design and optimization module to predict pharmaceutical composition based on predicted parameters and specific pharmaceuticals dose forms, establishing design and optimization module for preparation process of pharmaceuticals, and predicting the preparation route and parameters according to predicted parameters, pharmaceutical dose form and pharmaceutical composition and pharmaceuticals prescription in vivo prediction and evaluation module to predict and evaluate the pharmacokinetic behavior of the pharmaceuticals formulation in accordance with the parameters, pharmaceuticals composition and preparation process.
   USE - The method and system are useful for virtual design and evaluation of pharmaceutical preparation prescriptions.
   ADVANTAGE - The method accelerate the development of pharmaceutical products without any laboratory equipment, ensures complete virtual design and evaluation of pharmaceutical preparation prescriptions through computer platform, shortens pharmaceutical development cycle and reduces medicine development costs.
TF TECHNOLOGY FOCUS - PHARMACEUTICALS - Preferred Method: In the method, the medicine prescription research prediction module is obtained by establishing the medicines with pharmaceutical auxiliary materials database, using molecular simulation or machine learning method and predicting the pharmaceuticals physicochemical parameter, stability, basic pharmacokinetic parameters and pharmaceuticals compatibility with accessories. The medicine prescription design and optimization module is realized by establishing a pharmaceutical prescription database to predict pharmaceuticals composition comprises auxiliary material selection and percentage according to the parameter obtained by the prediction and the specific pharmaceutical dose form, using molecular simulation, prescription rules or machine learning method. The design and optimization module of the medicine preparing process by establishing a pharmaceuticals production technology database according to the parameter obtained by the prediction, the pharmaceuticals dose form and pharmaceutical composition, and then predicting the preparation technique route and parameter using simulation and machine learning method. The medicine prescription in vivo pharmacokinetics prediction and evaluation module is realized by establishing a pharmaceutical and pharmaceutical prescription in vivo dynamics database, according to the parameter obtained by the prediction, pharmaceutical compositions and preparation process, using pharmacokinetic model, physiologic pharmacokinetic model or machine learning method to predict and evaluate pharmaceuticals formulary in vivo pharmacokinetic behavior. The molecular simulation techniques includes quantum mechanics, molecular docking, molecular dynamics, Monte Carlo or quantitive structure activity relationship. The machine learning method includes an artificial neural network, a decision tree, perceptron, support vector machine, integrated learning, principal component analysis, dimensionality reduction and metric learning, clustering, Bayesian classifier, Gaussian process regression, linear discriminant analysis, nearest neighbor method, radial basis function kernel, maximum expectation algorithm, Bayesian algorithm, Markov chain, Monte Carlo method, expert systems, in-depth learning, intensive learning or migration learning method. The process simulation technique includes a reaction process and reaction kinetic simulation, unit process simulation, preparation process simulation, process parameter optimization of preparation process, dynamic simulation and optimization, steady-state simulation and optimization, or dynamic simulation. The medicine preparation comprises tablet, capsule, pill, film, granule, solution agent, aromatic water, syrup, injection, eye drops, nasal drops, mixture, lotion, liniment, suppository, aerosol, spraying agent, jellies, transdermal administration preparation, ointment, paste, slow-release preparation, microsphere preparation, a liposome, a nanoparticle formulation, target formulation, the traditional Chinese medicine preparation or biological medical preparation. The medicine preparing process includes crushing, sieving, mixing, pelletizing, crystallizing, drying, tableting, coating, packaging and freezing. filtering, emulsifying, dissolving, stirring, shaking, filling, dispersing, grinding, sterilizing, distilling, crushing, drying, kneading, sieving, granulating, sol, cutting, filling and dripping, homogenizing, cooling, grinding, separating, purifying, concentrating, extracting, finely filtering, extracting, soaking, squeezing, wetting, dipping or diacolation process.
DC B04 (Natural products and polymers. Including testing of body fluids (other than blood typing or cell counting), pharmaceuticals or veterinary compounds of unknown structure, testing of microorganisms for pathogenicity, testing of chemicals for mutagenicity or human toxicity and fermentative production of DNA or RNA. General compositions.); T01 (Digital Computers)
MC B04-A08; B04-A09; B04-A10; B04-A98; B11-C08H; B11-C11; B12-K04E3; T01-E04; T01-J05B3; T01-J06A1; T01-J15H; T01-J16A
IP G06F-017/50; G06N-099/00
PD CN108984811-A   11 Dec 2018   G06F-017/50   201917   Pages: 16   Chinese
AD CN108984811-A    CN10429764    05 Jun 2017
PI CN10429764    05 Jun 2017
UT DIIDW:2018A19775
ER

PT P
PN CN108757426-A
TI Oil field water injection plunger pump fault diagnostic method, involves configuring test sample based on two-dimensional matrix, and establishing known diagnostic model to determine fault type of mine water injection plunger pump.
AU XIANG J
   WANG S
   JIANG Y
   ZHONG Y
AE UNIV WENZHOU (UYWE-Non-standard)
GA 201889613G
AB    NOVELTY - The method involves obtaining one-dimensional original vibration signal of a predefined petroleum mine water injection plunger pump (S1). Minimum entropy de-convolution filtering processing of the one-dimensional original vibration signal is realized (S2). A two-dimensional matrix is established (S3). Training sample of a model is obtained. A convolution neural network is established. A known failure mode of a convolutional neural network diagnosis model is obtained (S4). A test sample is configured based on the two-dimensional matrix. A known diagnostic model is established to determine fault type of the predefined petroleum mine water injection plunger pump.
   USE - Oil field water injection plunger pump fault diagnostic method.
   ADVANTAGE - The method enables automatically identifying multi-fault in petroleum mine water plunger injection pump.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an oil field water injection plunger pump fault diagnostic method. '(Drawing includes non-English language text)'
   Step for obtaining one-dimensional original vibration signal of predefined petroleum mine water injection plunger pump (S1)
   Step for realizing minimum entropy de-convolution filtering processing of one-dimensional original vibration signal (S2)
   Step for establishing two-dimensional matrix (S3)
   Step for obtaining known failure mode of convolutional neural network diagnosis model (S4)
DC H01 (Obtaining crude oil and natural gas - including exploration, drilling, well completion, production and treatment. General off-shore platform and drilling technology is included together with the treatment of tar sands and oil shales (C10G, E21B).); T01 (Digital Computers)
MC H01-D03; H01-D12; T01-J04B2; T01-N01B3A
IP F04B-051/00; G06N-003/04
PD CN108757426-A   06 Nov 2018   F04B-051/00   201917   Pages: 19   Chinese
AD CN108757426-A    CN10724488    04 Jul 2018
PI CN10724488    04 Jul 2018
UT DIIDW:201889613G
ER

PT P
PN CN109100669-A
TI Overlapping echo-based single-scan synchronization and T2 magnetic resonance diffusion imaging method, involves training of data network until network converges and stabilizes, and reconstructing data to obtain ADC and T2 images.
AU CAI C
   MA L
   CHEN Z
   CAI S
   DING X
AE UNIV XIAMEN (UYXI-C)
GA 201903018W
AB    NOVELTY - The method involves opening an operation software of a MRI device. An imaging object is positioned in a region of interest. A tuning, shimming, and power and frequency correction is performed on the imaging object. A pre-compiled DT2M-OLED imaging sequence is introduced and parameters of a pulse sequence is obtained. Data sampling process is performed on the DT2M-OLED imaging sequence. Signals of the echo chains are obtained. The signals of the echo chains are normalized. An image domain signal is reconstructed by a convolutional neural network to obtain experimental data. A random template is generated. A convolution nerve network model is built by utilizing tensorflow deep learning framework and InstallShield to set a related parameter of a training. A data network is trained until the training data network converges and stabilizes to obtain a trained network model. The experimental data is reconstructed to obtain ADC and T2 images.
   USE - Overlapping echo-based single-scan synchronization and T2 magnetic resonance diffusion imaging method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a graphical illustration of an overlapping echo-based single-scan synchronization and t2 magnetic resonance diffusion imaging method.
DC P31 (Diagnosis, surgery (A61B).); S01 (Electrical Instruments); S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S01-E02A; S01-H07A; S05-D; T01-J04B2; T01-N01B3; T01-N01E; T01-N03
IP G01R-033/50; G01R-033/54; G01R-033/563; A61B-005/055
PD CN109100669-A   28 Dec 2018   G01R-033/50   201916   Pages: 13   Chinese
AD CN109100669-A    CN10765276    12 Jul 2018
PI CN10765276    12 Jul 2018
UT DIIDW:201903018W
ER

PT P
PN CN109101806-A
TI Style migration based privacy portrait data marking method, involves combining extracted image content and image style together to generate style image, performing data marking operation in human face image, and storing mark information.
AU SHANG L
   WANG H
   ZHANG Z
AE ZHEJIANG ICARE VISION TECHNOLOGY CO LTD (ZHEJ-Non-standard)
GA 201902991Q
AB    NOVELTY - The method involves generating an image texture by utilizing a depth learning texture model and a built model. Content of an image is extracted without including the style of the image by utilizing a neural network. Extracted image content and image style are combined with each other to generate a style image. Data marking operation is performed in a human face image. Mark information is stored.
   USE - Style migration based privacy portrait data marking method.
   ADVANTAGE - The method enables increasing data marking accuracy and data security of the human face image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a style migration based privacy portrait data marking method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10E; T01-J12C; T01-N01B3; T01-N01D1B; T01-N01D2; T01-N02B1B
IP G06F-021/32; G06F-021/62
PD CN109101806-A   28 Dec 2018   G06F-021/32   201916   Pages: 5   Chinese
AD CN109101806-A    CN10940018    17 Aug 2018
PI CN10940018    17 Aug 2018
UT DIIDW:201902991Q
ER

PT P
PN CN109101492-A
TI Method for performing entity extraction by using history dialog action in natural language processing, involves determining relationship between multiple entities on rules, named entity and current task.
AU XU H
AE NANJING WALKYREN NETWORK TECHNOLOGY CO (NANJ-Non-standard)
GA 201902999M
AB    NOVELTY - The method involves determining word in text to-be-processed by distributed vector according to session information input by a user (S1), where word is recorded as word vector. Name entity is obtained (S2) by utilizing a convolutional neural network based on relationship between word vector and word vector context. Historical dialog information is combined (S3) to determine named entity and current task according to an attention mechanism and historical dialog information. Relationship between multiple entities is determined (S4) based on rules, named entity and current task. Text to-be-processed is encoded by the convolutional neural network according to relationship between word vector and word vector context.
   USE - Method for performing entity extraction by using history dialog action in natural language processing.
   ADVANTAGE - The method enables effectively finding specific type of text entity, and improving accuracy of entity extraction, and extracting text to-be-processed, and effectively assisting manual finishing information or service acquisition task.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for performing entity extraction by using history dialog action in natural language processing.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for performing entity extraction by using history dialog action in natural language processing. '(Drawing includes non-English language text)'
   Step for determining word in text to-be-processed by distributed vector according to dialog information inputted by a user (S1)
   Step for obtaining name entity by utilizing a convolutional neural network (S2)
   Step for combining historical dialog information to determine named entity and current task according to attention mechanism and historical dialog information (S3)
   Step for determining relationship between multiple entities on rules, named entity and current task (S4)
DC T01 (Digital Computers)
MC T01-J05B4P; T01-J11A1; T01-J16C3; T01-N01D
IP G06F-017/27; G06F-017/30; G06N-003/04
PD CN109101492-A   28 Dec 2018   G06F-017/27   201916   Pages: 6   Chinese
AD CN109101492-A    CN10825397    25 Jul 2018
PI CN10825397    25 Jul 2018
UT DIIDW:201902999M
ER

PT P
PN CN109034136-A
TI Method for processing image by using imaging device, involves determining road area in current frame image, and establishing preset depth learning model when ratio of current frame image is smaller than preset value.
AU YANG W
   NICOLAS P
AE HUBEI ECARX TECHNOLOGY CO LTD (HUBE-Non-standard)
GA 2018A4850P
AB    NOVELTY - The method involves determining a road area in a current frame image. The road area is extracted in the current frame image. Judgment is made to check whether a ratio of the current frame image is greater than a preset value or not. A resolution value of the road area is adjusted when the ratio of the current frame image is greater than a preset value. A preset depth learning model is established when the ratio of the current frame image is smaller than the preset value. The road area in the current frame image is selected according to a preset generation mode.
   USE - Method for processing an image by using an imaging device (claimed).
   ADVANTAGE - The method enables reducing amount of calculation when processing high-resolution picture, improving calculating speed, and saving computational resources.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for processing an image by using an imaging device
   (2) a computer-readable storage medium for storing a set of instructions to execute a method for processing an image by using an imaging device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for processing an image by using an imaging device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J10B2; T01-J10B3A; T01-J21; T01-J30A; T01-S03; W04-W05A
IP G06K-009/00; G06K-009/32
PD CN109034136-A   18 Dec 2018   G06K-009/00   201916   Pages: 20   Chinese
AD CN109034136-A    CN11039245    06 Sep 2018
PI CN11039245    06 Sep 2018
UT DIIDW:2018A4850P
ER

PT P
PN CN108766555-A
TI Computer diagnostic method for malignant degree of pancreatic neuroendocrine tumors, involves providing patient data to be diagnosed as input to diagnostic model, and corresponding pancreatic neuroendocrine tumor malignancy as output.
AU HUANG B
   XIAO H
   LUO Y
   FENG S
   SONG C
AE UNIV SHENZHEN (UYSZ-C)
GA 201889412E
AB    NOVELTY - The method involves obtaining raw data including patient image data and patient pathological grading data of pancreatic neuroendocrine tumors. The raw data is divided into patients, and the training set, the test set and the verification set are obtained. The convolutional neural network is used for training and testing, and the diagnostic model of malignant degree of pancreatic neuroendocrine tumor is obtained according to the training set, test set and verification set. The patient data to be diagnosed is provided as input to the diagnostic model of pancreatic neuroendocrine tumor malignancy, and the corresponding pancreatic neuroendocrine tumor malignancy is provided as output.
   USE - Computer diagnostic method for malignant degree of pancreatic neuroendocrine tumors e.g. islet cell tumors.
   ADVANTAGE - The method realizes a computer diagnosis of the malignant degree of pancreatic neuroendocrine tumor through deep learning, and makes the result of image diagnosis more objective and accurate, and reduce the probability of misdiagnosis, while reducing the burden of imaging doctors, improve the efficiency of doctors to some extent.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computer diagnostic system for malignant degree of pancreatic neuroendocrine tumors.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating the steps of computer diagnostic method for the malignant degree of pancreatic neuroendocrine tumors. (Drawing includes non-English language text)
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D08A; S05-G02G3; S05-G02G9; S05-P; T01-J10B3A; T01-N01B3A; T01-N01E
IP G16H-050/20; G06N-003/04; G06N-003/08
PD CN108766555-A   06 Nov 2018   G16H-050/20   201916   Pages: 21   Chinese
AD CN108766555-A    CN10306839    08 Apr 2018
PI CN10306839    08 Apr 2018
UT DIIDW:201889412E
ER

PT P
PN CN106250931-A
TI Random convolutional neural network based high resolution image scene classification method, involves obtaining classification and training images, sharing combination parameter from parameter library, and obtaining classification image.
AU BU B
   ZHANG F
   ZHANG L
AE UNIV WUHAN (UYWU-C)
GA 2017007253
AB    NOVELTY - The method involves determining a data average value. A classification image and a training image are obtained. Negative gradient direction is determined based on optimization function. A basic convolutional neural network model is established with respect to a feature extraction stage of a network parameter. A minimize basic convolutional neural network model is established by using traditional negative gradient direction reverse error propagation algorithm. A combination parameter from a parameter library is shared. A classification image is obtained.
   USE - Random convolutional neural network based high resolution image scene classification method.
   ADVANTAGE - The method enables improving network generalization ability in model training process so as to improve training efficiency of a network model, thus reducing processing cost.
   DESCRIPTION OF DRAWING(S) - The drawing shows a perspective block diagram of a random convolutional neural network based high resolution image scene classification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B2; T01-J10B2; T01-J10D; T01-J16C1; T01-N01B3
IP G06K-009/62
PD CN106250931-A   21 Dec 2016   G06K-009/62   201704   Pages: 11   Chinese
AD CN106250931-A    CN10628128    03 Aug 2016
PI CN10628128    03 Aug 2016
CP CN106250931-A
      CN104102919-A   UNIV TONGJI (UYTJ)   WANG H, YU D
      CN105243398-A   UNIV XIAN JIAOTONG (UYXJ)   WANG J, SHI W, ZHANG S, GONG Y
CR CN106250931-A
      FAN ZHANG: "Scene Classification via a Gradient Boosting Random Convolutional Network Framework", IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,relevantClaims[1-2],relevantPassages[1794-17972-3]
UT DIIDW:2017007253
ER

PT P
PN CN109101966-A
TI Deep learning based workpiece location and pose identifying and estimating system, has deep learning network for identifying orientation and posture, and estimation model for identifying and locating object picture and pose estimation.
AU BU W
   ZHANG B
   XU X
   PENG C
   XIAO J
AE NINGBO INST MATERIALS TECHNOLOGY & ENG C (CANM-C)
GA 2019029882
AB    NOVELTY - The system has a deep learning network for identifying orientation and posture for estimating a network design. An output item is provided on a full connection layer for obtaining angle information. A data collection module constructs a training set during work piece constructing process for collecting different postures of a picture as a training sample. A model training module trains and obtains a workpiece identification location and pose estimation model when a loss value reaches a preset threshold. The workpiece identification location and pose estimation model identifies and locates workpiece object picture and pose estimation.
   USE - Deep learning based workpiece location and pose identifying and estimating system.
   ADVANTAGE - The system realizes classification identification and position determination of different types of the workpiece to estimate a space posture of an individual work piece and improve production efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning based workpiece location and pose identifying and estimating method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based workpiece location and pose identifying and estimating method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J05B2; T01-J10B2; T01-J15A4; T01-N01B3; T04-D02; T04-D04; T04-D07D5; W04-W05A
IP G06K-009/32; G06K-009/62
PD CN109101966-A   28 Dec 2018   G06K-009/32   201915   Pages: 12   Chinese
AD CN109101966-A    CN10591858    08 Jun 2018
PI CN10591858    08 Jun 2018
UT DIIDW:2019029882
ER

PT P
PN CN109102029-A
TI Maximization information generated network model synthesis human face sample quality evaluating method, involves performing training set accuracy testing process by classification model when difference between human face sample set is large.
AU CHEN G
   LUO J
   DU X
   REN H
   LIU Y
   HE H
   LIU C
   CHEN D
   LI J
AE UNIV CHONGQING SCI & TECHNOLOGY (UYNQ-C)
GA 201902986C
AB    NOVELTY - The method involves performing a data pre-processing process. A synthetic evaluation model of human face sample sets is constructed by a convolutional neural network, where the synthetic evaluation model comprises an input layer input layer reading face sample. Image data is converted into a two- dimensional matrix. Characteristics of the human face sample is extracted and synthesized by a convolution layer. High-level semantic feature are extracted by a deep convolutional layer. A training set accuracy testing process is performed by a classification model when difference between the human face sample sets is large.
   USE - Maximization information generated network model synthesis human face sample quality evaluating method.
   ADVANTAGE - The method enables realizing high reliability in a human face sample quality evaluating process.
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J05B2; T01-J10B2; T01-J16C3; T01-N01B3A; T04-D04; T04-D07F1
IP G06K-009/62; G06N-003/04
PD CN109102029-A   28 Dec 2018   G06K-009/62   201915   Pages: 9   Chinese
AD CN109102029-A    CN10964677    23 Aug 2018
PI CN10964677    23 Aug 2018
UT DIIDW:201902986C
ER

PT P
PN CN109101905-A
TI Deep convolutional neural network based SAR image classifying and identifying method, involves processing SAR image by filtering original input data, and classifying feature information of convolutional neural network by using depth study.
AU CHEN J
   LV J
   WANG X
   HU J
   LIU H
   PING X
   WANG F
AE UNIV HOHAI (UYHO-C)
GA 201902989F
AB    NOVELTY - The method involves processing a SAR image by filtering original input data. The SAR image is cut. Feature information of a convolutional neural network is classified by using depth study. The convolutional neural network is provided with an input layer, a convolution layer and an output layer. The original input data is defined as an image training sample. The convolution layer is provided with multi-filter layers, where different size of the multi-filter layers are determined. A maximum dimension pool layer is set.
   USE - Deep convolutional neural network based SAR image classifying and identifying method.
   ADVANTAGE - The method enables improving SAR image identification precision.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep convolutional neural network based SAR image classifying and identifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J10B1; T01-J10B2; T01-N01B3
IP G06K-009/00; G06K-009/40; G06K-009/42; G06K-009/62; G06N-003/04; G06N-003/08
PD CN109101905-A   28 Dec 2018   G06K-009/00   201915   Pages: 8   Chinese
AD CN109101905-A    CN10840186    27 Jul 2018
PI CN10840186    27 Jul 2018
UT DIIDW:201902989F
ER

PT P
PN CN109102025-A
TI Deep learning-joint optimization based pedestrian recognition method, involves optimizing constructing neural network structure model parameter for pedestrian recognition based on deep learning Siamese neural network structure model.
AU CHENG J
   WANG Y
   SU Y
   LIN L
   WANG W
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 201902986G
AB    NOVELTY - The method involves screening number balancing positive and negative pedestrian sample collection for constructing a data set. A deep learning Siamese neural network structure model is established, where the structure model includes two convolutional neural networks and multi-level feature fusion modules. Hyper-different characteristics of a pedestrian are extracted. Hyper-H characteristics are sent to two classification networks. A constructing neural network structure model parameter is optimized for pedestrian recognition based on the deep learning Siamese neural network structure model.
   USE - Deep learning-joint optimization based pedestrian recognition method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a deep learning Siamese neural network structure model. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J10B2A; T01-N01B3; T04-D04
IP G06K-009/62; G06N-003/04
PD CN109102025-A   28 Dec 2018   G06K-009/62   201915   Pages: 12   Chinese
AD CN109102025-A    CN10932825    15 Aug 2018
PI CN10932825    15 Aug 2018
UT DIIDW:201902986G
ER

PT P
PN CN109101934-A
TI Car type identifying method, involves converting to-be-identified image of car into HSV image, and obtaining car information corresponding to hue value of car in HSV image from car type identification information.
AU DENG L
AE GUANGDONG MATVIEW TECHNOLOGY CO LTD (GUAN-Non-standard)
GA 201902988S
AB    NOVELTY - The method involves training a pre-collected image sample by using convolutional neural network algorithm to build a car identification model. The car is shot by a camera in a zebra crossing area to obtain to-be-identified image of the car. Car type identification operation is performed on the to-be-identified image by adopting the car type identification model to obtain car type identification information. The to-be-identified image of the car is converted into a HSV image. Car information corresponding to hue value of the car in the HSV image is obtained from the car type identification information.
   USE - Car type identifying method.
   ADVANTAGE - The method enables effectively reducing judging range of the car to improve car identification accuracy.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a car type identifying device
   (2) a computer-readable storage medium for storing set of instructions for identifying type of a car.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a car type identifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B4P; T01-J10B2; T01-J10D; T01-N01B3; T01-S03
IP G06K-009/00; G06F-017/30; G06N-003/04; G06N-003/08; G06T-007/40; G06T-007/90; G08G-001/017
PD CN109101934-A   28 Dec 2018   G06K-009/00   201915   Pages: 18   Chinese
AD CN109101934-A    CN10948918    20 Aug 2018
PI CN10948918    20 Aug 2018
UT DIIDW:201902988S
ER

PT P
PN CN109102543-A
TI Method for locating object in logistics system based on image segmentation, involves training complete convolutional neural network to establish target neural network, and marking and locating object image according to target neural network.
AU DENG Y
AE SHENZHEN DORABOT CO LTD (SHEN-Non-standard)
GA 201902974J
AB    NOVELTY - The method involves collecting training images. The training images are marked to establish a training database. A complete convolutional neural network is established. The training database is input into the complete convolutional neural network. The complete convolutional neural network is trained to establish a target neural network. An object image in a logistics system is marked and located according to the target neural network. Images of multiple overlapping training objects at different angle are captured by a camera, where the complete convolutional neural network includes a convolution layer, a pooling layer and a deconvolution layer.
   USE - Method for locating an object in a logistics system based on image segmentation.
   ADVANTAGE - The method enables training the complete convolutional neural network model by using the training samples collected in an actual application scene to mark and locate the object image in the logistics system according to the target neural network, thus increasing robustness of object locating process and image segmentation precision. The method enables realizing segmentation process of multiple overlapping envelope areas while handling envelopes in the logistics system, thus improving logistics sorting accuracy.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an image segmentation based object locating device
   (2) a storage medium for storing a set of instructions for locating an object based on image segmentation.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an image segmentation based logistics object locating method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); T05 (Counting, Checking, Vending, ATM and POS Systems)
MC T01-J04B2; T01-J05B4P; T01-J10B2; T01-J10B3; T01-N01A2E; T01-N01B3; T01-N02B2B; T04-D04; T05-K02
IP G06T-007/70; G06T-007/11; G06K-009/62
PD CN109102543-A   28 Dec 2018   G06T-007/70   201915   Pages: 12   Chinese
AD CN109102543-A    CN10943480    17 Aug 2018
PI CN10943480    17 Aug 2018
UT DIIDW:201902974J
ER

PT P
PN CN109104197-A
TI Convolutional neural network non-reducing sparse data decoding circuit, has multiplier for receiving current period of 16-bit calculated data and 16 bit weight, and adder for adding convolution result, where result is sent to adder.
AU DU G
   WU J
   ZHANG H
   ZHANG D
   SONG Y
   ZHANG Y
   YANG Z
AE UNIV HEFEI TECHNOLOGY (UYHE-C)
GA 2019029409
AB    NOVELTY - The circuit has a convolutional neural network arranged with a convolutional layer. A storage block stores output characteristic diagram. A channel receives output feature map in each batch. An encoding circuit is provided with a k-th compression unit, a multi-path selector, a selection controller, a tail address cache queue and a dynamic RAM. A length counter and a position counter receive an enable signal. Each multiplier receives current period of 16-bit calculated data and 16 bit weight. An adder adds convolution result, where the convolution result is sent to the adder corresponding to a router.
   USE - Convolutional neural network non-reducing sparse data decoding circuit.
   ADVANTAGE - The circuit can reduce accessing power consumption data, reading power consumption time and transmission time and improve utilization ratio of PE.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a convolutional neural network non-reducing sparse data decoding method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a convolutional neural network non-reducing sparse data decoding circuit. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-D01; T01-D02; T01-E02A; T01-H01B3; T01-J04B2
IP H03M-007/30; G06N-003/04
PD CN109104197-A   28 Dec 2018   H03M-007/30   201915   Pages: 28   Chinese
AD CN109104197-A    CN11340018    12 Nov 2018
PI CN11340018    12 Nov 2018
UT DIIDW:2019029409
ER

PT P
PN CN109102064-A
TI Neural network quantization compression method, involves performing unsigned index conversion process in floating-point value based on coefficient deviation, and performing network operation based on floating point value.
AU GE Y
   WANG J
   ZHU X
AE HANGZHOU XIONGMAI INTEGRATED CIRCUIT TEC (HANG-Non-standard)
GA 201902985H
AB    NOVELTY - The method involves performing network quantization compression process for calculating quantization step size of a data according to maximum value, minimum value and quantization bit width. An unsigned index is determined. Coefficient deviation between floating point values is determined. Unsigned index conversion process is performed in the floating-point value based on coefficient deviation between an unsigned index and the floating-point value Network operation is performed based on the floating point value. A testing data set is obtained according to a neural network. A quantization index is calculated.
   USE - Neural network quantization compression method.
   ADVANTAGE - The method enables counting data range to ensure accuracy, performing data range compressing process by utilizing symbol-free quantization index so as to improve operation precision and avoiding quantization space waste, increasing calculation precision, reducing influence and establishing a deep learning network model for operating on an embedded device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a neural network. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W01 (Telephone and Data Transmission Systems)
MC T01-D01; T01-E02B; W01-A02A
IP G06N-003/04; H03M-007/30
PD CN109102064-A   28 Dec 2018   G06N-003/04   201915   Pages: 7   Chinese
AD CN109102064-A    CN10668567    26 Jun 2018
PI CN10668567    26 Jun 2018
UT DIIDW:201902985H
ER

PT P
PN CN109102400-A
TI Block chain system, has block chain main chain provided with first block chain node, and second block chain node for processing data stored in block chain main chain and block chain side chain and provides data processing result.
AU HAN Y
AE HENGQIN MIDA TECHNOLOGY CO LTD (HENG-Non-standard)
GA 201902977U
AB    NOVELTY - The system has a block chain main chain provided with first block chain node. A block chain side chain is provided with a second block chain node. The second block chain node processes data stored in the block chain main chain and the block chain side chain and provides data processing result. The second block chain node extracts data features by performing deep learning process. A system main body is stored with financial transaction data. The block chain side chain transfers digital assets with the block chain main chain. The block chain side chain transfers digital assets to a block chain backbone.
   USE - Block chain system.
   ADVANTAGE - The system processes the financial transaction data, and provides financial transaction data processing result to satisfy demands of different industries.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a block chain system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05A1; T01-J30A
IP G06Q-040/04
PD CN109102400-A   28 Dec 2018   G06Q-040/04   201915   Pages: 7   Chinese
AD CN109102400-A    CN10783731    17 Jul 2018
PI CN10783731    17 Jul 2018
UT DIIDW:201902977U
ER

PT P
PN CN109102469-A
TI Convolutional neural network based remote sensing image panchromatic sharpening method, involves selecting part region, and inputting optimal convolutional neural network model to obtain high resolution multi-spectral remote sensing image.
AU HE L
   ZHU J
   RAO Y
AE UNIV SOUTH CHINA TECHNOLOGY (UYSC-C)
GA 201902976D
AB    NOVELTY - The method involves reading an original multi-spectral remote sensing image data. Length and width of a panchromatic remote sensing image is determined. The original multi-spectral remote sensing image part area is selected. Training samples are pre-processed. Sub-sampling is performed to obtain a block of training samples. A convolution neural network model is constructed. Input and output characteristic graph of a neuron is determined. Convolution kernel convolution neural network model weight and bias are initialized. Euclidean distance loss is selected as a function of output layer to obtain Euclidean distance between the network predicted image and a reference image. The weight and bias are iteratively updated. Another part region in the original multi-spectral remote sensing image is selected. An optimal convolutional neural network model is input to obtain high resolution multi-spectral remote sensing image.
   USE - Convolutional neural network based remote sensing image panchromatic sharpening method.
   ADVANTAGE - The method enables effectively reducing spectral distortion of the processing result to enhance sharpening effect.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based remote sensing image panchromatic sharpening method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B1; T01-J10B3A; T01-J10D; T01-N01B3; T04-D07D3; T04-E
IP G06T-005/00; G06N-003/04
PD CN109102469-A   28 Dec 2018   G06T-005/00   201915   Pages: 12   Chinese
AD CN109102469-A    CN10721821    04 Jul 2018
PI CN10721821    04 Jul 2018
UT DIIDW:201902976D
ER

PT P
PN CN109101629-A
TI Method for representing network based on deep network structure and node attributes, involves considering network structure and node attribute information of node and learning node characteristic representation though neural network.
AU HONG R
   HE Y
   WU L
   WANG M
AE UNIV HEFEI TECHNOLOGY (UYHE-C)
GA 201902996A
AB    NOVELTY - The method involves representing a learning node characteristic through a neural network by considering influence of a network structure and node attribute information. A node adjacency matrix and an attribute relation matrix are generated between two nodes. A structure probability transfer matrix is obtained by performing normalization process on the node adjacency matrix and the attribute relation matrix. A multi-level random walk model probability matrix is obtained. Probability relationship arrays of different scales are combined to obtain a global information matrix. A low-dimension network representation matrix is obtained by an automatic coder according to an inter-node global information matrix.
   USE - Method for representing a network based on deep network structure and node attributes.
   ADVANTAGE - The method enables reducing data sparsity influence, and encoding global information of a node in a network into a low-dimensional dense vector by constructing a deep neural network, thus representing the node in the network in an accurate manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a network structure. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B4P; T01-J07B; T01-N01B3; T01-N01D
IP G06F-017/30; G06N-003/02
PD CN109101629-A   28 Dec 2018   G06F-017/30   201915   Pages: 11   Chinese
AD CN109101629-A    CN10922587    14 Aug 2018
PI CN10922587    14 Aug 2018
UT DIIDW:201902996A
ER

PT P
PN CN109102509-A
TI Segmentation model training method, involves obtaining second segmentation result of cardiac CT images according to ventricular position information, and utilizing deep learning network model as segmentation model.
AU HU Z
   MA H
   WU Y
   LIANG D
   YANG Y
   LIU X
   ZHENG H
AE SHENZHEN INST ADVANCED TECHNOLOGY (CAAT-C)
GA 201902975B
AB    NOVELTY - The method involves collecting cardiac CT images of multiple cardiac computer tomography scans for obtaining ventricular position information. The cardiac CT images are input into a deep learning network model to train the deep learning network model. A first segmentation result of the cardiac CT images is obtained, where the first segmentation result of the cardiac CT images is output by a depth learning network model. A second segmentation result of the cardiac CT images is obtained according to the ventricular position information. The deep learning network model is utilized as a segmentation model.
   USE - Segmentation model training method.
   ADVANTAGE - The method enables obtaining higher segmentation accuracy of the segmentation model, and training the deep learning network model to obtain automatic ventricle segmentation of cardiac CT image segmentation model.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a segmentation model training device
   (2) a computer readable storage medium for storing a set of instructions for training a segmentation model.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a segmentation model training method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J10B2; T01-N01B3; T01-N01E; W04-W05A
IP G06T-007/10
PD CN109102509-A   28 Dec 2018   G06T-007/10   201915   Pages: 15   Chinese
AD CN109102509-A    CN10716876    03 Jul 2018
PI CN10716876    03 Jul 2018
UT DIIDW:201902975B
ER

PT P
PN CN109101495-A
TI Image recognition and knowledge map paper counter text generating method, involves establishing knowledge map, determining abstract tag of basic picture video data as search key, and generating abstract character based on similarity text.
AU HUANG J
AE SHANGHAI BAOZUN E COMMERCE CO LTD (SHAN-Non-standard)
GA 201902999J
AB    NOVELTY - The method involves extracting basic picture video data by performing depth learning operation (S1). Decomposing operation is performed in the basic picture video data. Depth image algorithm is trained. A knowledge map is established (S2). The basic picture video data is associated with prevalence information. An abstract tag of the basic picture video data is determined (S3) as a search key. A similarity text is searched in a professional article database. An abstract character is generated according to the similarity text. Logic relationship is determined.
   USE - Image recognition and knowledge map paper counter text generating method.
   ADVANTAGE - The method enables increasing text generating efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an image recognition and knowledge map paper counter text generating method. '(Drawing includes non-English language text)'
   Step for performing depth learning operation (S1)
   Step for establishing knowledge map (S2)
   Step for determining abstract tag of basic picture video data as search key (S3)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B3; T01-J05B4F; T01-J10B2A; T01-J11A1; T01-J30A; T04-K03B
IP G06F-017/27; G06F-017/25; G06F-017/30
PD CN109101495-A   28 Dec 2018   G06F-017/27   201915   Pages: 6   Chinese
AD CN109101495-A    CN10982554    27 Aug 2018
PI CN10982554    27 Aug 2018
UT DIIDW:201902999J
ER

PT P
PN CN109101712-A
TI Graph network based product model designing system, has data collection module for collecting index parameter and design parameter, and product model generating module receives optimal design parameter to obtain design index parameter.
AU INVENTOR U
AE SHIJIAZHUANG CHUANGTIAN ELECTRONIC TECHN (SHIJ-Non-standard)
GA 2019029948
AB    NOVELTY - The system has a data collection module for collecting a sample model index parameter and a sample optimization design parameter to establish a data database. A setting module sets a design index parameter of a product model. A picture network generating module establishes a database graph corresponding to network. A picture network learning module inputs a sample model index parameter and a sample optimal design parameter to a depth neural network to establish a depth learning model of the depth neural network. A product model generating module receives an optimal design parameter to obtain a product model design index parameter.
   USE - Graph network based product model designing system.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a graph network based product model designing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a graph network based product model designing system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B4P; T01-J15X; T01-N01B3
IP G06F-017/50; G06N-003/08
PD CN109101712-A   28 Dec 2018   G06F-017/50   201915   Pages: 27   Chinese
AD CN109101712-A    CN10846434    27 Jul 2018
PI CN10846434    27 Jul 2018
UT DIIDW:2019029948
ER

PT P
PN CN109101935-A
TI Thermal imaging camera based human action capturing method, involves determining space position of target point, and forming image for extracting image information obtained from camera, and analyzing obtained image to form target image.
AU JIANG L
   BAO J
   LIU D
AE SHENZHEN ZHONGSHIDIAN DIGITAL TECHNOLOGY (SHEN-Non-standard)
GA 201902988R
AB    NOVELTY - The method involves obtaining an image. The image is captured by using a camera. The obtained image is processed through a PC end. Binocular range measurement is performed. Direct difference between left and right views of a target point imaging and distance between a target point and an imaging surface are inverse proportioned. A space position of the target point is determined. The image is formed for extracting image information obtained from the camera. The obtained image is analyzed to form a target image. Inner and outer parameters of the camera are obtained through binocular camera calibration.
   USE - Thermal imaging camera based human action capturing method.
   ADVANTAGE - The method enables realizing artificial intelligent depth learning identification for providing motion capture system scheme, and using infrared thermal imaging technology without affecting from sun light, and realizing strong diffraction for human visual disorders to make people to view temperature distribution of an object surface.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a thermal imaging camera based human action capturing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J10B2; T04-D07D5; W04-M01D2C; W04-M01D2J; W04-M01E1
IP G06K-009/00; G06T-007/80
PD CN109101935-A   28 Dec 2018   G06K-009/00   201915   Pages: 7   Chinese
AD CN109101935-A    CN10950354    20 Aug 2018
PI CN10950354    20 Aug 2018
UT DIIDW:201902988R
ER

PT P
PN CN109104199-A
TI Huffman code encoding method, involves obtaining remaining length in current independent sequence of placeholder code to finally form sequential connection by independent sequence and coding sequence of specified length.
AU JIANG Z
AE CHONGQING WUQI TECHNOLOGY CO LTD (CHON-Non-standard)
   SHANGHAI WUQI TECHNOLOGY CO LTD (SHAN-Non-standard)
GA 2019029407
AB    NOVELTY - The method involves executing a Huffman code by a coding unit at first specified length of input sequence. Code value i.e. very preamble value, is obtained in Huffman encoding sequence. Second specified length of independent sequence is determined in the code value. Remaining length is obtained in current independent sequence of a placeholder code without affecting decoding result to finally form sequential connection by the independent sequence and coding sequence of the second specified length.
   USE - Huffman code encoding method.
   ADVANTAGE - The method enables adapting a CNN without need to search boundary so as to improve coding efficiency in an effective manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a Huffman code decoding method
   (2) a data compression and decompression method
   (3) a convolutional neural network (CNN) weight parameter compression and decompression method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating an operation of a Huffman code encoding method.
DC T01 (Digital Computers); U21 (Logic Circuits, Electronic Switching and Coding)
MC T01-D01; T01-N03A2; U21-A05A2A
IP H03M-007/40
PD CN109104199-A   28 Dec 2018   H03M-007/40   201915   Pages: 12   Chinese
AD CN109104199-A    CN10998614    29 Aug 2018
PI CN10998614    29 Aug 2018
UT DIIDW:2019029407
ER

PT P
PN CN109102547-A
TI Object identification deep learning model-based robot grabbing pose estimation method, involves realizing iteration and optimization of pose estimation of target object during process of moving end of robot to target object.
AU LI M
   WANG J
   REN M
AE SHANGHAI JIEKA ROBOT TECHNOLOGY CO LTD (SHAN-Non-standard)
GA 201902974E
AB    NOVELTY - The method involves performing fusion of a two-dimensional visual information and a three-dimensional visual information, and obtaining a target object point cloud. An object pose estimation is performed by registering the point cloud of the target object with a cloud template in a template library. A sample error avoidance algorithm is utilized based on sample accumulation for avoiding an error condition. A visual system is utilized to continuously repeat the above steps to realize an iteration and optimization of the pose estimation of the target object during a process of moving an end of a robot to the target object.
   USE - Object identification deep learning model-based robot grabbing pose estimation method.
   ADVANTAGE - The method enables performing rapid target detection using target detection YOLO model by the algorithm, and reducing a calculation amount for segmentation and matching the three-dimensional point cloud of the target object, while improving the operation efficiency and accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an object identification deep learning model-based robot grabbing pose estimation method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-J30A
IP G06T-007/80; G06T-007/30
PD CN109102547-A   28 Dec 2018   G06T-007/80   201915   Pages: 9   Chinese
AD CN109102547-A    CN10803444    20 Jul 2018
PI CN10803444    20 Jul 2018
UT DIIDW:201902974E
ER

PT P
PN CN109102019-A
TI HP-Net convolutional neural network-based image classification method, involves performing regularization operation, and inputting output result of convolutional neural network to soft max classifier to obtain classification result.
AU LI X
   WU X
   SHI C
   LV J
   LI L
   GUO F
   LUO C
   ZHANG X
   LIU S
   LEE C
AE UNIV CHENGDU INFORMATION TECHNOLOGY (UYUH-C)
GA 201902986M
AB    NOVELTY - The method involves reading an input image. The input image is divided into image blocks. A number of the image block is determined. A check image is input into a convolutional neural network for training process. A one-dimensional matrix is obtained through three-layer full connecting layer after multiple extracting features. Regularization operation is performed. An output result of the convolutional neural network is input to a soft max classifier to obtain a classification result. Convolution processing is performed to the check image. Convolution processing is performed to a convolution kernel.
   USE - HP-Net convolutional neural network-based image classification method.
   ADVANTAGE - The method enables realizing better performance, and ensuring flowers data set in existing network model.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a HP-Net convolutional neural network-based image classification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J05B2; T01-J10B2; T01-N01B3; T04-D04
IP G06K-009/62; G06N-003/04
PD CN109102019-A   28 Dec 2018   G06K-009/62   201915   Pages: 8   Chinese
AD CN109102019-A    CN10907142    09 Aug 2018
PI CN10907142    09 Aug 2018
UT DIIDW:201902986M
ER

PT P
PN CN109101717-A
TI Intelligent automation solid rocket engine reliability predicting method, involves performing solid rocket engine reliability prediction process based on characteristic of self-learning model generalization capability intensity property.
AU LIN J
   WANG K
   YANG Y
   LIN Z
   CHEN M
AE UNIV CHONGQING (UYCQ-C)
GA 2019029945
AB    NOVELTY - The method involves constructing a real data set based on solid rocket engine field storage reliability experiment record data. A fuzzy data set is constructed as a stress-strength interference model according to the solid rocket engine acceleration storage reliability experiment record data. Solid rocket engine reliability data and accelerated storage reliability data are fused to form a deep learning database to divide a training set, a validation set and a testing set. Solid rocket engine reliability prediction process is performed based on characteristic of deep learning algorithm robustness and self-learning model generalization capability intensity property.
   USE - Reality and fuzzy data fusion depth learning based intelligent automation solid rocket engine reliability predicting method.
   ADVANTAGE - The method enables effectively improving solid rocket motor reliability prediction precision.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a reality and fuzzy data fusion depth learning based intelligent automation solid rocket engine reliability predicting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B4P; T01-J07D; T01-J15X; T01-J16C2; T01-J30A
IP G06F-017/50
PD CN109101717-A   28 Dec 2018   G06F-017/50   201915   Pages: 10   Chinese
AD CN109101717-A    CN10891087    07 Aug 2018
PI CN10891087    07 Aug 2018
UT DIIDW:2019029945
ER

PT P
PN CN109101888-A
TI Visitors flow monitoring and pre-warning method, involves collecting video shot by camera, training human head detection model based on deep learning network, and obtaining trained human head detection model to output alarm signal.
AU LIU Y
   DING S
   ZHAO W
   XU K
   QU P
   ZHOU Y
AE UNIV NANJING AGRIC (UYNA-C)
GA 201902989W
AB    NOVELTY - The method involves collecting a video shot by a camera. Video illumination is determined based on the camera video. Compensated Gaussian model detection algorithm is utilized to extract foreground and foreground outputs. Area ratio of the foreground output is determined. High-density crowd tracking process is performed according to a scenic spot density threshold value. A high-density crowd image is output by using migration learning technology. A human head detection model is trained based on a deep learning network. The trained human head detection model is obtained to output an alarm signal.
   USE - Visitors flow monitoring and pre-warning method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a visitors flow monitoring and pre-warning method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-N01B3; T01-N02B2
IP G06K-009/00; G06K-009/32; G06N-003/04; G06N-003/08
PD CN109101888-A   28 Dec 2018   G06K-009/00   201915   Pages: 13   Chinese
AD CN109101888-A    CN10763293    11 Jul 2018
PI CN10763293    11 Jul 2018
UT DIIDW:201902989W
ER

PT P
PN CN109101897-A
TI Underwater robot target detection method, involves selecting maximum likelihood value corresponding to surrounding frame, and selecting highest bounding frame from remaining surrounding frames until surrounding frames are processed.
AU LIU Z
   ZHANG L
   YANG X
   QIAO H
AE CHINESE ACAD SCI AUTOMATION INST (CAZD-C)
GA 201902989M
AB    NOVELTY - The method involves obtaining an original image to be detected (A1). Pixel values of the original image to be detected are normalized (A2) to obtain a pre-processed image to be detected. The pre-processed image to be detected is input (A3) into a target detection network for detection. An enclosing frame and a category of a target object are obtained (A4) according to a surrounding frame of region of interest and probability belonging to target class. A maximum likelihood value is selected corresponding to the surrounding frame according to the region of interest belonging to the target class. A highest bounding frame is selected from remaining surrounding frames until surrounding frames are processed.
   USE - Underwater robot target detection method.
   ADVANTAGE - The method enables extracting feature maps based the target detection network by using a deformable convolutional neural network so as to effectively perform target detection by using a candidate region, thus improving detection accuracy and speed.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a storage device
   (2) a processing device
   (3) an underwater robot target detection system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an underwater robot target detection method. '(Drawing includes non-English language text)'
   Step for obtaining an original image to be detected (A1)
   Step for normalizing pixel values of original image to be detected to obtain pre-processed image to be detected (A2)
   Step for inputting pre-processed image to be detected into a target detection network for detection (A3)
   Step for obtaining enclosing frame and category of a target object according to a surrounding frame of region of interest and probability belonging to target class (A4)
DC T01 (Digital Computers)
MC T01-J10B2
IP G06K-009/00; G06K-009/32; G06N-003/04
PD CN109101897-A   28 Dec 2018   G06K-009/00   201915   Pages: 20   Chinese
AD CN109101897-A    CN10806439    20 Jul 2018
PI CN10806439    20 Jul 2018
UT DIIDW:201902989M
ER

PT P
PN CN109102069-A
TI Lookup table based rapid image convolution operation realizing method, involves obtaining mapping value by parity of reasoning, and determining and convoluting convolution region to obtain image pixel value of convolution region.
AU MAO Z
   DU H
   ZHANG X
   ZHANG L
   CHANG L
AE UNIV XIAN POST & TELECOM (USPT-C)
GA 201902985C
AB    NOVELTY - The method involves searching a multiplication result to perform additive operation to obtain a first characteristic mapping value (S5). An input image region is selected (S6) from multiplies for configuring a cyclic shift mode and determining size of the input image region. An index address is selected (S7) from a lookup table corresponding to the multiplication result. A second characteristic mapping value is obtained (S8) by parity of reasoning after performing the additive operation based on the multiplication result. A convolution region is determined (S9) under the cyclic shift mode and convoluted to obtain an image pixel value of the convolution region.
   USE - Lookup table based rapid image convolution operation realizing method.
   ADVANTAGE - The method enables finding the multiplication result from the lookup table in a parallel manner for adding the multiplication result to addition operation so as to obtain a point value in a feature mapping matrix and reduce operation amount of the convolution operation in a convolutional neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a lookup table based rapid image convolution operation realizing method. '(Drawing includes non-English language text)'
   Step for searching multiplication result to obtain first characteristic mapping value (S5)
   Step for selecting input image region (S6)
   Step for selecting index address (S7)
   Step for obtaining second characteristic mapping value (S8)
   Step for determining convolution region (S9)
DC T01 (Digital Computers)
MC T01-E02A; T01-E03; T01-G01A1; T01-J04B2; T01-J17; T01-N03A2
IP G06N-003/04; G06N-003/063; G06N-099/00
PD CN109102069-A   28 Dec 2018   G06N-003/04   201915   Pages: 11   Chinese
AD CN109102069-A    CN10804759    20 Jul 2018
PI CN10804759    20 Jul 2018
UT DIIDW:201902985C
ER

PT P
PN CN109101579-A
TI Customer service robot knowledge base ambiguity detection method, involves training depth learning model to update knowledge base, and continuing process until learning effect is enhanced to eliminate ambiguity of knowledge base.
AU OU Z
   PAN S
   LIU Y
   WU Y
   CHEN Z
   YANG Z
   HU X
   WEN L
AE SHENZHEN ZHUIYI TECHNOLOGY CO LTD (SHEN-Non-standard)
GA 201902997N
AB    NOVELTY - The method involves constructing a knowledge base, where the knowledge base is divided by FAQ that is provided with similar question sentence. A repository is divided into a test set and a training set. A depth learning model is trained to update the knowledge base according to an ambiguity detection result. The process is continued until learning effect is enhanced to eliminate ambiguity of the knowledge base. Corresponding preset number of FAQ data is randomly extracted corresponding to a testing category. Question sentence in the training set is input as an input part to the depth learning model.
   USE - Customer service robot knowledge base ambiguity detection method.
   ADVANTAGE - The method enables extracting data from the knowledge base as a training set and testing set for the depth learning model so as to improve learning effect.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a customer service robot knowledge base ambiguity detection method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B4P; T01-J10B2; T01-J16A; T01-J30A; T04-D04
IP G06F-017/30; G06K-009/62
PD CN109101579-A   28 Dec 2018   G06F-017/30   201915   Pages: 12   Chinese
AD CN109101579-A    CN10801678    19 Jul 2018
PI CN10801678    19 Jul 2018
UT DIIDW:201902997N
ER

PT P
PN CN109101972-A
TI Semantic segmentation convolutional neural network, has global feature extractor for storing weight semantics of global information by training neural network to obtain band code dividing convolutional neural network.
AU PANG Y
   SUN H
AE UNIV TIANJIN (UTIJ-C)
GA 201902987W
AB    NOVELTY - The network has a global feature extractor for adding a global feature fusion into a SegNet and defining an original structure of a SegNet characteristic in a local feature. The global feature extractor extracts feature called global features, global features and local features to context information. The global feature extractor extracts global features with existing element. The global feature extractor sets a weight attenuation coefficient and a training learning rate to a loss function value. The global feature extractor stores weight semantics of global information by training a neural network to obtain a band code dividing convolutional neural network.
   USE - Semantic segmentation convolutional neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a semantic segmentation convolutional neural network. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J16C3; T01-N01B3; T04-D02; T04-D03
IP G06K-009/34; G06K-009/46
PD CN109101972-A   28 Dec 2018   G06K-009/34   201915   Pages: 7   Chinese
AD CN109101972-A    CN10837814    26 Jul 2018
PI CN10837814    26 Jul 2018
UT DIIDW:201902987W
ER

PT P
PN CN109101976-A
TI Method for detecting defect on surface of arc extinguishing grid, involves inputting feature vector of Gabor feature image into convolutional neural network model, and determining probability of defects on surface of arc extinguishing grid.
AU SHU L
   GUO L
   WU G
   LIANG B
   CHEN W
AE UNIV WENZHOU (UYWE-Non-standard)
GA 201902987S
AB    NOVELTY - The method involves obtaining an original image of an arc-extinguishing grid (S1). The original image is pre-processed to obtain a pre-processed target image. The target image is processed (S2) into a gradient histogram. Feature vector of the gradient histogram is determined. The feature vector of the gradient histogram is extracted to obtain a Gabor feature image. Feature vector of the Gabor feature image is extracted by principal component analysis. The feature vector of the Gabor feature image is input (S3) into a preset convolutional neural network model. Probability of multiple defects on a surface of the arc extinguishing grid is determined.
   USE - Method for detecting defect on a surface of an arc extinguishing grid.
   ADVANTAGE - The method enables extracting the image of the arc-extinguishing grid from a background by using image feature extraction process so as to improve arc extinguishing grid surface defect detecting speed and accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for detecting defect on a surface of an arc extinguishing grid. '(Drawing includes non-English language text)'
   Step for obtaining original image of arc-extinguishing grid (S1)
   Step for processing target image into gradient histogram (S2)
   Step for inputting feature vector of Gabor feature image into convolutional neural network model and determining probability of defects on surface of arc extinguishing grid (S3)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B1; T01-J10B2; T04-D03
IP G06K-009/40; G06K-009/46; G06T-005/40; G06T-007/00
PD CN109101976-A   28 Dec 2018   G06K-009/40   201915   Pages: 12   Chinese
AD CN109101976-A    CN10749341    10 Jul 2018
PI CN10749341    10 Jul 2018
UT DIIDW:201902987S
ER

PT P
PN CN109101983-A
TI Deep learning based shoe and footprint key point detecting method, involves training network model by partial network structure for adjusting transfer learning mode, inputting image in network model, and marking coordinates output result.
AU SUN X
   YU X
   LI D
   CUI J
   ZHAO X
AE DALIAN EVERSPRY SCI & TECH CO LTD (DALI-Non-standard)
GA 201902987L
AB    NOVELTY - The method involves obtaining a shoe/footprint database. A network model is set. Loss function is calculated based on a sole/footprint profile. The network model is training by a partial network structure for adjusting a transfer learning mode. An image is input in the trained network model. A coordinates output result is marked. Result index evaluating is performed. Main direction is determined to measure effective area ratio and error rate. A sample picture is obtained by using artificial calibration mode marked key points for generating sample data set and footprint data set.
   USE - Deep learning based shoe and footprint key point detecting method.
   ADVANTAGE - The method enables reducing labor intensity of the sole/footprint profile.
   DESCRIPTION OF DRAWING(S) - The drawing shows a front view of a sole/footprint profile.
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J05B4P; T01-J10B2; T01-N01B3; T01-N01D1B; T04-D03; T04-D04; W04-W05A
IP G06K-009/46; G06K-009/62; G06T-007/11; G06T-007/62; G06F-017/30
PD CN109101983-A   28 Dec 2018   G06K-009/46   201915   Pages: 22   Chinese
AD CN109101983-A    CN10870908    02 Aug 2018
PI CN10870908    02 Aug 2018
UT DIIDW:201902987L
ER

PT P
PN CN109101899-A
TI Convolutional neural network-based human face detecting method, involves obtaining human face detecting result of to-be-detected picture in detection output layer of modified single-step multi-scale detector.
AU WANG L
   DONG Y
   BAI H
   XIONG F
AE BEIJING FACEALL TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 201902989K
AB    NOVELTY - The method involves inputting a to-be-detected picture into a modified single-step multi-scale detector after training. The modified single-step multi-scale detector is constructed by adding an enhanced interleaving structure and a context feature fusion structure to a S3FD. An anchor policy is changed. Loss functions are adjusted. A human face detecting result of the to-be-detected picture is obtained in a detection output layer of the modified single-step multi-scale detector. The modified single-step multi-scale detector is trained to obtain a trained modified single-step multi-scale detector. The enhanced interleaving structure is utilized in multiple preset convolution layers by the S3FD to form a net output structure.
   USE - Convolutional neural network-based human face detecting method.
   ADVANTAGE - The method enables strengthening ability to utilize features, and improving detecting ability of small faces so as to improve recall rate and face detection accuracy.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a convolutional neural network-based human face detecting system
   (2) a convolutional neural network-based human face detecting device
   (3) a non-transitory computer readable storage medium for storing set of instruction for detecting human face based on convolutional neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network-based human face detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B2; T01-L02; T01-N01B3; T04-D07F1
IP G06K-009/00; G06N-003/04; G06N-003/08
PD CN109101899-A   28 Dec 2018   G06K-009/00   201915   Pages: 12   Chinese
AD CN109101899-A    CN10813668    23 Jul 2018
PI CN10813668    23 Jul 2018
UT DIIDW:201902989K
ER

PT P
PN CN109101866-A
TI Segmenting silhouette based pedestrian re-identifying method, involves establishing pedestrian comparison sample identification model according to color image of preset pedestrian comparison sample corresponding to silhouette.
AU WANG L
   HUANG Y
   SONG C
AE CHINESE ACAD SCI AUTOMATION INST (CAZD-C)
GA 201902990D
AB    NOVELTY - The method involves obtaining pedestrian feature based on a pre-building pedestrian re- recognition model according to color image segmentation silhouette. Similarity between the pedestrian and pedestrian identification feature is calculated corresponding to detected pedestrian. Pedestrian identity characteristics are obtained corresponding to maximum similarity. Pedestrian identity characteristic identity information is obtained by pedestrian. The pedestrian recognition model is selected as a deep convolutional neural network model according to preset pedestrian test sample. A pedestrian comparison sample identification model is established according to a color image of preset pedestrian comparison sample corresponding to the silhouette.
   USE - Segmenting silhouette based pedestrian re-identifying method.
   ADVANTAGE - The method enables eliminating image background noise, and improving pedestrian recognition accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a segmenting silhouette based pedestrian re-identifying system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a segmenting silhouette based pedestrian re-identifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B1; T01-J10B2A; T01-J10B3B; T04-D03A; T04-D04; T04-D08
IP G06K-009/00; G06K-009/62; G06T-007/11; G06T-007/194
PD CN109101866-A   28 Dec 2018   G06K-009/00   201915   Pages: 15   Chinese
AD CN109101866-A    CN10567647    05 Jun 2018
PI CN10567647    05 Jun 2018
UT DIIDW:201902990D
ER

PT P
PN CN109101984-A
TI High-speed paper counter based convolutional neural network image recognition method, involves determining loss value according to preset gradually loss function, and detecting convergence convolutional neural network image.
AU WANG R
AE BEIJING AUTHENMETRIC DATA TECHNOLOGY CO (BEIJ-Non-standard)
GA 201902987K
AB    NOVELTY - The method involves determining first probability value of training image by using preset convolutional neural network. Preset soft interval normalized exponential function value, interval value, second probability value of the training image are calculated. Loss value is determined according to the second probability value and preset gradually loss function. Judgment is made to check whether preset convolution neural network is convergence convolutional neural network when the loss value is converged. Convergence convolutional neural network image is detected.
   USE - High-speed paper counter based convolutional neural network image recognition method.
   ADVANTAGE - The method enables improving unknown images recognizing efficiency and recognition results generating accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a high-speed paper counter based convolutional neural network image recognition device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a high-speed paper counter based convolutional neural network image recognition method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B2A; T01-N01B3; T04-D04
IP G06K-009/62; G06N-003/04
PD CN109101984-A   28 Dec 2018   G06K-009/62   201915   Pages: 17   Chinese
AD CN109101984-A    CN10468011    20 Jun 2017
PI CN10468011    20 Jun 2017
UT DIIDW:201902987K
ER

PT P
PN CN109102457-A
TI Convolutional neural network based intelligent target image color changing system, has color changing result correction module that performs image re-rendering process and image color fine tuning process according to user requirement.
AU WU S
   ZHANG M
   JIN H
   WU C
AE HANGZHOU MIHUI TECHNOLOGY CO LTD (HANG-Non-standard)
GA 201902976N
AB    NOVELTY - The system has an image training set preparation module for converting a picture into hue, saturation, value (HSV) and changing color of a pattern by modifying H-channel content to generate an image training set at multi-color. A convolutional network- training module input a training image to a LAB and performs color variation process. The convolutional network- training module input the training image to a convolutional network for performing a convolutional network training process. A color changing result correction module provides an individual color editing function and performs an image re-rendering process and an image color fine tuning process according to user requirement.
   USE - Convolutional neural network based intelligent target image color changing system.
   ADVANTAGE - The system realizes target image color changing process without changing an input image content and eliminates color noise and edge blurring problems in target image color changing process.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a convolutional neural network based intelligent target image color changing system realizing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based intelligent target image color changing system realizing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B3A; T01-J10B3B; T01-N01B3; T01-N01D1B; T01-N01D2; T04-D08
IP G06T-003/00; G06N-003/04; G06N-003/08
PD CN109102457-A   28 Dec 2018   G06T-003/00   201915   Pages: 10   Chinese
AD CN109102457-A    CN10601630    12 Jun 2018
PI CN10601630    12 Jun 2018
UT DIIDW:201902976N
ER

PT P
PN CN109099910-A
TI Inertial navigation unit array based high-precision inertial navigation system, has algorithm processor arranged on system PCB, and algorithm processor connected to output port for outputting operation result of processor to external device.
AU XIAO Y
   LI G
   SHI L
   CHEN H
AE GUANGDONG STARCART TECHNOLOGY CO LTD (GUAN-Non-standard)
GA 2019030375
AB    NOVELTY - The system has an inertial measurement unit (IMU) array provided with multiple IMU chips i.e. MPU9250 chip, that are arranged on an IMU matrix system PCB according to a set rule. An algorithm processor is arranged on the IMU matrix system PCB, and is connected to the IMU array for reading data of the IMU array in parallel, and calculating a true value of the IMU array according to a depth learning algorithm. The algorithm processor is connected with an output port for outputting an operation result of the algorithm processor to an external device. Upper and lower surfaces of the IMU matrix system PCB are connected with eight IMU chips, where the algorithm processor comprises an FPGA chip i.e. XC7Z020 type.
   USE - Inertial navigation unit array based high-precision inertial navigation system.
   ADVANTAGE - The system can form an array by utilizing sixteen low-precision IMU chips so as to cancel a part of inherent error term by using the IMU array arranged according to a certain rule, and can apply a deep learning algorithm to train law of error generation, thus improving overall measurement accuracy, and realizing effect of the high-precision IMU chip and greatly reducing production cost.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an inertial navigation unit array based high-precision inertial navigation system realizing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an inertial navigation unit array based high-precision inertial navigation system.
DC S02 (Engineering Instrumentation); T01 (Digital Computers); V04 (Printed Circuits and Connectors)
MC S02-B08G; S02-B08X; T01-F06; T01-J16C2; T01-J21; T01-J30A; V04-Q30C
IP G01C-021/16
PD CN109099910-A   28 Dec 2018   G01C-021/16   201915   Pages: 10   Chinese
AD CN109099910-A    CN10714200    29 Jun 2018
PI CN10714200    29 Jun 2018
UT DIIDW:2019030375
ER

PT P
PN CN109101865-A
TI Deep learning based pedestrian identification method, involves calculating similarity degree between target and reference pedestrian images, and ranking target pedestrian image according to similarity degree to obtain recognition result.
AU XIONG W
   XIONG Z
   TONG L
   FENG C
   WANG J
   ZENG C
   LIU M
   WANG C
   GUAN L
   JIN J
   JIA X
AE UNIV HUBEI TECHNOLOGY (UYHI-C)
GA 201902990E
AB    NOVELTY - The method involves performing pedestrian feature extraction by combining global features and local features. A Euclidean distance value is calculated in feature metric under distance constraint of a feature vector. Loss function is established based on metric matrix. Loss function is added with constraint function based on traditional Triplet Loss to optimize the CNN model. A test data set image is input into the trained CNN model to obtain image features. A similarity degree between a target pedestrian image and a reference pedestrian image is calculated by using the Euclidean distance value. The target pedestrian image is ranked according to the similarity degree to obtain a pedestrian recognition result.
   USE - Deep learning based pedestrian identification method.
   ADVANTAGE - The method enables avoiding downlink complex scene and scene change, and realizing pedestrian identification with strong practicability, stable algorithm and rapid identification speed.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning based pedestrian identification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04A; T01-J10B2A; T01-J30A
IP G06K-009/00; G06K-009/46; G06K-009/62
PD CN109101865-A   28 Dec 2018   G06K-009/00   201915   Pages: 11   Chinese
AD CN109101865-A    CN10549705    31 May 2018
PI CN10549705    31 May 2018
UT DIIDW:201902990E
ER

PT P
PN CN109102065-A
TI Programmable system-on-chip based convolutional neural network accelerator, has CPU for controlling output characteristic pattern and parameters of convolutional neural network layers to adapt to different architecture of neural network.
AU XIONG X
   LI Z
   ZENG Y
   HU X
AE UNIV GUANGDONG TECHNOLOGY (UGTE-C)
   CHIPEYE MICROELECTRONICS FOSHAN LTD (CHIP-Non-standard)
GA 201902985G
AB    NOVELTY - The accelerator has a direct memory access (DMA) for reading and transmitting data of a characteristic pattern input memory under control of a CPU, an offset memory and a weight memory. A chip outer memory writes data of the characteristic pattern input memory. The CPU controls input feature map, offset weight, output characteristic pattern in on-chip memory storage locations and parameters of convolutional neural network layers to adapt to different architecture of a neural network. A calculating unit is provided with a FIFO queue, a state machine, data selectors and an average pool module.
   USE - Programmable system-on-chip (PSoC) based convolutional neural network accelerator.
   ADVANTAGE - The accelerator measures large calculated amount to improve high-parallelism of a convolutional neural network calculating part by using CPU serial algorithm and state control.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a PSoC based convolutional neural network accelerator. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-E01B; T01-H01C2; T01-N01D
IP G06N-003/04; G06N-003/063
PD CN109102065-A   28 Dec 2018   G06N-003/04   201915   Pages: 10   Chinese
AD CN109102065-A    CN10689938    28 Jun 2018
PI CN10689938    28 Jun 2018
UT DIIDW:201902985G
ER

PT P
PN CN109102549-A
TI Image light source color detecting method, involves determining illumination characteristic of quantization value map, performing light source color detection process corresponding to input to network model, and outputting detection result.
AU XU B
   QIU G
AE UNIV SHENZHEN (UYSZ-C)
GA 201902974C
AB    NOVELTY - The method involves performing quantization process. An overall image color feature is extracted. Global binarization process is performed. A quantized binary pattern is obtained. Four image blocks are determined based on similarity of the image. Four images are input into four input depth learning network models for determining an illumination characteristic. An illumination characteristic of a quantization value map is determined. Light source color detection process is performed corresponding to input to a regression network model. A detection result is output. A quantized binary map is obtained. Images are classified according to the illumination similarity.
   USE - Image light source color detecting method.
   ADVANTAGE - The method enables extracting characteristics related to light source color and accurately detecting light source color of an image.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an image light source color detecting device
   (2) a computer device
   (3) a storage medium for storing a set of instructions for detecting image light source color.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an image light source color detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J10B3B; T01-N01B3; T04-D03; T04-D07D3; T04-D08
IP G06T-007/90; G06T-007/12; G06K-009/46; G06N-003/08; G06N-003/04
PD CN109102549-A   28 Dec 2018   G06T-007/90   201915   Pages: 14   Chinese
AD CN109102549-A    CN10942161    17 Aug 2018
PI CN10942161    17 Aug 2018
UT DIIDW:201902974C
ER

PT P
PN CN109102475-A
TI Method for removing rain-drop image by electronic device, involves inputting category type into convolutional neural network, and updating parameter of convolutional neural network with residual structure to obtain final rain-drop image.
AU XU J
   DONG Y
   BAI H
   XIONG F
AE BEIJING FACEALL TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 2019029768
AB    NOVELTY - The method involves obtaining a to-be-detected image with raindrops. The to-be-detected image is separated into a high-frequency component image and a low-frequency component image. The high-frequency component image is inputted into a trained convolutional neural network with a residual structure. A rain-removed image is outputted. The de-rain image is combined with the to-be-detected image to obtain a rain-drop image. The rain-drop image is inputted into the trained convolutional neural network with a discriminant structure. A category type of the rain-drop image corresponding to the to-be-detected image is outputted. The category type is inputted into the convolutional neural network with the residual structure. A parameter of the convolutional neural network with the residual structure is updated to obtain a final rain-drop image.
   USE - Method for removing a rain-drop image by an electronic device (claimed).
   ADVANTAGE - The method enables obtaining texture details of the to-be-detected image without rain, so that the final rainless image can be close to the raindrop image corresponding to the to-be-detected image.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for removing a rain-drop image by an electronic device
   (2) a computer readable storage medium for storing a set of computer programs for removing a rain-drop image by an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for removing a rain-drop image by an electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B1; T01-J10B2; T04-D03; T04-D04
IP G06T-005/00; G06K-009/46; G06K-009/62; G06N-003/04
PD CN109102475-A   28 Dec 2018   G06T-005/00   201915   Pages: 11   Chinese
AD CN109102475-A    CN10917404    13 Aug 2018
PI CN10917404    13 Aug 2018
UT DIIDW:2019029768
ER

PT P
PN CN109102002-A
TI Method for classifying image based on convolutional neural network, involves loading training samples of concept machine recurrent neural network into storage pool, and determining image category corresponding to test sample.
AU XUE F
   LIU Y
   LI X
   GU J
   LUO S
   LIU H
AE UNIV CHONGQING (UYCQ-C)
GA 2019029874
AB    NOVELTY - The method involves establishing a Resnet50 residual neural network, where the Resnet50 residual neural network comprises a convolution layer, a pooling layer and a connection layer. A concept recurrent neural network is established. A training sample set of the image is input into a fused neural network. Inverse discrimination basis of the image is calculated. Reverse concept machine symbols of the image are obtained. Training samples of the concept machine recurrent neural network are loaded into a storage pool. Image category corresponding to the test sample is determined.
   USE - Method for classifying an image based on a convolutional neural network and conceptual machine recurrent neural network.
   ADVANTAGE - The method enables utilizing the concept machine recurrent neural network to incrementally recognize images without repeating learning and training of learned images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for classifying image based on convolutional neural network. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J05B2A; T01-J10B2A; T01-N01B3A; T04-D04
IP G06K-009/62; G06N-003/04; G06N-003/08
PD CN109102002-A   28 Dec 2018   G06K-009/62   201915   Pages: 11   Chinese
AD CN109102002-A    CN10783905    17 Jul 2018
PI CN10783905    17 Jul 2018
UT DIIDW:2019029874
ER

PT P
PN CN109102157-A
TI Method for intelligently dispatching work order list of bank based on computer device by utilizing computer device, involves determining and distributing event working order of single resolution system corresponding to network model event.
AU XUE W
   ZHANG W
   TU W
   ZHU S
   BU Y
   WANG N
AE BANK COMMUNICATIONS CO LTD (BANK-Non-standard)
GA 201902983B
AB    NOVELTY - The method involves generating a word vector matrix corresponding to an event working order to obtain a summary text of the received event working order. The word vector matrix entered into a neural network model based on training of a working order of a historical event. An event working order of a single resolution system is determined corresponding to the neural network model, where the neural network model comprises a first-stage convolutional neural network layer and a second-stage convolutional neural network layer. The event working order of the single resolution system is distributed.
   USE - Method for intelligently dispatching work order list of a bank based on computer device by utilizing a computer device (claimed).
   ADVANTAGE - The method enables intelligently dispatching the work order based on advanced learning to improve work order list dispatching accuracy and efficiency so as to save labor cost and work event solving time technical effect, thus effectively improving customer satisfaction.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for intelligently dispatching work order list of a bank based on computer device by utilizing a computer device
   (2) a computer readable storage medium for storing set of instructions for intelligently dispatching work order list of a bank based on computer device by utilizing a computer device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for intelligently dispatching work order list of a bank based on computer device by utilizing a computer device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-E01A; T01-J10B2; T01-N01A1; T01-N01A2; T01-N01B3; T01-N01D; T04-D04
IP G06Q-010/06; G06Q-040/02; G06N-003/04; G06K-009/62
PD CN109102157-A   28 Dec 2018   G06Q-010/06   201915   Pages: 19   Chinese
AD CN109102157-A    CN10755334    11 Jul 2018
PI CN10755334    11 Jul 2018
UT DIIDW:201902983B
ER

PT P
PN CN109101552-A
TI Depth learning based phishing website URL detecting method, involves iteratively training convolutional neural network model by utilizing cross entropy loss function based on Adam adaptive time estimation algorithm to optimize loss function.
AU YANG P
   ZENG P
   LI Y
   ZHANG C
   ZHENG B
AE UNIV SOUTHEAST (UYSE-C)
GA 201912449N
AB    NOVELTY - The method involves extracting partial correlation characteristics by an embedding matrix based on a convolutional neural network layer of a convolutional neural network. Convolutional neural network model establishing complexity is reduced based on extraction of the partial correlation characteristics by utilizing a pool layer. Semantics and long-range dependency relationship is established in a pooled sequence based on a long and short term memory network (LSTM). The dependency relationship is input to a Softmax unit. A convolutional neural network model is iteratively trained by utilizing cross entropy loss function based on an Adam adaptive time estimation algorithm to optimize the loss function.
   USE - Depth learning based phishing website URL detecting method.
   ADVANTAGE - The method enables avoiding propagation characteristic redundancy and training the convolutional neural network model through the LSTM network so as to accurately detect phishing website URL in a quick manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a convolutional neural network model. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J05B4P; T01-J16C3; T01-N01B3; T01-N02A1A
IP G06F-017/30; G06N-003/04
PD CN109101552-A   28 Dec 2018   G06F-017/30   201915   Pages: 10   Chinese
AD CN109101552-A    CN10750707    10 Jul 2018
PI CN10750707    10 Jul 2018
UT DIIDW:201912449N
ER

PT P
PN CN109101913-A
TI Method for identifying pedestrian by utilizing electronic device, involves obtaining characteristic vector of to-be-identified video image sequence, and outputting pedestrian recognition result according to identified video image sequence.
AU YANG X
   DONG Y
   BAI H
   XIONG F
AE BEIJING FACEALL TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 2019029898
AB    NOVELTY - The method involves training a preset pedestrian recognition model to obtain a video image sequence, where the preset pedestrian recognition model comprises a first convolutional neural network and a second convolutional network. Multiple regional characteristics of an image in a to-be-identified video image sequence are obtained by the first convolutional neural network. Weight of a pedestrian is obtained by the second convolutional network corresponding to the each regional characteristic of an image in the to-be-identified video image sequence. A characteristic vector of the to-be-identified video image sequence is obtained. Pedestrian recognition result is outputted according to the to-be-identified video image sequence.
   USE - Method for identifying a pedestrian by utilizing an electronic device (claimed).
   ADVANTAGE - The method enables improving quality of the image so as to increase better quality image contribution of the characteristic vector of the to-be-identified video image sequence, and improving pedestrian identification accuracy.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for identifying a pedestrian by utilizing an electronic device
   (2) a non-transitory computer-readable storage medium for storing a set of instructions for identifying a pedestrian by utilizing an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for identifying a pedestrian by utilizing an electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B1; T01-J10B2A; T01-N01B3; T01-S03; T04-D04
IP G06K-009/00; G06K-009/62
PD CN109101913-A   28 Dec 2018   G06K-009/00   201915   Pages: 15   Chinese
AD CN109101913-A    CN10862600    01 Aug 2018
PI CN10862600    01 Aug 2018
UT DIIDW:2019029898
ER

PT P
PN CN109102143-A
TI Method for monitoring yield production by using computer device, involves recognizing and processing produced audio data through pre-trained convolutional neural network, and determining output of device within predetermined duration.
AU YANG Z
   TIAN W
   TAN Y
   ZHUANG Y
   CHEN R
   HUANG Z
   WANG Y
AE SHUOCHENG XIAMEN TECHNOLOGY CO LTD (SHUO-Non-standard)
GA 201902983M
AB    NOVELTY - The method involves obtaining produced audio data of a to-be-monitored device in predetermined duration. The produced audio data is recognized and processed through a pre-trained convolutional neural network. An output of the to-be-monitored device is determined within the predetermined duration. Starting processes and ending processes are identified in the predetermined duration. Starting time of production in first predetermined time interval is determined when continuous recognition is determined as a process starting point. Ending time of the production in the first predetermined time interval is determined when the continuous recognition is determined as a process ending point.
   USE - Method for monitoring yield production by using a computer device (claimed).
   ADVANTAGE - The method enables realizing monitoring of production in different characteristics of different structures and better universality, and avoiding changing an original structure of the device to adapt for compact and tight packaging, and providing recording device for monitoring multiple components simultaneously to achieve one-to-many monitoring, and saving cost, and reducing influence of a line, and improving expansibility and changing software without hardware adjustment when monitoring requirements are obtained.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a yield production monitoring device
   (2) a computer readable storage medium for storing a set of instructions to monitor yield production.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for monitoring a yield by using a computer device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J18; T01-N01A2; T01-N02B2
IP G06Q-010/06; G06N-003/04
PD CN109102143-A   28 Dec 2018   G06Q-010/06   201915   Pages: 14   Chinese
AD CN109102143-A    CN10630453    19 Jun 2018
PI CN10630453    19 Jun 2018
UT DIIDW:201902983M
ER

PT P
PN CN109102462-A
TI Deep learning based video super-resolution reconstruction method, involves performing lens segmentation operation on low-resolution video and trained network model, and determining output of neural network model as ultra-resolution video.
AU ZHANG D
   ZHANG X
   NI P
AE UNIV CHINA JILIANG (UYJA-C)
GA 201902976K
AB    NOVELTY - The method involves collecting two sets of public video pairs with same content, where the first set is low resolution video and second set is high definition video. The High-definition video is captured by different mobile phones and different camera. The high-definition video is converted into continuous images by utilizing frame of the high-definition video and stored processed video image data as hierarchical data format (HDF5) file. Number of nodes in an input layer of the deep neural network is selected. Lens segmentation operation is performed on low-resolution video and trained a neural network model. Output of the neural network model is determined as reconstructed ultra-resolution video.
   USE - Deep learning based video super-resolution reconstruction method.
   ADVANTAGE - The method enables removing camera jitter, object motion blur, defocus blur, lens optical blur, reducing compression distortion, noise and degradation factors.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based video super-resolution reconstruction method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W01 (Telephone and Data Transmission Systems)
MC T01-J10B3A; T01-J10D; T01-N01B3; T01-N01D1B; T01-N01D2; W01-A06A3
IP G06T-003/40; G06N-003/04; G06N-003/08
PD CN109102462-A   28 Dec 2018   G06T-003/40   201915   Pages: 9   Chinese
AD CN109102462-A    CN10864938    01 Aug 2018
PI CN10864938    01 Aug 2018
UT DIIDW:201902976K
ER

PT P
PN CN109101915-A
TI Deep learning based pedestrian face and attribute identification network structure design method, involves forming pedestrian identity characteristic layer according to target fusion feature, and classifying pedestrian attributes.
AU ZHANG D
   CHEN S
AE UNIV CHINA JILIANG (UYJA-C)
GA 2019029896
AB    NOVELTY - The method involves performing face key point detection on a human face image sequence to obtain a corresponding personal face key point. A face key point track is determined by changing a key point position. Feature fusion process is performed to primary fusion feature and secondary fusion features to obtain target fusion features, where dimension of the primary fusion feature is about specific range, and dimension of the secondary fusion features is about specific range. A pedestrian identity characteristic layer is formed according to the obtained target fusion feature. Pedestrian attributes are classified.
   USE - Deep learning based pedestrian face and attribute identification network structure design method.
   ADVANTAGE - The method enables realizing attribute identification and classification process in an effective manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based pedestrian face and attribute identification network structure design method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J05B2; T01-J10B2; T01-N01B3; T04-D04; T04-D07D5; T04-D07F1; W04-W05A
IP G06K-009/00; G06K-009/62; G06T-007/246
PD CN109101915-A   28 Dec 2018   G06K-009/00   201915   Pages: 14   Chinese
AD CN109101915-A    CN10864964    01 Aug 2018
PI CN10864964    01 Aug 2018
UT DIIDW:2019029896
ER

PT P
PN CN109100371-A
TI Laser total-reflection 3C transparent component defect detecting device, has laser emitter and locating drive device that are connected together, and detecting camera for shooting and collecting image of light working machine.
AU ZHANG G
   MING W
   ZHANG H
   LU Y
   YIN L
   ZHANG Z
   GENG T
   SHEN F
AE GUANGDONG HUST IND TECHNOLOGY RES INST (GUAN-Non-standard)
GA 2019030261
AB    NOVELTY - The device has a laser emitter and a locating drive device that are connected together. A detecting camera, a depth learning calculation unit and an acousto-optic alarm device are connected to an ARM (RTM: 32 bit reduced instruction set computer processor) embedded controller through a CAN bus. The detecting camera shoots and collects an image of a light working machine. A total reflection transmission workbench is provided with a total reflection transparent component. The total reflection transparent component is located between a total reflection upper auxiliary piece and a total reflection lower auxiliary piece.
   USE - Laser total-reflection 3C transparent component defect detecting device.
   ADVANTAGE - The device has stable performance and high defect detecting quality and efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a laser total-reflection 3C transparent component defect detecting device. '(Drawing includes non-English language text)'
DC S03 (Scientific Instrumentation); T01 (Digital Computers)
MC S03-E04F; S03-E04X; T01-J08A
IP G01N-021/958; G01N-021/01
PD CN109100371-A   28 Dec 2018   G01N-021/958   201915   Pages: 16   Chinese
AD CN109100371-A    CN11251366    25 Oct 2018
PI CN11251366    25 Oct 2018
UT DIIDW:2019030261
ER

PT P
PN CN109102005-A
TI Depth learning based shallow model knowledge sample data transmitting method, involves extracting integral characteristic, and determining classification/prediction error accuracy and network complexity of final model.
AU ZHANG J
   TIAN J
   ZHANG D
   YANG M
   XU X
   LIU W
   WEN C
AE UNIV HANGZHOU DIANZI (UYHH-C)
GA 2019029872
AB    NOVELTY - The method involves pre-processing data for filtering an original one-dimensional time domain waveform signal to remove high-frequency noise. One-dimensional time domain waveform signal is processed. Maximum amplitude and average characteristic are determined. Frequency domain transforming is performed. Computing base/frequency vector and power spectrum feature are determined. Determination is made to check whether frequency spectrum diagram is converted to time-frequency domain by a two-dimensional structure. Integral characteristic is extracted. Classification/prediction error accuracy and network complexity of a final model are determined.
   USE - Depth learning based shallow model knowledge sample data transmitting method.
   ADVANTAGE - The method enables improving validity verifying efficiency by determining classification/prediction accuracy error indexes.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a depth learning based shallow model knowledge sample data transmitting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J04C; T01-J05B2; T01-J10B2; T01-N01B3; T01-N01D; T04-D03A; T04-D04; T04-D07D3; W04-W05A
IP G06K-009/62
PD CN109102005-A   28 Dec 2018   G06K-009/62   201915   Pages: 11   Chinese
AD CN109102005-A    CN10812756    23 Jul 2018
PI CN10812756    23 Jul 2018
UT DIIDW:2019029872
ER

PT P
PN CN109101926-A
TI Convolutional neural network-based air target detecting method, involves classifying optimized feature by using SoftMax algorithm to obtain primary detecting model, and performing training model final testing to obtain test result.
AU ZHANG Q
   WAN C
   BIAN S
AE UNIV HENAN TECHNOLOGY (UYHY-C)
   ZHENGZHOU AIYI ELECTRONICS TECHNOLOGY CO (ZHEN-Non-standard)
GA 201902988W
AB    NOVELTY - The method involves obtaining air target image data, where air target image data comprises training set and test set. Learning frame building depth is measured. Configuration of a model is determined by using conventional SSD model. Feature extraction is performed by using a VGG-16-type network. Deconvolution structure cascade connection is performed by the conventional SSD model. A position of loss function is determined between a predicted frame and an actual frame for carrying-out weighted summation. Total loss function is determined. Optimized feature is classified by using SoftMax algorithm to obtain a primary detecting model. A final detection model is obtained. Training model final testing is performed to obtain a test result.
   USE - Convolutional neural network-based air target detecting method.
   ADVANTAGE - The method enables improving detecting accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network-based air target detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-N01B3A
IP G06K-009/00; G06N-003/04; G06K-009/62
PD CN109101926-A   28 Dec 2018   G06K-009/00   201915   Pages: 12   Chinese
AD CN109101926-A    CN10924226    14 Aug 2018
PI CN10924226    14 Aug 2018
UT DIIDW:201902988W
ER

PT P
PN CN109101958-A
TI Deep learning based human face detecting system, has data input module, face detection network and result output module orderly connected with each other, and model training module for training data in data set and face detection network.
AU ZHANG Y
   LI B
AE ZHONGXIANG BOQIAN INFORMATION TECHNOLOGY (ZHON-Non-standard)
GA 2019029889
AB    NOVELTY - The system has a data input module, a face detection network and a result output module orderly connected with each other. A model training module is connected with the face detection network. The data input module inputs detection data in the face detection network. The face detection network performs face detection operation according to the detection data. The result output module displays the detection result of the face detection network. The model training module trains data in a data set and the face detection network, where the data input module is compiled by Python (RTM: High-level programming language) program to retrieve a hard disk file in MPEG-1 Audio Layer 4 (MP4) format.
   USE - Deep learning based human face detecting system.
   ADVANTAGE - The system performs training operation in an automatic manner and increases face detecting precision and accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a deep learning based human face detecting system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-H01B4; T01-J10B2; T01-N01B3; T01-N01D2; T04-D04; T04-D07F1; W04-W05A; W04-X03E1
IP G06K-009/00; G06K-009/62
PD CN109101958-A   28 Dec 2018   G06K-009/00   201915   Pages: 7   Chinese
AD CN109101958-A    CN11299422    01 Nov 2018
PI CN11299422    01 Nov 2018
UT DIIDW:2019029889
ER

PT P
PN CN109102014-A
TI Depth convolutional neural network based class unbalanced image classifying method, involves training image classification network by using small sample image training set, and classifying input image by optimal image classification network.
AU ZHENG H
   LIU J
   DU A
   YU Z
AE UNIV CHINA OCEAN (UYOC-C)
GA 201902986S
AB    NOVELTY - The method involves selecting a sample image of a small category (S1) in an original data set. Number of sample images of small category is increased to obtain a training set of the original small category sample image. The original small category sample image training set is input (S2) to generate a generated image sample generated in the confrontation network. The generated image sample is added (S3) to the original small-category sample image training set to obtain the generated small-category sample. An image classification network is trained by using the original small class sample image training set. The image classification network is trained (S4) by using the generated small-category sample image training set. The input image is classified (S5) by an optimal image classification network.
   USE - Depth convolutional neural network based class unbalanced image classifying method.
   ADVANTAGE - The method enables utilizing the single unbalanced data set to realize better classification effect for small sample category, which improves the accuracy of image classification data set type under unbalanced condition.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a depth convolutional neural network based class unbalanced image classifying method. '(Drawing includes non-English language text)'
   Step for selecting a sample image of a small category (S1)
   Step for inputting original small category sample image training set (S2)
   Step for adding generated image sample (S3)
   Step for training image classification network (S4)
   Step for classifying input image (S5)
DC T01 (Digital Computers)
MC T01-J05B2; T01-J10B1; T01-J10B2; T01-J10C; T01-N01B3
IP G06K-009/62; G06N-003/04
PD CN109102014-A   28 Dec 2018   G06K-009/62   201915   Pages: 17   Chinese
AD CN109102014-A    CN10866743    01 Aug 2018
PI CN10866743    01 Aug 2018
UT DIIDW:201902986S
ER

PT P
PN CN109101878-A
TI Straw fuel value estimated image analyzing method, involves setting parameters of camera, dividing straw image by semantic segmentation network, calculating proportion, and calculating combustion value of straw fuel by proportion.
AU ZHENG Y
   LIANG S
   CHEN C
   HE D
   LI L
AE UNIV ZHEJIANG TECHNOLOGY (UYZT-C)
GA 2019029905
AB    NOVELTY - The method involves setting parameters of a camera. A straw fuel is captured by utilizing the camera. An image is collected and uploaded to a server. An image of the current frame is deleted when similarity is greater than or equal to a threshold value. An image in a cloud server is marked by utilizing a marking platform to obtain a data set. The data set is expanded by a data-enhanced process. A semantic segmentation network is trained by the data set. A straw image is divided by a semantic segmentation network. A proportion is calculated. A combustion value of the straw fuel is calculated by the proportion.
   USE - Straw fuel value estimated image analyzing method.
   ADVANTAGE - The method enables collecting and training images, analyzing and reporting the result to the power generation system through the wireless network, deleting unnecessary image data, uploading effective image data, reducing unnecessary costs and obtaining the proportion of straw accurately by the deep learning process.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a straw fuel value estimated image analyzing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a straw fuel value estimated image analyzing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J04A; T01-J10B2; T01-J10B3A; T01-J16C3; T01-N01D1B; T01-N01D3; T04-D02; T04-D04; W04-E20G
IP G06K-009/00; G06K-009/20; G06K-009/62
PD CN109101878-A   28 Dec 2018   G06K-009/00   201915   Pages: 15   Chinese
AD CN109101878-A    CN10704367    01 Jul 2018
PI CN10704367    01 Jul 2018
UT DIIDW:2019029905
ER

PT P
PN CN109102483-A
TI Method for training image enhancement model by using electronic device, involves training Unet convolutional neural network based on output image, target image and preset loss function scheme to establish trained image enhancement model.
AU ZHOU M
   LI Z
   ZHANG W
   LI Q
   LV Y
AE XIAMEN MEITUZHIJIA TECHNOLOGY CO LTD (XIAM-Non-standard)
GA 2019029760
AB    NOVELTY - The method involves inputting an initial image into a Unet convolutional neural network. High-level abstraction local feature of the initial image is extracted from the Unet convolutional neural network. High-level abstraction local feature reduction process is performed to obtain high-level abstraction global feature of the initial image. Deconvolution process is performed on high-level abstraction integrated feature in the Unet convolutional neural network to obtain an output image. The Unet convolutional neural network is trained based on the output image, a target image and preset loss function scheme to establish a trained image enhancement model.
   USE - Method for training an image enhancement model by using an electronic device (claimed).
   ADVANTAGE - The method enables combining the high-level abstraction local feature and the high-level abstraction global feature of the image to combine multiple information of the image for learning, and outputting the image as a target image by the image enhancement model in a better manner. The method enables improving reinforced effect of the image output by the image enhancement model.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an image enhancement model training device
   (2) a readable storage medium for storing a set of instructions for training an image enhancement model by using an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for training an image enhancement model by using an electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B1; T01-N01B3
IP G06T-005/50
PD CN109102483-A   28 Dec 2018   G06T-005/50   201915   Pages: 14   Chinese
AD CN109102483-A    CN10821282    24 Jul 2018
PI CN10821282    24 Jul 2018
UT DIIDW:2019029760
ER

PT P
PN CN109102463-A
TI Method for reconstructing ultra-resolution image by using electronic device, involves inputting three pixel matrix to second layer depth learning model, and performing integration process on pixel matrix to obtain ultra-resolution image.
AU ZHU Z
   DONG Y
   BAI H
   XIONG F
AE BEIJING FACEALL TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 201902976J
AB    NOVELTY - The method involves setting size of an image. Gradation process is performed on the size of the image. A to-be-detected first size image is obtained, where the first size image is input to a first layer depth learning model to output a three pixel matrix with first size. Three pixel matrix is input to a second layer depth learning model to output the pixel matrix of a second size, where the second size is larger than the first size. Integration process is performed on the three pixel matrix to obtain ultra-resolution image.
   USE - Method for reconstructing an ultra-resolution image by using an electronic device (claimed).
   ADVANTAGE - The method enables improving low-resolution image and processing speed and increasing reconstruction degree.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a computer readable storage medium for storing set of instructions for reconstructing an ultra-resolution image by using an electronic device
   (2) a device for reconstructing an ultra-resolution image by using an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for reconstructing an ultra-resolution image by using an electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J10B1; T01-J10B3A; T01-J30A; W04-W05A
IP G06T-003/40
PD CN109102463-A   28 Dec 2018   G06T-003/40   201915   Pages: 10   Chinese
AD CN109102463-A    CN10918370    13 Aug 2018
PI CN10918370    13 Aug 2018
UT DIIDW:201902976J
ER

PT P
PN CN109084778-A
TI Binocular vision navigation method, involves selecting initial value vector, calculating element increment value, and calculating position coordinate vector is calculated to obtain current running position in world coordinate system.
AU SUN H
   ZHOU L
   FENG J
AE DALIAN VADE INTELLIGENT VISION TECHNOLOGY INNOVATION CENT CO (DALI-Non-standard)
GA 201902384T
AB    NOVELTY - The method involves extracting a frame image block characteristic with corner point features. An initial value vector is selected. Residual error is calculated. An element increment value is calculated. A position coordinate vector is calculated to obtain a current running position in a world coordinate system. Edge feature is determined by utilizing texture and edge information by adopting deep learning algorithm. Judgment is made to check whether straight line edges are detected by Hough transformation. Camera road surface information is detected to determine a road side part by a camera.
   USE - Binocular vision navigation method.
   ADVANTAGE - The method enables improving anti-interference ability, reducing environment influence for timely judging field environment so as to ensure accurate location, thus establishing precise planning route, and realizing high-risk environment inspection work process.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a binocular vision navigation system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a side view of a binocular vision navigation system. '(Drawing includes non-English language text)'
DC S02 (Engineering Instrumentation); T01 (Digital Computers)
MC S02-B08; T01-E03; T01-J04A; T01-J04C; T01-J16C2; T01-J21A; T01-J21B; T01-J30A
IP G01C-021/20; G01C-021/34
PD CN109084778-A   25 Dec 2018   G01C-021/20   201915   Pages: 8   Chinese
AD CN109084778-A    CN11097502    19 Sep 2018
PI CN11097502    19 Sep 2018
UT DIIDW:201902384T
ER

PT P
PN CN109086799-A
TI Method for identifying crop leaf disease, involves obtaining certain number of different types of plant disease leaves and processing, followed by constructing improved convolutional neural network model training test data.
AU SUN J
   WANG L
   LU B
   TAN W
   WU X
   SHEN J
   DAI C
AE UNIV JIANGSU (UYJS-C)
GA 201901470U
AB    NOVELTY - A crop leaf disease identifying method involves obtaining a certain number of different types of plant disease leaves as image crop test data and processing to obtain test data of training set and testing set, constructing an improved convolutional neural network model training test data and selecting an AlexNet classification model as a basic model and training the convolutional neural network and testing the results to complete the crop leaf disease identification.
   USE - Method for identifying crop leaf disease.
   ADVANTAGE - The method enables identifying crop leaf disease with increased identification effect, in accurate manner.
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-E01B; T01-J05B2; T01-J10B2; T01-N01B3A; T04-D04
IP G06K-009/62; G06N-003/04
PD CN109086799-A   25 Dec 2018   G06K-009/62   201915   Pages: 10   Chinese
AD CN109086799-A    CN10723612    04 Jul 2018
PI CN10723612    04 Jul 2018
UT DIIDW:201901470U
ER

PT P
PN CN109031654-A
TI Convolutional neural network based adaptive optical correcting method, involves loading drive signal into each wave-front corrector such that wavefront corrector generates deformation quantity of To-be-corrected distortion wavefront.
AU MA H
   ZHANG W
AE UNIV ANHUI AGRIC (UYAH-C)
GA 2018A4911C
AB    NOVELTY - The method involves training convolutional neural network training based on distortion far field light intensity image and a wave-front corrector driving signal based on a convolutional neural network model. To-be-corrected distortion wavefront is divided into two paths by a wave-front corrector and an optical path splitting after the convolutional neural network model is constructed. A drive signal of wavefront corrector distortion is obtained by the convolutional neural network model. The drive signal is loaded into an each wave-front corrector such that the wavefront corrector generates deformation quantity of the to-be-corrected distortion wavefront.
   USE - Convolutional neural network based adaptive optical correcting method.
   ADVANTAGE - The method enables obtaining the wave-front corrector driving signal according to an input light intensity by using a convolution neural network model, generating deformation quantity of the to-be-corrected distortion wavefront to correct incident wavefront aberration without need iteration optimization, which realizes simple structure and easy to realize, low cost and high system bandwidth.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a convolutional neural network based adaptive optical correcting system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a front view of a convolutional neural network based adaptive optical correcting system.
DC P81 (Optics (G02).); T01 (Digital Computers)
MC T01-J04B2; T01-J10B1; T01-L02; T01-N01B3
IP G02B-026/06; G02B-027/00; G06N-003/04; G06N-003/08
PD CN109031654-A   18 Dec 2018   G02B-026/06   201915   Pages: 12   Chinese
AD CN109031654-A    CN11057241    11 Sep 2018
PI CN11057241    11 Sep 2018
UT DIIDW:2018A4911C
ER

PT P
PN CN109034045-A
TI Method for identifying leukocyte based convolutional neural network, involves performing integrated training process on integrated learning classifier, and obtaining FCL layer weight parameter of WBC-Net convolutional neural network.
AU TAN G
   ZHANG L
   WAN H
AE UNIV CENT SOUTH (UYCS-C)
GA 2018A48530
AB    NOVELTY - The method involves establishing a VGG-Net convolutional neural network by using an Image Net data group. A network structure model and training parameter of a layer are determined. Initial weight of a fine convolutional neural network is obtained. Cross entropy loss function is realized to calculate a classification error of a forward propagation. Parameter values of the layer of a WBC-Net convolutional neural network is obtained. Feedback transmission process is performed. Network structure parameter is obtained. A group of an integrated learning classifier is trained. Integrated training process is performed on the integrated learning classifier. FCL layer weight parameter of the WBC-Net convolutional neural network is obtained.
   USE - Method for identifying leukocyte based convolutional neural network.
   ADVANTAGE - The method enables obtaining cell data, accessing standard data group, -performing re-use transfer learning process, establishing model parameter of VGG-Net transfer to the WBC-Net convolution nerve network, extracting best effect of the feature layer to obtain feature parameter of the trained integrated classifier, and realizing identification function of the leukocyte.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for identifying leukocyte based convolutional neural network. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-E01A; T01-J04B2; T01-J05B2B; T01-J10B2; T01-N01B3; T01-N01D1B
IP G06K-009/00; G06K-009/62; G06N-003/04
PD CN109034045-A   18 Dec 2018   G06K-009/00   201915   Pages: 11   Chinese
AD CN109034045-A    CN10802059    20 Jul 2018
PI CN10802059    20 Jul 2018
UT DIIDW:2018A48530
ER

PT P
PN CN109002752-A
TI Depth learning based rapid complex common scene pedestrian detecting method, involves retaining remaining confidence value of maximum predicted frame, and selecting prediction data as space coordinates to detect target pedestrian confidence.
AU ZHANG F
AE BEIJING B & G VIEW TECHNOLOGY DEV CO LTD (BEIJ-Non-standard)
GA 2018A3450A
AB    NOVELTY - The method involves reading a specific scene in a picture database by dual-linear interpolation algorithm for stretching or compressing pixel size. A weight value of a legacy training network is obtained for changing an end structure of a convolutional neural network. A pedestrian task is detected by a scene database for adjusting and training the convolutional neural network. A pedestrian video common scenario is read for decomposing a video into a single frame. The pixel size is stretched or compressed by bilinear interpolation algorithm. Target detection of a pedestrian in a picture is performed by the trained network. A confidence threshold value is set for predicting the convolutional neural network by filtering. A remaining confidence value of a maximum predicted frame is retained. Retention prediction data is selected as space position coordinates to detect pedestrian confidence of a predicted target.
   USE - Depth learning based rapid complex common scene pedestrian detecting method.
   ADVANTAGE - The method enables inputting an image as an input of the convolutional neural network during testing phase by threshold filtering and non-maximum inhibition of the convolutional neural network for filtering an output prediction result, and detecting position information of pedestrians so as to realize intelligent monitoring of the pedestrian.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a depth learning based rapid complex common scene pedestrian detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-E01B; T01-J05B4F; T01-J10B2; T01-J10B3A; T01-N01B3A; T01-N02B2; T04-D03; T04-D04
IP G06K-009/00; G06K-009/62; G06T-003/40
PD CN109002752-A   14 Dec 2018   G06K-009/00   201915   Pages: 9   Chinese
AD CN109002752-A    CN10021283    08 Jan 2018
PI CN10021283    08 Jan 2018
CP CN109002752-A
      CN106022237-A   UNIV ELECTRONIC SCI & TECHNOLOGY (UYEL-Non-standard)   LI H, FAN J, ZHOU H, HU H, CAO B
      CN106127815-A   UNIV GUANGDONG TECHNOLOGY (UGTE)   LIN L, LIU B, XIAO Y
      CN106228575-A   UNIV GUANGDONG TECHNOLOGY (UGTE)   LIN L, LIU B, XIAO Y
      CN106650721-A   WU X (WUXX-Individual)   WU X, ZHANG R
      CN106650919-A   GUIZHOU POWER GRID CORP INFORMATION (CSPG);  STATE GRID CORP CHINA (SGCC)   YAN L, LIU J, HU W, ZHANG S, JIN X, LI J, GAO D, LIU Y, CUI S, LIU D
      CN106845549-A   ZHUHAI XIYUE INFORMATION TECHNOLOGY CO (ZHUH-Non-standard)   MA J, WANG Z, ZHOU W
      CN107203740-A   UNIV HUAQIAO (UYHQ)   DU J, ZHENG D, FAN W, ZHANG H, ZHAI C
UT DIIDW:2018A3450A
ER

PT P
PN CN108985353-A
TI Image based air pollution degree classifying method, involves determining probability of classification as air pollution state, and returning air pollution degree classification result for displaying current air pollution degree level.
AU LI K
   MA J
   YANG J
   HAN Y
AE UNIV TIANJIN (UTIJ-C)
GA 2018A19643
AB    NOVELTY - The method involves establishing a training database and a training model. A deep feature is extracted based on a deep learning network model. A test image of an overall sky or half sky is collected by a user through a photographic equipment i.e. mobile phone. The test image is uploaded to a server. Deep feature extraction process is performed on the image collected by the user according to a mixture model. Maximum probability of classification is determined as a current air pollution state according to Softmax function. An air pollution degree classification result is returned to a user client side to display a current air pollution degree level.
   USE - Image based air pollution degree classifying method.
   ADVANTAGE - The method enables estimating air pollution degree based on the shooting image in convenient and accurate manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an image based air pollution degree classifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B2; T01-J05B4P; T01-J10B2; T01-N01B3A; T01-N01D1B; T01-N01D3
IP G06K-009/62
PD CN108985353-A   11 Dec 2018   G06K-009/62   201915   Pages: 9   Chinese
AD CN108985353-A    CN10680623    27 Jun 2018
PI CN10680623    27 Jun 2018
UT DIIDW:2018A19643
ER

PT P
PN CN108985372-A
TI Gelule theory and PLSA routing based medical image classification method, involves performing gelule and classified forecast process, and constructing image reconstructed full connection network to output generated medical image.
AU LIU S
   JIA X
   HONG J
   LIN Z
   MA Z
   QIU Y
   GUAN L
   LIAO X
   GAO W
AE UNIV GUANGDONG POLYTECHNIC NORMAL TIANHE (UGTH-C)
GA 2018A1963J
AB    NOVELTY - The method involves inputting an original medical image. A gelule and classified forecast process is performed based on a convolutional neural network. Network routing is established between gelule layers by using a probabilistic latent semantic analyzes model to transfer PLSA information. An image reconstructed full connection network is constructed to output a generated medical image. Network parameters are updated by using a backward propagation technique. Hidden variable posterior probability is determined by maximizing a log-likelihood expected function.
   USE - Gelule theory and PLSA routing based medical image classification method.
   ADVANTAGE - The method enables introducing a PLSA probability model to design a gelule routing algorithm and routing consistency, adopting reasonable consistency to process a medical image classification task, automatically learning the medical image, finding size between features based on position and direction information to improve classification precision, recovering the medical image to realize image reconstruction and improving convolutional neural network generalization ability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a gelule theory and PLSA routing based medical image classification method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D; T01-J05B2; T01-J10B1; T01-J10B2; T01-J16C3; T01-N01D1B; T01-N01E1
IP G06K-009/62; G06K-009/34
PD CN108985372-A   11 Dec 2018   G06K-009/62   201915   Pages: 10   Chinese
AD CN108985372-A    CN10758184    11 Jul 2018
PI CN10758184    11 Jul 2018
UT DIIDW:2018A1963J
ER

PT P
PN CN108969980-A
TI Method for counting treadmill steps, involves counting weighted summation result of first and second identification parameter of running motion as effective running motion, when motion is within predetermined range.
AU WANG M
AE GUANGZHOU SHIYUAN ELECTRONICS TECHNOLOGY (GUAZ-C)
   GUANGZHOU SHIRUI ELECTRONIC TECHNOLOGY (GUAZ-C)
GA 2018A30012
AB    NOVELTY - The method involves obtaining (101) the image information collected by the camera and piezoelectric curve detected by the pressure sensor. The image information comprises a user leg action of continuous images. The image information is sequentially input (102) to the trained first convolutional neural network model according to the preset frame number packet, and first identification parameter of the running action is obtained corresponding to the set of image information. The running action of the second identification parameter in the piezoelectric curve is calculated (103). The running motion in the piezoelectric curve of the running motion and the image information are one-to-one correspondence based on the time. The number of steps is counted in the running motion. The weighted summation result of the first identification parameter and the second identification parameter of running motion is counted (104) as effective running motion when motion is within the predetermined range.
   USE - Method for counting steps in treadmill (claimed).
   ADVANTAGE - The neural network model extracts the time and spatial characteristics of the user running movement. The running movements of the user are recognized accurately. The role of the piezoelectric curve in counting the number of running is considered, so that the final running step results are more accurate.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a treadmill; and
   (2) a storage medium storing instructions for performing a method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the method for counting running step number. (Drawing includes non-English language text)
   Step for obtaining the image information collected by the camera and piezoelectric curve detected by the pressure sensor, and the image information comprises a user leg action of continuous images (101)
   Step for inputting image information sequentially to the trained first convolutional neural network model according to the preset frame number packet, and obtaining first identification parameter of the running action corresponding to the set of image information (102)
   Step for calculating running action of the second identification parameter in the piezoelectric curve (103)
   Step for counting weighted summation result of the first identification parameter and the second identification parameter of running motion as effective running motion, when motion is within the predetermined range (104)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J10B2A; T01-S03; W04-X01A5C; W04-X01C1
IP A63B-022/02; A63B-024/00; A63B-071/06; G06K-009/00
PD CN108969980-A   11 Dec 2018   A63B-022/02   201915   Pages: 15   Chinese
AD CN108969980-A    CN10689635    28 Jun 2018
PI CN10689635    28 Jun 2018
UT DIIDW:2018A30012
ER

PT P
PN CN108985453-A
TI Asymmetric ternary weighting based deep neural network model compressing method, involves performing parameter updating process based on floating-point-type network weight, and performing compression storage process on deep neural network.
AU WU J
   DING J
   WU H
AE SUZHOU INST ADVANCED STUDY USTC (UCST-C)
GA 2018A1961Q
AB    NOVELTY - The method involves weighting floating point weight of a trained deep neural network into asymmetric ternary value before forward calculation process during deep neural network training process. Parameter updating process is performed based on original floating-point-type network weight. Compression storage process is performed on a trained deep neural network based on two-bit encoding process. Sixteen ternary values are stored as a one-bit fixed-point integer by performing shift operation. Approximate calculation process of the floating point weight of the trained deep neural network is performed.
   USE - Asymmetric ternary weighting based deep neural network model compressing method.
   ADVANTAGE - The method enables removing redundant parameters of the trained deep neural network and compressing a trained deep neural network model so as to effectively improve recognition accuracy of quantization process on a big data set.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a symmetric ternary weighting based deep neural network model compression device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a deep neural network model. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-D01; T01-D02; T01-E02B; T01-N01B3
IP G06N-003/08; H03M-007/30
PD CN108985453-A   11 Dec 2018   G06N-003/08   201915   Pages: 13   Chinese
AD CN108985453-A    CN10674698    27 Jun 2018
PI CN10674698    27 Jun 2018
UT DIIDW:2018A1961Q
ER

PT P
PN CN108985454-A
TI Civil airliner individual target identification method, involves testing correct rate of neural network, and identifying characteristic of aircraft in communication signal by signal pulse to obtain probability of identifying target.
AU XU X
   ZHANG X
   WANG C
   HE W
   LI S
AE SOUTHWEST ELECTRONIC TECHNOLOGY INST CHI (CETC-C)
GA 2018A1961P
AB    NOVELTY - The method involves converting identifying characteristics of ADS-B communication signal into an image space structure characteristic. A deep convolutional neural network model is established according to a target number for constructing a neural network module. A pulse image is divided between different targets to evaluate depth convolution. The pulse image is utilizes as an input for training the neural network module based on the convolutional neural network algorithm. Image samples are generated from ADS-B communication signal data for training and parameter optimization. A classification correct rate of the deep convolutional neural network is tested. A characteristic of an aircraft in the communication signal is directly identified by a signal pulse to obtain probability of identifying an individual target.
   USE - Civil airliner individual target identification method.
   ADVANTAGE - The method enables realizing high recognition rate.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a civil airliner individual target identification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J05B2; T01-J07D1; T01-J10B2A; T01-N01B3A; T04-D04
IP G06N-003/08; G06N-003/063; G06N-003/04; G06K-009/00
PD CN108985454-A   11 Dec 2018   G06N-003/08   201915   Pages: 12   Chinese
AD CN108985454-A    CN10683112    28 Jun 2018
PI CN10683112    28 Jun 2018
UT DIIDW:2018A1961P
ER

PT P
PN CN108985443-A
TI Method for generating neural network for action identification by utilizing electronic device, involves generating deformed convolutional neural network according to kernel offset information based on initial convolutional neural network.
AU ZHANG C
   WU Q
AE MEGVII TECHNOLOGY LTD (MEGV-Non-standard)
GA 2018A1961Y
AB    NOVELTY - The method involves extracting a target image to obtain an optical flow characteristic. Convolution kernel offset information is obtained according to the optical flow characteristic. A deformed convolutional neural network is generated according to the convolution kernel offset information based on an initial convolutional neural network. Light stream information is obtained. A flow characteristic is obtained to extract the light stream information. A characteristic vector is generated according to the flow characteristic. A dimension of convolution kernel is obtained as the convolution kernel offset information.
   USE - Method for generating neural network for action identification by utilizing an electronic device (claimed).
   ADVANTAGE - The method enables reducing problem of poor existing image recognition neural network for action identification in the prior art.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for generating neural network for action identification by utilizing an electronic device
   (2) an action identification method
   (3) an action identification device
   (4) a processor-executable non-volatile computer readable medium comprises a set of instructions for performing a method for generating neural network for action identification by utilizing an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for generating neural network for action identification by utilizing an electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B2A; T01-J10B3A; T01-S03; T04-D02; T04-D04
IP G06N-003/04; G06K-009/00
PD CN108985443-A   11 Dec 2018   G06N-003/04   201915   Pages: 18   Chinese
AD CN108985443-A    CN10728821    04 Jul 2018
PI CN10728821    04 Jul 2018
UT DIIDW:2018A1961Y
ER

PT P
PN CN108965920-A
TI Video content item dividing method, involves outputting result of to-be-trained video sub-model to training sample, determining calibration output result of training sample to be training target, and training convolutional neural network.
AU CHEN C
   YANG X
   TIAN D
AE BEIJING EVOMEDIA TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 2018A0117G
AB    NOVELTY - The method involves inputting a video into a pre-training video content item model to obtain a data entry for representing the video. Different categories of a video content in the video are identified by using different video sub-models to obtain a recognition result. The data entry is generated based on the recognition result of the video by the video sub-models. The video content item model is contained in a to-be-trained video sub-model in a convolutional neural network. A result of the to-be-trained video sub-model is output to a training sample. A Calibration output result of the training sample is determined to be a training target. The convolutional neural network is trained.
   USE - Video content item dividing method.
   ADVANTAGE - The method enables automatically obtaining a data item for representing the video, realizing automatic classifying of the video content, so as to reduce artificial resource waste caused by manual video content item, and satisfying high standard requirements for quickly issuing a media audiovisual program.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a video content item dividing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a video content item dividing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W03 (TV and Broadcast Radio Receivers)
MC T01-J10B2A; T01-J10B3A; T01-N01B3; T01-N01D1B; W03-A16C5A; W03-A16C5K
IP H04N-021/234; H04N-021/44; H04N-021/235; H04N-021/435
PD CN108965920-A   07 Dec 2018   H04N-021/234   201915   Pages: 16   Chinese
AD CN108965920-A    CN10896708    08 Aug 2018
PI CN10896708    08 Aug 2018
UT DIIDW:2018A0117G
ER

PT P
PN CN208098642-U
TI High-speed X-ray express sorting system, has background server connected with document camera, and removing structure connected with automatic sorting line for receiving instruction of sorting line to express potential safety hazard.
AU NI H
AE NI H (NIHH-Individual)
GA 201894678E
AB    NOVELTY - The utility model claims a high-speed X-ray express sorting system, comprising an automatic sorting line for transmitting an express parcel, a high-spot meter installed on the automatic sorting line, and used for obtaining barcode identification information included in the express, and outputting the barcode identification information to a background server, an X-ray machine installed on the automatic sorting line, and used for collecting X-ray picture of the express parcel, and outputting the obtained X-ray picture to the background server. The utility model has the beneficial effects of: combining a leaping X-ray machine and a high-speed sorting machine to improve the speed of acquiring the X-ray picture and identify the picture; and the dangerous goods in the X-ray film are quickly detected and judged through the artificial intelligence and the deep learning method; the problematic items are screened and removed by an automatic sorting line system to ensure the accuracy and efficiency of the inspection, reduce human interference and reduce costs.
DC T01 (Digital Computers); T05 (Counting, Checking, Vending, ATM and POS Systems); X25 (Industrial Electric Equipment)
MC T01-C06; T01-N01B3; T01-N01D1B; T01-N01D2; T01-N01D3; T05-K05; X25-F06
IP B07C-005/34; B07C-005/02; B07C-005/36
PD CN208098642-U   16 Nov 2018   B07C-005/34   201915   Pages: 6   Chinese
AD CN208098642-U    CN20062531    15 Jan 2018
PI CN20062531    15 Jan 2018
UT DIIDW:201894678E
ER

PT P
PN CN107979554-A
TI Radio signal modulation identification method for multi-scale convolutional neural network, involves comparing recognition result with real category of test set and statistically identifying correct rate.
AU YANG S
   JIAO L
   HUANG Z
   WU Y
   WANG Z
   LI Z
   ZHANG B
   SONG Y
   WANG H
AE UNIV XIDIAN (UYXN-C)
GA 201835022A
AB    NOVELTY - The method involves training a multi-scale convolutional neural network model. The order of all samples in the training sample set is disrupted. The neural network training process is completed, when the number of iterations set by the multi-scale network is reached. The trained neural network model is obtained. The recognition accuracy rate is obtained. The test sample set is inputted into a trained neural network model to obtain the recognition result. The recognition result is compared with the real category of the test set and the correct rate is statistically identified.
   USE - Radio signal modulation identification method for multi-scale convolutional neural network.
   ADVANTAGE - The universal adaptability is achieved. No artificial feature extraction is required with reduced complexity. The accurate and stable classification result are achieved and can be used in the signal classification and identification technology field.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the radio signal modulation identification method for multi-scale convolutional neural network. (Drawing includes non-English language text)
DC T01 (Digital Computers); W01 (Telephone and Data Transmission Systems)
MC T01-J03; T01-J05B2; T01-N01B3A; W01-A09B
IP H04L-027/00; H04L-027/18
PD CN107979554-A   01 May 2018   H04L-027/00   201833   Pages: 13   Chinese
AD CN107979554-A    CN11144077    17 Nov 2017
PI CN11144077    17 Nov 2017
UT DIIDW:201835022A
ER

PT P
PN CN107977656-A
TI Pedestrian recognition method, involves extracting multiple global features of multiple frames in video sequence image, and obtaining image feature sequence level by adopting time pyramid syncretizing algorithm.
AU ZHANG S
   TIAN Q
   GAO W
   LI J
AE UNIV PEKING (UYPK-C)
GA 2018350613
AB    NOVELTY - The method involves receiving a video sequence image. A space alignment process of the video sequence image is performed by utilizing a two-dimensional affine transformation process. Multiple global features of multiple frames in the video sequence image are extracted through convolutional neural network after performing the two-dimensional affine transformation process. An image feature sequence level is obtained by adopting a time pyramid syncretizing algorithm according to the global features of the frames in the video sequence image.
   USE - Pedestrian recognition method.
   ADVANTAGE - The method enables identifying data set based on the video sequence image so as to improve pedestrian recognition accuracy and reducing complexity of the time pyramid syncretizing algorithm.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a pedestrian recognition system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a pedestrian recognition system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B1; T01-J10B2A; T01-J10B3A
IP G06K-009/00; G06T-003/00
PD CN107977656-A   01 May 2018   G06K-009/00   201833   Pages: 8   Chinese
AD CN107977656-A    CN11435514    26 Dec 2017
PI CN11435514    26 Dec 2017
UT DIIDW:2018350613
ER

PT P
PN CN107967468-A
TI Driverless cars based auxiliary control system, has motor driving module connected with motor and secondary display terminal, and deep learning network provided with five convolution layers, two pool layers and two connection layers.
AU LIU Z
AE LIU Z (LIUZ-Individual)
GA 201834387K
AB    NOVELTY - The system has a camera module connected with an image collecting module. The image collecting module and a front light control module are connected with a depth convolutional network server. A motor driving module is connected with a motor and a secondary display terminal. The front light control module is connected with a front light and detects an image brightness average value from the camera module. The front light is controlled based on brightness average value result. A deep learning network is provided with five convolution layers, two pool layers and two connection layers.
   USE - Driverless cars based auxiliary control system.
   ADVANTAGE - The system can identify name identification image, analyze the image, extract image characteristic of a detected target so as to the unmanned motor drive driving car deceleration, and realize non-auxiliary unmanned car control function.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a driverless cars based auxiliary control system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); X21 (Electric Vehicles)
MC T01-J04B2; T01-J07D1; T01-J10B2; T01-N01B3; T04-D04; X21-A01L; X21-A04; X21-A05A; X21-K
IP B60L-015/20; G06K-009/00; G06K-009/62; G06N-003/04; G06N-003/08
PD CN107967468-A   27 Apr 2018   G06K-009/00   201833   Pages: 21   Chinese
AD CN107967468-A    CN10055725    19 Jan 2018
PI CN10055725    19 Jan 2018
UT DIIDW:201834387K
ER

PT P
PN CN107967817-A
TI Multi-camera deep learning based car park intelligent management system, has vehicle behavior judging sub-module for monitoring and determining vehicle behavior based on training of neural network model.
AU ZHANG H
   WANG J
AE ZHANG H (ZHAN-Individual)
   WANG J (WANG-Individual)
GA 201836421M
AB    NOVELTY - The system has a server terminal provided with a data collecting module, and a network transmission module for transmitting collected vehicle video and/or image data to a background calculation module for processing parking vehicle information. A vehicle behavior judging sub-module monitors and determines vehicle behavior based on training of a neural network model. The background calculation module is provided with a vehicle parking sub-module for realizing automatic identification and management of a car park depth neural network based on a multi-camera.
   USE - Multi-camera deep learning based car park intelligent management system.
   ADVANTAGE - The system is convenient to use, and improves neural network recognition rate and stability, and has strong adaptability, convenient installation, low manufacturing cost and strong anti-electromagnetic interference ability.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a multi-camera deep learning based car park intelligent management method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a multi-camera deep learning based car park intelligent management system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T05 (Counting, Checking, Vending, ATM and POS Systems); T07 (Traffic Control Systems); W04 (Audio/Video Recording and Systems)
MC T01-J07D1; T01-N01B3; T01-N01D1B; T01-N01D3; T01-N02B2; T05-C03; T05-D02; T07-A01B; T07-A03; T07-F; W04-W05A
IP G07B-015/02; G08G-001/017; G08G-001/14
PD CN107967817-A   27 Apr 2018   G08G-001/14   201833   Pages: 17   Chinese
AD CN107967817-A    CN11144185    17 Nov 2017
PI CN11144185    17 Nov 2017
UT DIIDW:201836421M
ER

PT P
PN CN107958216-A
TI Semi-supervised multimodal depth learning classification method, involves searching approximated marked sample based on cluster mark, and predicting mark information of un-marked sample according to mark information of marked sample.
AU LI Z
   HUANG L
   LIU C
   WANG T
   ZHANG D
   ZHAO L
   DAN X
   WANG Y
   WU H
AE UNIV SHENYANG AEROSPACE (UYAE-C)
GA 201834753U
AB    NOVELTY - The method involves dividing a high spectrum image into a spectral modal, a spatial texture modal and a spatial correlation modal corresponding to depth of a convolutional neural network. Different modal information is determined for performing a clustering process. An intrinsic attribute type of each modal is obtained corresponding to a hidden attribute classifier. Corresponding relation between two classes is determined. An approximated marked sample is searched based on a cluster mark. Mark information of an un-marked sample is predicted according to the mark information of a marked sample.
   USE - Semi-supervised multimodal depth learning classification method.
   ADVANTAGE - The method enables providing sufficient sample using a semi-supervised process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a semi-supervised multimodal depth learning classification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J04B2; T01-J05B2; T01-J05B3; T01-J10B2; T01-N01B3; T01-N03A2; W04-W05A
IP G06K-009/00; G06K-009/62; G06N-003/04; G06N-003/08
PD CN107958216-A   24 Apr 2018   G06K-009/00   201833   Pages: 11   Chinese
AD CN107958216-A    CN11202305    27 Nov 2017
PI CN11202305    27 Nov 2017
UT DIIDW:201834753U
ER

PT P
PN CN107909564-A
TI Deep learning based full convolution network image crack detecting method, involves training full convolution neural network model, and performing crack image detecting process in test data set by utilizing neural network model.
AU WU X
   WANG S
   LIU X
   ZHANG Y
   LIU T
   LIU C
   MAO J
AE UNIV KUNMING SCI & TECHNOLOGY (UKST-C)
GA 201830763P
AB    NOVELTY - The method involves collecting a crack image. The crack image is divided into a training data set and a test data set. An image crack region label is marked in the training data set. A convolutional neural network model is established for performing full crack image detection process. The full convolution neural network model is trained by utilizing the training data set. Crack image detecting process is performed in the test data set by utilizing the trained convolutional neural network model. The crack image is converted into a uniform size image by utilizing interpolation algorithm.
   USE - Deep learning based full convolution network image crack detecting method.
   ADVANTAGE - The method enables effectively increasing crack information obtaining effect in a network model so as to realize effective progressive transmission, and improving network model crack detection precision.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning based full convolution network image crack detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J10B2; T01-J10D; T01-N01B3A
IP G06T-007/00; G06T-007/12; G06T-007/136
PD CN107909564-A   13 Apr 2018   G06T-007/00   201833   Pages: 17   Chinese
AD CN107909564-A    CN10992707    23 Oct 2017
PI CN10992707    23 Oct 2017
UT DIIDW:201830763P
ER

PT P
PN CN107868979-A
TI Controlling silicon single crystal diameter based on constant pull speed control structure comprises obtaining thermal field temperature and crystal diameter data pairs and determining algorithm.
AU LIU D
   DUAN W
   ZHANG X
AE UNIV XIAN TECHNOLOGY (UYXT-C)
GA 201829896U
AB    NOVELTY - Controlling silicon single crystal diameter based on constant pull speed control structure comprises (i) obtaining thermal field temperature and crystal diameter data pairs; (ii) determining algorithm using the output correlation time-delay and obtaining the thermal field temperature-crystal diameter nonlinear large time-delay model using lipschitz quotient, stack sparse auto-encoder; and (iii) solving the temperature control rate of the thermal field using generalized predictive control method of the stack sparse auto-encoder and realizing the real-time control of the crystal diameter.
   USE - The method is useful for controlling silicon single crystal diameter based on constant pull speed control structure.
   DETAILED DESCRIPTION - Controlling silicon single crystal diameter based on constant pull speed control structure comprises (i) obtaining thermal field temperature and crystal diameter data pairs specified as (T(k), D(k)), k=1, 2, ..., M; (ii) determining algorithm using the output correlation time-delay and obtaining the thermal field temperature-crystal diameter nonlinear large time-delay model using lipschitz quotient, stack sparse auto-encoder; and (iii) solving the temperature control rate of the thermal field using generalized predictive control method of the stack sparse auto-encoder and realizing the real-time control of the crystal diameter.
TF TECHNOLOGY FOCUS - INORGANIC CHEMISTRY - Preferred Components: In the conventional silicon single crystal growth control system, the thermal field temperature and crystal diameter sampling signals are obtained through a thermal field temperature detection device and a diameter detection device. In the step (ii), assume the differential equation of the silicon crystal thermal field temperature-crystal diameter nonlinear large time-delay process, use the output correlation time-delay determination method to obtain the time-delay parameter of the thermal field temperature-crystal diameter process, the Lipschitz quotient used to determine the thermal field temperature-input and output order of the nonlinear dynamic system in the crystal diameter, the thermal field temperature-crystal diameter nonlinear model function obtained using stacked sparse automatic encoder. The difference equation is specified as D(k)=f-D(k-1)....D(k-ny),T(kd),...,T(kd-nu)+ sigma (k), where f is a nonlinear continuous function, Nu, ny, and d are nonlinear input and output orders and system time delay, and sigma (k) is a white noise signal. In the step (iv), the thermal field temperature-crystal diameter nonlinear system is time-delayed and the input and output orders are determined, the thermal field temperature-crystal diameter nonlinear model function f is obtained using a stacked sparse automatic encoder, the acquisition of f can be divided into two phases, namely unsupervised pre-training and supervised global fine-tuning in the unsupervised pre-training phase, sparse auto from the bottom of a stacked sparse auto coder, the encoder starts training alone to minimize the error between input and output, the training of the underlying sparse automatic encoder is completed, the hidden layer output of the sparse automatic encoder is used as the input of the next sparse automatic encoder and layer by layer until all sparse automatic encoders are trained. The supervised global fine-tuning is the last stage of stack self-encoding neural network training, this stage treats all layers of the network as a model during each iteration, the batch gradient descent method is used to optimize the network ownership values and thresholds, complete stack sparse autoencoders through unsupervised pre-training and supervised global fine-tuning, the network training weights and thresholds are used as the silicon single crystal thermal field temperature-crystal diameter model parameters, the thermal field temperature-crystal diameter nonlinear large time-delay model function is obtained. In the step (iii), the constant-speed silicon single-crystal growth control structure, the linear output of the silicon single crystal growth process at the current time is predicted online by a stack sparse automatic encoder prediction model based on the current crystal diameter output and crystal diameter setting, calculate crystal diameter expectation reference by first-order smoothing model, the deviation vector is obtained by predicting the crystal diameter output sequence and the diameter reference trajectory, the thermal field temperature control quantity T(k) is solved by optimizing the predictive control performance index in order to achieve crystal diameter control, the specific control rate solution process comprises stacking sparse automatic encoder Generalized predictive control performance index, where N1 is the maximum prediction time domain and Nu is the control time domain, Nu less than or equal to N1, lambda j is the control weight constant, the role is to limit the dramatic change in the thermal field temperature increase delta T(k), reducing the influence of silicon single crystal diameter is a multi-step prediction output of the prediction model in order to make the crystal diameter D(k) smoothly transition to the set crystal diameter Ds(k), the first-order smoothing model with the softening factor a is used to calculate the diameter reference trajectory Dr(k+i) using the gradient descent method to calculate the thermal field temperature control increment for equation, in combination with the control quantity T(k-1) at the previous moment, the thermal field temperature control quantity at time k is specified as T(k)=T(k-1)+1,0,...,0(I+ mu lambda )-1 delta Due, where mu is the optimal step size, the matrix delta Du, the control weighting coefficient lambda , and the deviation vector e are for the N-layer stacked sparse automatic encoder, the sensitivity in matrix delta Du, i=0,1,...,N1-d,0 less than or equal to h less than or equal to i, if i-Nu-d and the derivative of the hyperbolic tangent function, mi is the number of nodes in the i-th layer.
DC E36 (Non-metallic elements, semi-metals (Se, Te, B, Si) and their compounds (except for E35).); J04 (Chemical/physical processes/apparatus - including catalysis, catalysts (excluding specific e.g. enzymatic or polymerisation catalysts), colloid chemistry, laboratory apparatus and methods, testing, controlling, general encapsulation, detection and sampling (excluding clinical testing) (B01J, L).); T01 (Digital Computers); U11 (Semiconductor Materials and Processes)
MC E11-Q03N; E12-A01; E31-P06A; J04-A04; T01-J04B2; U11-B01; U11-B05C; U11-C15D
IP C30B-015/22; C30B-029/06
PD CN107868979-A   03 Apr 2018   C30B-029/06   201833   Pages: 26   Chinese
AD CN107868979-A    CN10772913    31 Aug 2017
PI CN10772913    31 Aug 2017
DN 107015-0-0-0-A K P
CI R01666-A K P
RG 1666-P
UT DIIDW:201829896U
ER

PT P
PN CN109087315-A
TI Convolutional neural network based image recognizing and locating method, involves constructing combined training set, and training convolutional neural network according to combined training set for identifying and locating target image.
AU CAO T
   LIU C
AE INST ELECTRONICS CHINESE ACAD SCI (CAEI-C)
GA 2019014589
AB    NOVELTY - The method involves constructing a convolutional neural network corresponding to an image to-be-identified. An image subset to-be-identified and a subset of a target image are constructed. A combined training set is constructed corresponding to the target image, where the combined training set includes the identified image and the target image subset. The convolutional neural network is trained according to the combined training set for identifying and locating the target image. Target image color characteristics and reflection characteristics are determined for performing image segmentation.
   USE - Convolutional neural network based image recognizing and locating method.
   ADVANTAGE - The method enables mixing the target image and the identified image together for training and testing the convolutional neural network to previously input training data of a testing image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based image recognizing and locating method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2A; T01-J10B3B; T01-N01B3A; T04-D08
IP G06T-007/11; G06T-007/90; G06N-003/04
PD CN109087315-A   25 Dec 2018   G06T-007/11   201914   Pages: 20   Chinese
AD CN109087315-A    CN10963632    22 Aug 2018
PI CN10963632    22 Aug 2018
UT DIIDW:2019014589
ER

PT P
PN CN109087386-A
TI Method for reconstructing three-dimensional human face, involves performing deep learning process on feature points of target face, and registering cloud model with three-dimensional points to obtain final target face point cloud model.
AU CHAO Z
   WANG S
AE CHENGDU TONGJIA YOUBO TECHNOLOGY CO LTD (CHEN-Non-standard)
GA 201901456M
AB    NOVELTY - The method involves obtaining multiple three-dimensional feature points of a target face. Left and right binocular two-dimensional images of the target face are obtained. Deep learning process is performed on the three-dimensional feature points of the target face to obtain a target face point cloud model. The target face point cloud model is registered with target face three-dimensional points to obtain a final target face point cloud model. Three-dimensional feature point coordinates of the target face are obtained by adopting binocular stereo matching algorithm.
   USE - Method for reconstructing a three-dimensional human face.
   ADVANTAGE - The method enables reconstructing a three-dimensional human face with scale information and surface information, obtaining three-dimensional reconstruction result in an effective manner and increasing utilization rate of the three-dimensional human face model.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a three-dimensional human face reconstruction system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for reconstructing a three-dimensional human face. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B3A; T01-J10C4; T01-J30A
IP G06T-017/00; G06T-003/00
PD CN109087386-A   25 Dec 2018   G06T-017/00   201914   Pages: 11   Chinese
AD CN109087386-A    CN10566434    04 Jun 2018
PI CN10566434    04 Jun 2018
UT DIIDW:201901456M
ER

PT P
PN CN109086690-A
TI Method for extracting image feature of ReID by using electronic device, involves performing global pooling process on feature image, and processing channel features or spatial features to obtain image of image feature.
AU FAN X
   ZHANG X
   ZHANG C
AE MEGVII TECHNOLOGY LTD (MEGV-Non-standard)
GA 201901473K
AB    NOVELTY - The method involves performing feature extraction process on an image by using a first convolutional neural network to obtain a spatial feature image. The feature image is divided into multiple parts in a spatial dimension. Global pooling process is performed on each part to obtain spatial features. Number of channels of the feature image is amplified by using a second convolutional neural network to obtain an amplified feature image. The global pooling process is performed on the feature image. Channel features or spatial features are processed to obtain an image of an image feature.
   USE - Method for extracting image feature of a ReID by using an electronic device (claimed).
   ADVANTAGE - The method enables realizing better local detail and channel feature with global sense so as to effectively carry out description of image features.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for extracting image feature of a ReID by using an electronic device
   (2) a ReID image target identification device
   (3) a ReID image target identifying method
   (4) a computer readable storage medium for storing a set of instructions for extracting image feature of a ReID by using an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for extracting image feature of a ReID by using an electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2
IP G06K-009/00
PD CN109086690-A   25 Dec 2018   G06K-009/00   201914   Pages: 18   Chinese
AD CN109086690-A    CN10776757    13 Jul 2018
PI CN10776757    13 Jul 2018
UT DIIDW:201901473K
ER

PT P
PN CN109087305-A
TI Deep convolutional neural network based crack image segmentation method, involves placing obtained small area image in trained deep convolutional neural network for prediction, and obtaining probability map of original image.
AU FAN Z
   WU Y
   ZHU G
   LU J
AE UNIV SHANTOU (USHT-C)
GA 201901458K
AB    NOVELTY - The method involves extracting a small area image of a neighborhood point of a center point from an original image with each pixel point of the original image in a training set as a center point. The small area image is divided into a positive sample and a negative sample according to a central point of the small area image. A deep convolutional neural network is established, where the deep convolutional neural network includes a convolutional layer, a maximum pooled layer and a fully connected layer. Positive and negative samples of the original image of the training set are placed into the deep convolutional neural network for training according to a set ratio. The original image in the test set is operated. The obtained small area image is placed in the trained deep convolutional neural network for prediction. A probability map of the original image is obtained.
   USE - Deep convolutional neural network based crack image segmentation method.
   ADVANTAGE - The method enables realizing the segmentation and detection of the crack image efficiently and accurately.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep convolutional neural network based crack image segmentation method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-N01B3A
IP G06T-007/11; G06N-003/04; G06N-003/08; G06T-007/00
PD CN109087305-A   25 Dec 2018   G06T-007/11   201914   Pages: 9   Chinese
AD CN109087305-A    CN10676243    26 Jun 2018
PI CN10676243    26 Jun 2018
UT DIIDW:201901458K
ER

PT P
PN CN109086437-A
TI Fusion Faster-RCNN Wasserstein encoder based image retrieval method, involves constructing local characteristic image database, and encrypting local feature map from encoder to finish fine granularity search operation of image.
AU FENG Y
   ZHANG Y
   SHANG J
   QIANG B
   QIU Y
AE UNIV CHONGQING (UYCQ-C)
   UNIV GUILIN ELECTRONIC TECHNOLOGY (UYGE-C)
GA 201901479P
AB    NOVELTY - The method involves building (S1) a deep learning framework. A Faster-RCNN model is deployed and trained (S2) for adjusting network weights. A global feature extraction image is captured (S3) for constructing a feature image picture library. A Wasserstein encoder is constructed (S4). Euclidean distance between global features is calculated (S5) to obtain first similarity. Features of a candidate area are extracted (S6) according to an extracted image. A local characteristic image database is constructed. A local feature map is encrypted (S7) from an encoder for calculating second similarity to finish a fine granularity search operation of an image.
   USE - Fusion Faster-RCNN Wasserstein encoder based image retrieval method.
   ADVANTAGE - The method enables improving image searching speed and accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a fusion faster-RCNN wasserstein encoder based image retrieval method. '(Drawing includes non-English language text)'
   Step for building deep learning framework (S1)
   Step for training faster-RCNN model (S2)
   Step for capturing global feature extraction image (S3)
   Step for constructing Wasserstein encoder (S4)
   Step for calculating euclidean distance between global features (S5)
   Step for extracting features of candidate area (S6)
   Step for encrypting local feature map (S7)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-D01; T01-J05B4F; T01-J10B1; T01-J10B2; T01-J10D; T01-N01B3; T01-N03A2; T04-D04
IP G06F-017/30; G06K-009/62; G06N-003/04
PD CN109086437-A   25 Dec 2018   G06F-017/30   201914   Pages: 10   Chinese
AD CN109086437-A    CN10926656    15 Aug 2018
PI CN10926656    15 Aug 2018
UT DIIDW:201901479P
ER

PT P
PN CN109086798-A
TI Data marking method, involves obtaining position change data of characteristic key point, and generating marking data of characteristic key point according to position change data in image after deformation of reference model.
AU FU Y
   LI W
   FANG L
   WANG Y
   LEI Y
   ZUO X
   HUANG N
   MA J
   JIN Y
AE MAGICS TECHNOLOGY BEIJING CO LTD (MAGI-Non-standard)
GA 201901470V
AB    NOVELTY - The method involves performing manual data marking process on a reference model to obtain a characteristic key point. Position change data of the characteristic key point is obtained based on reference model deformation. Marking data of the characteristic key point is generated according to the position change data in an image after deformation of the reference model. A three-dimensional profile model of an object to-be-identified is established. Texture mapping process is performed on the three-dimensional profile model. A deformation position of the characteristic key point is determined according to the reference model.
   USE - Data marking method.
   ADVANTAGE - The method enables reducing training data production cost and environment scene complexity, controlling training data combination mode, avoiding controllable legal risk so as to satisfy depth learning requirements. The method enables improving data collecting quality and reducing data collecting period, legal risk and data marking resource consumption rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a data marking device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a data marking method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J10B3A; T04-D04; T04-D07D5
IP G06K-009/62
PD CN109086798-A   25 Dec 2018   G06K-009/62   201914   Pages: 14   Chinese
AD CN109086798-A    CN10716845    03 Jul 2018
PI CN10716845    03 Jul 2018
UT DIIDW:201901470V
ER

PT P
PN CN109086255-A
TI Deep learning based reference document automatic labeling method, involves calculating segment features for each segment of resulting segment set, and completing reference document automatic annotation by training citation tagging model.
AU GAO L
   AN D
   TANG Z
AE UNIV PEKING (UYPK-C)
GA 201901484A
AB    NOVELTY - The method involves extracting citation entries contained in a document by analyzing content of the document to locate a reference area in the document. A citation annotation model is analyzed based on deep learning by using citation training data with annotations. BibTeX data is collected from an internet to perform pre-processing. Depth characteristics in the sequence of citation words are automatically extracted by using a sequence marking network architecture and bidirectional long-short term memory (LSTM) unit. Segment features are calculated for each segment of a resulting segment set. Reference document automatic annotation is completed by using a training citation tagging model.
   USE - Deep learning based reference document automatic labeling method.
   ADVANTAGE - The method enables easily expanding field and style of reference documents with high working accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning based reference document automatic labeling system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a deep learning based reference document automatic labeling method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J04A; T01-J05B1; T01-J11A; T01-M02A1C; T01-N01B3; T01-N01D2; W04-W05A
IP G06F-017/21
PD CN109086255-A   25 Dec 2018   G06F-017/21   201914   Pages: 10   Chinese
AD CN109086255-A    CN10744884    09 Jul 2018
PI CN10744884    09 Jul 2018
UT DIIDW:201901484A
ER

PT P
PN CN109086837-A
TI Convolutional neural network based method for classifying user attribute of electronic device, involves receiving output of attribute classification result of detected object from user attribute convolutional neural network model.
AU GAO S
AE GAO S (GAOS-Individual)
GA 201901469T
AB    NOVELTY - The method involves inputting psychological attribute related electronic data of multiple users as an input vector set into a user attribute classification convolutional neural network model. Psychological results of multiple users obtained by a questionnaire are input as a target vector set into a training feedback model. The user attribute classification convolutional neural network model is updated. Psychological attribute related electronic data of a measured object is sent as an input to the user attribute classification convolutional neural network model. An attribute classification result of the measured object is received as an output from the user attribute classification convolutional neural network model.
   USE - Convolutional neural network based method for classifying a user attribute of an electronic device. (claimed).
   ADVANTAGE - The method enables simplifying user attribute classifying operation and improving user attribute classification efficiency and accuracy.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a convolutional neural network based device for classifying a user attribute of an electronic device
   (2) a computer readable storage medium for storing set of instructions for performing user attributes classification process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based method for classifying user attribute of electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B2; T01-J10B2; T01-N01B3
IP G06K-009/62; G06N-003/04
PD CN109086837-A   25 Dec 2018   G06K-009/62   201914   Pages: 15   Chinese
AD CN109086837-A    CN11246303    24 Oct 2018
PI CN11246303    24 Oct 2018
UT DIIDW:201901469T
ER

PT P
PN CN109086647-A
TI Method for detecting smoke in image by utilizing electronic device, involves inputting smoke area and non-smoke area corresponding to smoke area image to convolutional neural network, and obtaining parameter value of neural network.
AU GUO Y
   DONG Y
   BAI H
   XIONG F
AE BEIJING FACEALL TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 201901474E
AB    NOVELTY - The method involves processing a smoke video to obtain a smoke image frame. Smoke image color characteristic and movement characteristic are extracted. A smoke area and a non-smoke area are determined in a smoke area image. The smoke area and the non-smoke area corresponding to the smoke area image are input to a convolutional neural network. A parameter value of the convolutional neural network is obtained. Judgment is made to whether check motion direction satisfies a predetermined condition. The smoke area is determined when motion direction satisfies the predetermined condition.
   USE - Method for detecting smoke in an image by utilizing an electronic device (claimed).
   ADVANTAGE - The method enables realizing better robustness and increasing smoke detecting accuracy.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for detecting smoke in an image by utilizing an electronic device
   (2) a computer-readable storage medium for storing a set of instructions for detecting smoke in an image by utilizing an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for detecting smoke in an image by utilizing an electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J10B3B; T01-S03; T04-D08
IP G06K-009/00
PD CN109086647-A   25 Dec 2018   G06K-009/00   201914   Pages: 22   Chinese
AD CN109086647-A    CN10506665    24 May 2018
PI CN10506665    24 May 2018
UT DIIDW:201901474E
ER

PT P
PN CN109077720-A
TI Method for processing cardiac electrical signal by using computer device, involves obtaining identification result of detector, and determining to-be-detected electrical signal type according to identification result with maximal proportion.
AU HU J
AE GUANGZHOU SHIYUAN ELECTRONICS TECHNOLOGY (GUAZ-C)
GA 201901681K
AB    NOVELTY - The method involves selecting signal detectors to identify to-be-detected signals, where each signal detector is connected with an adaptive threshold detector, a support vector machine detector and a depth characteristic detector. An identification result of each detector is obtained. To-be-detected cardiac electrical signal type is determined according to an identification result with maximal proportion. A transient signal is extracted in the detection signal. An instantaneous cardiac electrical signal is used as an identifying characteristic parameter to establish a deep learning model.
   USE - Method for processing a cardiac electrical signal by using a computer device (claimed).
   ADVANTAGE - The method enables obtaining accurate signal recognition result and strong fault tolerance.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for processing a cardiac electrical signal by using a computer device
   (2) a computer readable storage medium storing a set of instruction for processing a cardiac electrical signal by using a computer device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for processing a cardiac electrical signal by using a computer device. '(Drawing includes non-English language text)'
DC P31 (Diagnosis, surgery (A61B).); S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D01; T01-J06A
IP A61B-005/046
PD CN109077720-A   25 Dec 2018   A61B-005/046   201914   Pages: 17   Chinese
AD CN109077720-A    CN10730210    05 Jul 2018
PI CN10730210    05 Jul 2018
UT DIIDW:201901681K
ER

PT P
PN CN109084724-A
TI Binocular vision based deep learning obstacle and vehicle distance measuring method, involves measuring obstacle distance by using distance calculation formula to realize target recognition and ranging.
AU HU S
   ZHANG J
   SHI H
AE UNIV XIAN TECHNOLOGY (UYXT-C)
GA 2019015201
AB    NOVELTY - The method involves constructing a binocular vision data acquisition system including a binocular camera package with first and second cameras, where the first and second cameras are relatively fixed on a camera support frame. A binocular camera projection model is established based on a principle of pinhole camera, and the binocular camera is calibrated. Internal parameter matrix of the binocular camera is calculated in the projection model, and a relative geometric relationship between the first and second cameras is obtained. Two-dimensional centroid coordinates of the cameras are substituted to obtain binocular vision. Three-dimensional coordinates of an obstacle object in a space are obtained by a data acquisition system model, and obstacle distance is measured by using a distance calculation formula to realize target recognition and ranging.
   USE - Binocular vision based deep learning obstacle and vehicle distance measuring method.
   ADVANTAGE - The method enables detecting the distance between the obstacle and a vehicle in an environment suing a deep learning target detection algorithm and the camera model in a quick, simple and effective manner, by only installing the binocular camera model on a vehicle body. The method enables satisfying needs of real-time detection of blind spots in the vehicle, and playing an early warning role for drivers to ensure safety of the drivers.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a binocular vision based deep learning obstacle and vehicle distance measuring method. '(Drawing includes non-English language text)'
DC S02 (Engineering Instrumentation); T01 (Digital Computers)
MC S02-B01; T01-J04A; T01-J07D3A; T01-J30A
IP G01C-003/00; G06T-007/80
PD CN109084724-A   25 Dec 2018   G01C-003/00   201914   Pages: 14   Chinese
AD CN109084724-A    CN10737200    06 Jul 2018
PI CN10737200    06 Jul 2018
UT DIIDW:2019015201
ER

PT P
PN CN109086805-A
TI Depth neural network based constrained task clustering method, involves outputting disparity data set sample from network, coupling self-encoding network and neural network, and performing task clustering process by clustering algorithm.
AU HUANG J
   WANG J
AE UNIV SOUTH CHINA TECHNOLOGY (UYSC-C)
GA 201901470N
AB    NOVELTY - The method involves obtaining data set (S1). Data set pre-processing operation is performed (S2) for obtaining disparity between data set samples. A self-encoding network and a deep neural network are established (S3). A disparity vector of the data set is obtained (S4) from an encoding network. A disparity data set sample is output from the encoding network. A self-encoding network and a deep neural network are coupled (S5). Task clustering process is performed by a clustering algorithm. A network model prediction result is obtained. A sequence is obtained from an encoder. A connected neural network is updated.
   USE - Depth neural network based constrained task clustering method.
   ADVANTAGE - The method enables combining original data set and constraint data, performing dimension reduction operation and depth neural network learning feature self-encoding process, and determining network model loss function based on gradient descent optimization algorithm so as to effectively improve clustering accuracy of clustering algorithm.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a depth neural network based constrained task clustering method. '(Drawing includes non-English language text)'
   Step for obtaining data set (S1)
   Step for performing data set pre-processing operation for obtaining disparity between data set samples (S2)
   Step for establishing self-encoding network and deep neural network (S3)
   Step for obtaining disparity vector of data set from encoding network (S4)
   Step for coupling self-encoding network and neural network (S5)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-D02; T01-J05B3; T01-J10B2; T01-N03A2; T04-D04; T04-D07D3
IP G06K-009/62
PD CN109086805-A   25 Dec 2018   G06K-009/62   201914   Pages: 10   Chinese
AD CN109086805-A    CN10765487    12 Jul 2018
PI CN10765487    12 Jul 2018
UT DIIDW:201901470N
ER

PT P
PN CN109086867-A
TI Data transmission method for convolutional neural network acceleration system, involves providing data exchange module for exchanging data with external host over peripheral component interconnect interface.
AU LI K
   ZOU F
   SUN H
   LI Q
   QI D
   HE K
AE WUHAN MEITONG TECHNOLOGY CO LTD (WUHA-Non-standard)
GA 2019014695
AB    NOVELTY - The method involves using a data pre-processing module for reading a corresponding convolution kernel parameter and an input feature map from a data storage module according to a calculated phase of current. The convolution kernel parameter and the input feature map are utilized for adjusting a four-dimensional convolution kernel parameter into a three-dimensional convolution kernel parameter. The data storage module is utilized for storing a set of model parameters along with an intermediate feature map calculation result and final calculation result. A data exchange module is provided for exchanging data with an external host over a peripheral component interconnect interface.
   USE - Data transmission method for convolutional neural network acceleration system (claimed).
   ADVANTAGE - The data exchange module is provided for exchanging data with the external host over the peripheral component interconnect interface, thus allowing efficient parallel flow mode operation, reducing power consumption, greatly improving the convolutional neural network processing speed to meet the real-time requirements.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a convolutional neural network acceleration system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a convolutional neural network acceleration system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-C03; T01-J04B2; T01-N01D
IP G06N-003/04; G06N-003/063
PD CN109086867-A   25 Dec 2018   G06N-003/04   201914   Pages: 17   Chinese
AD CN109086867-A    CN10710069    02 Jul 2018
PI CN10710069    02 Jul 2018
UT DIIDW:2019014695
ER

PT P
PN CN109086723-A
TI Transfer learning based human face detecting method, involves training convolutional neural network to obtain target grid parameters of network, and identifying real face image in target data set by using network of target data set.
AU LI L
   CHEN W
   LIAO G
   WU Y
AE UNIV GUANGDONG TECHNOLOGY (UGTE-C)
GA 201901472R
AB    NOVELTY - The method involves normalizing a human face image in a collected target data set according to a size of a human face image in a source data set. A transfer portion of a convolutional neural network of the source data set is directly migrated. Fine adjustment of non-transfer portion of the convolutional neural network of the source data set is performed to obtain a grid structure of a convolutional neural network of the target data set. The convolutional neural network is trained to obtain target grid parameters of the convolutional neural network of the target data set. A real face image is identified in the target data set by using the convolutional neural network of the target data set.
   USE - Transfer learning based human face detecting method.
   ADVANTAGE - The method enables realizing rapid re-designing and training of the convolutional neural network of the data set.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a transfer learning based human face detecting device
   (2) a computer-readable storage medium for storing a set of instructions for detecting a human face based on transfer learning.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a transfer learning based human face detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-N01B3; T01-N01D1B; T01-S03; T04-D04; T04-D07F1
IP G06K-009/00; G06K-009/62
PD CN109086723-A   25 Dec 2018   G06K-009/00   201914   Pages: 15   Chinese
AD CN109086723-A    CN10890473    07 Aug 2018
PI CN10890473    07 Aug 2018
UT DIIDW:201901472R
ER

PT P
PN CN109088774-A
TI Distributed system deployment method, involves collecting parameter, generating configuration file according to collected parameter, and distributing configuration file to distributed system based on deep learning program.
AU LI M
AE ZHENGZHOU YUNHAI INFORMATION TECHNOLOGY (INEI-C)
GA 201901426U
AB    NOVELTY - The method involves collecting parameter. A configuration file i.e. YAML (RTM: Human-readable data serialization language) file, is generated according to the collected parameter. The configuration file is distributed to a distributed system based on deep learning program, which comprises a sensor flow program.
   USE - Distributed system deployment method.
   ADVANTAGE - The method enables ensuring rapid and convenient depth deployment of distributed task, and effectively reducing complexity and error caused by manual deployment of resources.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a distributed system deployment device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a distributed system deployment method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-H07A; T01-J30A; T01-N01D2; T01-S03; W04-P01F3; W04-W05A
IP H04L-012/24; H04L-029/08
PD CN109088774-A   25 Dec 2018   H04L-012/24   201914   Pages: 6   Chinese
AD CN109088774-A    CN10987952    28 Aug 2018
PI CN10987952    28 Aug 2018
UT DIIDW:201901426U
ER

PT P
PN CN109086754-A
TI Method for recognizing human body movement posture in home based on deep learning, involves performing human body movement posture recognition process by adopting deep learning network technology using extracted human bone features.
AU LIN L
   LIU G
   ZHOU W
   YIN H
   CHEN J
   ZHOU Y
   LIU J
   SHEN C
AE UNIV TIANJIN SCI & TECHNOLOGY (UYTC-C)
GA 201901471Y
AB    NOVELTY - The method involves performing human body detection to collect a human body image part, where the human body image is detected and obtained by using a color sensor and an infrared sensor in a Kinect image acquisition device. Bone feature extraction process is performed on the collected human body image part. A part of a data set is established. A deep learning network posture-CNN is constructed. Human body movement posture recognition process is performed by adopting deep learning network technology using extracted human bone features. A Kinect is utilized as a collection device for locating 20 human body key node points from each human body image such that a human body skeleton graph is obtained.
   USE - Method for recognizing human body movement posture in a home based on deep learning.
   ADVANTAGE - The method enables utilizing a convolutional neural network to improve recognition accuracy and reduce recognition time, reducing operation cost, simplifying human body movement posture recognizing in home for human safety monitoring through movement analysis.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for recognizing human body movement posture in a home based on deep learning. '(Drawing includes non-English language text)'
DC B04 (Natural products and polymers. Including testing of body fluids (other than blood typing or cell counting), pharmaceuticals or veterinary compounds of unknown structure, testing of microorganisms for pathogenicity, testing of chemicals for mutagenicity or human toxicity and fermentative production of DNA or RNA. General compositions.); T01 (Digital Computers)
MC B11-C08J; B11-C11; B12-K04C; T01-J10A; T01-J10B2A; T01-J10B3B; T01-N01B3; T01-N02B2
IP G06K-009/00; G06K-009/62; G06N-003/04
PD CN109086754-A   25 Dec 2018   G06K-009/00   201914   Pages: 10   Chinese
AD CN109086754-A    CN11177283    11 Oct 2018
PI CN11177283    11 Oct 2018
UT DIIDW:201901471Y
ER

PT P
PN CN109086866-A
TI Embedded device partial binary convolution method, involves establishing convolution layer of depth convolutional neural network according to statistics, and restoring accuracy by entire network through iterative operation.
AU LIU D
   LING Y
   LIANG L
AE UNIV CHONGQING (UYCQ-C)
GA 2019014696
AB    NOVELTY - The method involves establishing convolution layer of a depth convolutional neural network according to a statistics of each output feature map. A quantity is measured for an importance of convolution kernel. The convolution kernels of layer are sorted according to importance. A convolution kernel importance threshold is obtained. Two sets of convolution kernels on a storage space are re-arranged. A binary quantization on a convolutional layer is divided into non-important. Accuracy is restored by entire network through an iterative operation of quantization and training.
   USE - Embedded device partial binary convolution method.
   ADVANTAGE - The method enables ensuring accuracy of CNN on large data sets as well as reducing convolutional neural network calculation on embedded device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating an embedded device partial binary convolution method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J03; T01-J04B2; T01-N01B3
IP G06N-003/04
PD CN109086866-A   25 Dec 2018   G06N-003/04   201914   Pages: 9   Chinese
AD CN109086866-A    CN10706834    02 Jul 2018
PI CN10706834    02 Jul 2018
UT DIIDW:2019014696
ER

PT P
PN CN109086463-A
TI Regional convolutional neural network based question-answer community label recommending method, involves calculating recall accuracy rate and F1-score of network model, adjusting parameters, and generating final neural network model.
AU LIU J
   ZHOU P
   CHU W
   LI B
   CUI X
   CHEN X
   SHI Z
   PENG X
   ZHAO F
AE UNIV WUHAN (UYWU-C)
GA 2019014794
AB    NOVELTY - The method involves obtaining a training set by performing scrambling sequence process. Mikilovo process is performed to transform the training set into three-dimensional matrix. A convolution neural network model is established, where the convolution neural network model comprises a bidirectional circulating convolutional layer, a pool layer and a softmax layer. Iterative convolutional neural network training process is performed. The convolution neural network model is tested by utilizing a test set. Recall accuracy rate and F1-score of the convolution neural network model are calculated to test effect of the model. Parameters are adjusted. A final convolution neural network model is generated.
   USE - Regional convolutional neural network based question-answer community label recommending method.
   ADVANTAGE - The method enables realizing better relationship between words in a sentence by binding a bidirectional circular convolution layer with context, adopting a bidirectional recycling structure for accurately grasping the context of the word, processing large pictures to handle large data sets and realizing convolutional neural network mobility during image processing so as to recommend a label in a better manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a convolution neural network model. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04A; T01-J04B2; T01-J05B4F; T01-J11A; T01-N01A2D; T01-N01B3A
IP G06F-017/30; G06N-003/04
PD CN109086463-A   25 Dec 2018   G06F-017/30   201914   Pages: 8   Chinese
AD CN109086463-A    CN11139465    28 Sep 2018
PI CN11139465    28 Sep 2018
UT DIIDW:2019014794
ER

PT P
PN CN109087337-A
TI Hierarchical convolution characteristic based long-term target tracking method, involves utilizing frame by associated filter model when tracking response value is lower than threshold value, and performing target weight detection process.
AU LIU Y
   SHENG X
   LIANG H
   LI F
AE UNIV SHANDONG (USHA-C)
GA 201901457N
AB    NOVELTY - The method involves obtaining video data of a first frame and characteristic of a convolution layer by using a pre-trained depth convolutional neural network. Response of the convolution layer is obtained based on an associated filter model. A weighting calculation process is performed. A preset threshold value is updated. The associated filter model is updated when tracking response value is greater than the preset threshold value. A second frame is utilized by the associated filter model when the tracking response value is lower than the preset threshold value. A target weight detection process is performed based in a random algorithm.
   USE - Hierarchical convolution characteristic based long-term target tracking method.
   ADVANTAGE - The method enables improving target tracking precision and tracking speed, reducing calculating redundancy in a tracking process and guaranteeing reliability of a tracking result.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a hierarchical convolution characteristic based long-term target tracking system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a hierarchical convolution characteristic based long-term target tracking method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J10B2
IP G06T-007/246; G06N-003/04
PD CN109087337-A   25 Dec 2018   G06T-007/246   201914   Pages: 17   Chinese
AD CN109087337-A    CN11318709    07 Nov 2018
PI CN11318709    07 Nov 2018
UT DIIDW:201901457N
ER

PT P
PN CN109086408-A
TI Method for generating song text by using electronic device, involves generating text by using topic key word set, song name, song foot and paragraph structure by combining depth learning model with attention model.
AU LIU Z
   NIU C
AE TENCENT TECHNOLOGY SHENZHEN CO LTD (TNCT-C)
GA 201901480G
AB    NOVELTY - The method involves determining a topic key word set, a song name, a song foot and a paragraph structure according to input information of a user. Text is generated by using the topic key word set, the song name, the song foot and the paragraph structure by combining a depth learning model with an attention model. The generated text is combined with a score for generating a song file. Lyric of a sample song is obtained for identifying characters in the lyric of the sample song. A song name word vector is generated through the song name. A topic word vector is generated by using the topic key word set.
   USE - Method for generating song text by using an electronic device (claimed).
   ADVANTAGE - The method enables effectively generating diversified music and song.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for generating song text by using an electronic device
   (2) a computer readable medium comprising a set of instructions for generating song text by using an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for generating song text by using an electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J05B4P; T01-J30A; T01-S03; W04-W05A
IP G06F-017/30; G06N-003/08
PD CN109086408-A   25 Dec 2018   G06F-017/30   201914   Pages: 27   Chinese
AD CN109086408-A    CN10871128    02 Aug 2018
PI CN10871128    02 Aug 2018
UT DIIDW:201901480G
ER

PT P
PN CN109086803-A
TI Deep learning and personal factor based haze visibility detecting system, has neural network training module inputting haze visibility picture to convolutional neural network, and detection system identifying current haze visibility picture.
AU LV H
   CHENG X
   LI H
   LI D
   WANG T
   QIAN J
   REN J
AE UNIV NANJING POSTS & TELECOM (UNPT-C)
GA 201901470Q
AB    NOVELTY - The system has a database establishing module for constructing a haze visibility picture library according to different visibility picture. A visibility extracting module extracts an identifier of the haze visibility picture according to the haze visibility picture library. An individual factor extracting module extracts individual factor according to scene number. A neural network training module inputs the individual factor and the haze visibility picture to a convolutional neural network. A detection system classifies and identifies a current haze visibility picture through a system detecting module.
   USE - Deep learning and personal factor based haze visibility detecting system.
   ADVANTAGE - The system can extract data set characteristic of the convolutional neural network in an automatic manner for processing large data sets in a rapid manner so as to save a lot of training time.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning and personal factor based haze visibility detecting method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning and personal factor based haze visibility detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B4P; T01-J10B2; T01-N01B3; T04-D04
IP G06K-009/62; G06N-003/08
PD CN109086803-A   25 Dec 2018   G06K-009/62   201914   Pages: 17   Chinese
AD CN109086803-A    CN10755419    11 Jul 2018
PI CN10755419    11 Jul 2018
UT DIIDW:201901470Q
ER

PT P
PN CN109086700-A
TI Deep convolutional neural network based one-dimensional range image target identification method, involves collecting HRRP data set, and establishing target identification model to calculate probability value of radar signal.
AU PAN M
   YU Y
   YANG K
   LI X
   LV S
   ZHOU T
   CAO J
   LIU A
AE UNIV HANGZHOU DIANZI (UYHH-C)
GA 201901473A
AB    NOVELTY - The method involves collecting a HRRP data set. A testing sample of different data segments is obtained. A radar attitude value is calculated. Data pre-processing operation is performed. A small intensity value of each data segment is calculated. Energy normalization process is performed to obtain expansion data. An enhancement algorithm is established based on characteristics of a robust Boltzmann machine. A signal-to-noise ratio of the robust Boltzmann machine is calculated. A target identification model is established to calculate a probability value of a radar signal.
   USE - Deep convolutional neural network based one-dimensional range image target identification method.
   ADVANTAGE - The method enables improving sample stability and noise robustness effect and ensuring wide range of applications.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep convolutional neural network based one-dimensional range image target identification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W06 (Aviation, Marine and Radar Systems)
MC T01-D03; T01-J04A; T01-J10B2; T04-D03A; T04-D04; W06-A04
IP G06K-009/00; G06K-009/62; G01S-007/41
PD CN109086700-A   25 Dec 2018   G06K-009/00   201914   Pages: 18   Chinese
AD CN109086700-A    CN10806078    20 Jul 2018
PI CN10806078    20 Jul 2018
UT DIIDW:201901473A
ER

PT P
PN CN109086756-A
TI Deep neural network based text detecting and analyzing method, involves performing template matching process according to label template information and text area information of band category to generate structured information data.
AU QIAN H
   XIE C
   WANG H
   XU B
   LU W
AE ZHONGAN INFORMATION TECHNOLOGY SERVICE (ZHON-Non-standard)
GA 201901471W
AB    NOVELTY - The method involves performing template marking process to generate label template information. A text area to be detected is detected and classified by using a preset depth neural network detection model to generate text area information. Template matching process is performed according to the label template information and text area information of a band category to generate structured information data. Size and position of a template anchor point and a non-anchor text area are determined. Relationship between entity and the anchor point is determined.
   USE - Deep neural network based text detecting and analyzing method.
   ADVANTAGE - The method enables realizing rapid and accurate detection and analysis of document images to extract features of the document images in an accurate manner so as to realize wide range of applications.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep neural network based text detecting and analyzing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep neural network based text detecting and analyzing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B2B; T01-J10B2; T01-N01D2
IP G06K-009/20; G06K-009/34; G06K-009/62; G06N-003/04
PD CN109086756-A   25 Dec 2018   G06K-009/20   201914   Pages: 18   Chinese
AD CN109086756-A    CN10618508    15 Jun 2018
PI CN10618508    15 Jun 2018
UT DIIDW:201901471W
ER

PT P
PN CN109086806-A
TI Compressed image based internet-of-things portable-type device visually recognizing and accelerating method, involves utilizing already trained convolutional neural network, and displaying reasoning and recognizing results in real time.
AU QIAN H
   CHEN X
AE UNIV FUZHOU (UFZU-C)
GA 201901470M
AB    NOVELTY - The method involves performing image collecting process through a collecting end of a device to obtain an original image. The original image is compressed by low resolution. Amount of data is reduced. Compressed image data is transmitted to an inference end of the device. Already trained convolutional neural network is utilized by the inference end of the device for inference recognizing process. Reasoning and recognizing results are displayed in real time. The original image of the low resolution is obtained by using gaussian random matrix. Elements in the matrix are adjusted at normal distribution with variance of 0 and 1.
   USE - Low resolution compressed image based internet-of-things (IOT) portable-type device visually recognizing and accelerating method.
   ADVANTAGE - The method enables reducing calculating data amount and number of parameters.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a low resolution compressed image based IOT portable-type device visually recognizing and accelerating method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2A; T01-J10B3A; T01-J10D; T01-N01D1B; T01-N01F
IP G06K-009/62; G06N-003/04; G06N-003/08
PD CN109086806-A   25 Dec 2018   G06K-009/62   201914   Pages: 7   Chinese
AD CN109086806-A    CN10778973    16 Jul 2018
PI CN10778973    16 Jul 2018
UT DIIDW:201901470M
ER

PT P
PN CN109086753-A
TI Dual-channel convolutional neural network based traffic sign identification method, involves splicing intermediate result to obtain splicing characteristic, and calculating category of to-be-identified image to obtain recognition result.
AU QIAN Y
   ZHANG M
   ZHAO J
   LIU F
AE UNIV XINJIANG (UYXI-Non-standard)
GA 2019014720
AB    NOVELTY - The method involves acquiring a to-be-identified image by an input layer. The to-be-identified image is processed by a sub-convolution neural network to obtain an intermediate result with different characteristics, where number of the intermediate results are same as number of the sub-convolution neural network. The intermediate result is spliced by a connection layer to obtain splicing characteristic. Category of the to-be-identified image is calculated by an output layer according to the splicing characteristic to obtain a recognition result. Convolution operation is performed to the to-be-identified image to obtain a characteristic map.
   USE - Dual-channel convolutional neural network based traffic sign identification method.
   ADVANTAGE - The method enables effectively enhancing shape, color and other characteristics of extracting effect to reduce loss of important feature, thus obtaining distinguishabilities of characteristic information.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a dual-channel convolutional neural network based traffic sign identifying device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic illustration of a dual-channel convolutional neural network based traffic sign identification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B2A; T01-J10B3B; T04-D04; T04-D08
IP G06K-009/00; G06K-009/62; G06N-003/04
PD CN109086753-A   25 Dec 2018   G06K-009/00   201914   Pages: 12   Chinese
AD CN109086753-A    CN11170807    08 Oct 2018
PI CN11170807    08 Oct 2018
UT DIIDW:2019014720
ER

PT P
PN CN109086419-A
TI Method for realizing social communication based on scene and voice distribution in data processing field, involves retrieving user data according to detected scene, and calculating social value to establish voice communication between users.
AU QIN S
   XIONG L
   HU Z
AE GUANGZHOU XIAOPENG AUTOMOBILE TECHNOLOGY (GUAN-Non-standard)
GA 2019014807
AB    NOVELTY - The method involves collecting user data, where user data comprises scene data and user characteristic data. The collected user data is encoded. Feature code is generated by using a deep neural network for identifying learning skills. The collected data and the feature codes are stored in a database. A to-be-detected scene of a current user is collected. User data is retrieved according to the detected scene. A social value is calculated according to the retrieved user data to establish voice communication between current users. Real-time scene state data and/or scene state history data is obtained.
   USE - Method for realizing social communication based on scene and voice distribution in a data processing field.
   ADVANTAGE - The method enables combining scene and voice distribution process in a precise manner according to user requirements.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for realizing social communication based on scene and voice distribution in a data processing field.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for realizing social communication based on scene and voice distribution in a data processing field. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04A; T01-J05B4P; T01-N01A2; T01-N01B3; T01-N01D
IP G06F-017/30; G06N-003/08; G06Q-050/00
PD CN109086419-A   25 Dec 2018   G06F-017/30   201914   Pages: 9   Chinese
AD CN109086419-A    CN10888301    07 Aug 2018
PI CN10888301    07 Aug 2018
UT DIIDW:2019014807
ER

PT P
PN CN109087375-A
TI Deep learning based image hole filling method, involves transmitting training data set to train network model, obtaining input image by training network model, and obtaining void filling result by Hole-Net and Detail-Net processing function.
AU QUAN H
   SHEN Z
AE UNIV EAST CHINA NORMAL (UYEN-C)
GA 201901456U
AB    NOVELTY - The method involves constructing a natural scene data set from pngimg. Com website for collecting a foreground image. A target mask in the foreground image is detected. A NB background image is displayed from a SUN2012 type data set. A background image set is constructed. A neural network is constructed by using a generic confrontation network (GAN). A hole filling final result is obtained. An evaluation data set and a test data set are obtained. The training data set is transmitted to a train network model. An input image is obtained by training the network model. A void filling result is obtained by Hole-Net and Detail-Net processing function.
   USE - Deep learning based image hole filling method.
   ADVANTAGE - The method enables designing a semantic network so as to enhance reality of a two-stage deep learning architecture in the network, define a semantic network in enhanced content loss function, ensure sample consistency, select semantic results of a fully-network as an input, defining complex loss function, so that size and shape of a missing fully region can be detected, thus obtaining semantic realistic detail result in a simple manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a deep learning based image hole filling method.
DC T01 (Digital Computers)
MC T01-J10C5; T01-J16C3; T01-N01B3A; T01-N01D1B; T01-N01D2
IP G06T-011/40
PD CN109087375-A   25 Dec 2018   G06T-011/40   201914   Pages: 13   Chinese
AD CN109087375-A    CN10649384    22 Jun 2018
PI CN10649384    22 Jun 2018
UT DIIDW:201901456U
ER

PT P
PN CN109086707-A
TI DCNNs-LSTM model based human face tracking method, involves generating training data by face video, determining three-dimensional coordinates of model, and controlling face 3D model to realize expression tracking.
AU RAO Y
   SONG J
   JI P
   GOU M
   FAN B
   YANG P
   ZHENG Y
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 2019014734
AB    NOVELTY - The method involves generating a training data by a face video. Three-dimensional (3D) coordinates are marked according to video feature points of a face. An eye position of left and right eyes and 3D information of head offset are determined. Marking information is encapsulated in a single frame. A continuous face of pictures is determined. Facial and head information is extracted by using a deep convolutional neural network (DCNN) and a LSTM network. Three-dimensional coordinates of a model are determined. A face 3D model is controlled to realize expression tracking.
   USE - DCNNs-LSTM model based human face tracking method.
   ADVANTAGE - The method enables calculating Euclidean distance transition deformation coefficient and animation character by a data conversion model so as to realize expression and improve head transfer effect.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating an operation of a DCNNs-LSTM model based human face tracking method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-N01B3
IP G06K-009/00; G06N-003/04
PD CN109086707-A   25 Dec 2018   G06K-009/00   201914   Pages: 12   Chinese
AD CN109086707-A    CN10823018    25 Jul 2018
PI CN10823018    25 Jul 2018
UT DIIDW:2019014734
ER

PT P
PN CN109086405-A
TI Salient map and convolutional neural network based remote sensing image retrieving method, involves performing similarity metric search process, and retuning highest similarity of to-be-detected remote sensing image as result.
AU SHAO Z
   YANG K
   LI C
   ZHOU W
AE UNIV WUHAN (UYWU-C)
GA 201901480K
AB    NOVELTY - The method involves up-sampling and expanding a characteristic map of a convolution layer to original size of an input image to obtain synthesized characteristic of the convolution layer. Weighted integration process is performed on the synthesized characteristic of the convolution layer by using a saliency map. Coding process is performed in the synthesized characteristic of the convolution layer. Feature representation of a to-be-detected remote sensing image is extracted. Similarity metric search process is performed based on the feature representation of the to-be-detected remote sensing image in a searching image library. Highest similarity of the to-be-detected remote sensing image is returned as a result.
   USE - Salient map and convolutional neural network based remote sensing image retrieving method.
   ADVANTAGE - The method enables combining the salient map and the convolutional neural network so as to simultaneously obtain information of an obvious area and a background area, extract effective feature representation from the synthesized characteristic of the convolution layer, reduce remote sensing image retrieving cost and noise interference and realize robustness of the synthesized characteristic of the convolution layer.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a salient map and convolutional neural network based remote sensing image retrieving system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a salient map and convolutional neural network based remote sensing image retrieving method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J05B4P; T01-J10B3A; T01-N03A2
IP G06F-017/30; G06N-003/04; G06N-003/08
PD CN109086405-A   25 Dec 2018   G06F-017/30   201914   Pages: 10   Chinese
AD CN109086405-A    CN10862331    01 Aug 2018
PI CN10862331    01 Aug 2018
UT DIIDW:201901480K
ER

PT P
PN CN109087273-A
TI Neural network based method for restoring enhanced image by using image restoring system, involves performing fusion process of to-be-restored image, and performing linear transformation of image after normalizing characteristics.
AU TIAN C
   XU Y
AE HARBIN INST TECHNOLOGY SHENZHEN GRADUATE (HAIT-C)
GA 201901459D
AB    NOVELTY - The method involves converting to-be-restored image into multiple low-resolution images under different scaling factors (S1). The low-resolution images are input (S2) to a first depth convolutional neural network for obtaining multiple high-resolution images under the different scaling factors. The high-resolution images are converted (S3) into the to-be-restored image in same size. Fusion process of the to-be-restored image is performed to obtain a restoration image. Linear transformation of the to-be-restored image is performed after normalizing characteristics. The restoration image is input (S4) into a second deep convolutional neural network to obtain an optimized image.
   USE - Neural network based method for restoring an enhanced image by using an image restoring system (claimed).
   ADVANTAGE - The method enables preventing network degradation problems and accelerating convergence speed during training process.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a storage medium comprising a set of instructions for restoring an enhanced image by using an image restoring system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for restoring an enhanced image. '(Drawing includes non-English language text)'
   Step for converting to-be-restored image into multiple low-resolution images under different scaling factors (S1)
   Step for inputting low-resolution images to first depth convolutional neural network for obtaining multiple high-resolution images under different scaling factors (S2)
   Step for converting high-resolution images into to-be-restored image in same size (S3)
   Step for inputting restoration image into second deep convolutional neural network to obtain optimized image (S4)
DC T01 (Digital Computers)
MC T01-J10B1; T01-J10D
IP G06T-005/50; G06T-005/00; G06N-003/04; G06N-003/08
PD CN109087273-A   25 Dec 2018   G06T-005/50   201914   Pages: 16   Chinese
AD CN109087273-A    CN10803674    20 Jul 2018
PI CN10803674    20 Jul 2018
UT DIIDW:201901459D
ER

PT P
PN CN109086708-A
TI Method for detecting parking space for vehicle, involves setting tag mark coordinates of target area in parking target image, and initializing parameter of neural network based on training set and tag information.
AU TIAN Y
   QIAN L
   TIAN J
AE UNIV SHENZHEN (UYSZ-C)
GA 2019014733
AB    NOVELTY - The method involves obtaining a parking target image, and setting a training set and a test set for an object of interest in the parking target image. Tag mark coordinates of a target area are set, and a parameter of a neural network is initialized based on the training set and tag information. The training set is transmitted to the neural network, and training results are quantitatively evaluated, where the target area includes a corner region and/or a target parking space. The parameter is analyzed and modified if training result does not satisfy a predetermined requirement.
   USE - Method for detecting a parking space based on deep learning for a vehicle.
   ADVANTAGE - The tag mark coordinates of the target area in the parking target image are set, and the parameter of the neural network is initialized based on the training set and the tag information, thus allowing automatic and accurate detection of parking space, and hence achieving convenient parking of a vehicle while improving the user experience.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a parking space detecting system based on deep learning.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for detecting a parking space based on deep learning for a vehicle. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T07 (Traffic Control Systems)
MC T01-J07D1; T01-J10B2; T01-N01B3A; T01-N01D1B; T07-F
IP G06K-009/00; G06K-009/62; G06N-003/08
PD CN109086708-A   25 Dec 2018   G06K-009/00   201914   Pages: 8   Chinese
AD CN109086708-A    CN10824584    25 Jul 2018
PI CN10824584    25 Jul 2018
UT DIIDW:2019014733
ER

PT P
PN CN109086201-A
TI Software automatic testing method, involves establishing two-channel convolutional neural network model to compare test results with expected results, and analyzing entire test results to identify problem for generating test report.
AU WANG J
   SUN D
   LV Z
   YUAN S
   NAN Y
   LI Y
   HAO W
   WANG X
AE BEIJING SUGON INFORMATION IND CO LTD (SGII-C)
GA 201901485J
AB    NOVELTY - The method involves discovering current versions of software through a version push mechanism and automatically updated test environment. Test for test tool is prepared by test data and test cases. Automated test framework is called. Test suite configuration information is automatically generated according to source code of the test cases to execute the test cases and stored screenshots of operation and test results. A two-channel convolutional neural network model is established to compare the test results with expected results to obtain similarity results. The entire test results are analyzed to identify problem for generating test report.
   USE - Software automatic testing method.
   ADVANTAGE - The method enables realizing software automation test according to deep learning to increase accuracy of the test results.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a software automatic testing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a software automatic testing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-F05F; T01-J20C; T01-N03; T01-S03
IP G06F-011/36
PD CN109086201-A   25 Dec 2018   G06F-011/36   201914   Pages: 12   Chinese
AD CN109086201-A    CN10775767    16 Jul 2018
PI CN10775767    16 Jul 2018
UT DIIDW:201901485J
ER

PT P
PN CN109085851-A
TI Unmanned aircraft point landing method, involves inputting position information to unmanned aircraft control part to adjust position of unmanned aircraft, and performing unmanned aircraft landing process in landing area.
AU WANG X
   ZHOU Z
   WANG C
   ZHAO Y
AE HARBIN INST TECHNOLOGY (HAIT-C)
GA 201901493P
AB    NOVELTY - The method involves fixing multiple infrared beacon modules in a landing area. Images of the infrared beacon module are obtained through an infrared camera module by adopting deep learning algorithm. Image recognition process is performed by a neural network training model. A camera input image is input to a neural network loading model. Image processing operation is performed to obtain parking position information. The parking position information is input to an unmanned aircraft control part to adjust a position of the unmanned aircraft. Unmanned aircraft landing process is performed in the landing area.
   USE - Unmanned aircraft point landing method.
   ADVANTAGE - The method enables improving landing precision and landing reliability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an infrared camera module. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T06 (Process and Machine Control)
MC T01-J07D3A; T01-J10B2A; T01-J16C2; T01-N01B3; T06-B01X
IP G05D-001/10; G06N-003/08
PD CN109085851-A   25 Dec 2018   G05D-001/10   201914   Pages: 8   Chinese
AD CN109085851-A    CN11063313    12 Sep 2018
PI CN11063313    12 Sep 2018
UT DIIDW:201901493P
ER

PT P
PN CN109086828-A
TI Method for detecting battery pole piece of electronic device, involves displaying input image by pixels, and utilizing indicator for indicating non-burr of first class and second class after representing burr.
AU WEN Y
   LENG J
   LIU M
   GUO J
   LI X
AE BEIJING BAIDU NETCOM SCI & TECHNOLOGY CO (BIDU-C)
GA 2019014702
AB    NOVELTY - The method involves obtaining an image of a battery pole piece when the image meets a pre-set detecting condition. Detection result information is obtained. A battery pole piece burr detecting system is fixed with a convolutional neural network based semantic segmentation. The detection result information is utilized for indicating type of content. An input image is displayed by pixels. An indicator is utilized for indicating non-burr of first class and second class after representing burr. The image is inputted to a pre-trained battery pole piece burr detecting system.
   USE - Method for detecting a battery pole piece of an electronic device (claimed).
   ADVANTAGE - The method enables detecting battery pole piece burr based on semantic segmentation of a convolutional neural network.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for detecting a battery pole piece of an electronic device
   (2) a computer readable medium for comprising a set of instruction for detecting a battery pole piece of an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for detecting a battery pole piece of an electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J16C3; T01-N01D1B; T01-N01D2; T01-S03; T04-D04
IP G06K-009/62; G06N-003/04
PD CN109086828-A   25 Dec 2018   G06K-009/62   201914   Pages: 17   Chinese
AD CN109086828-A    CN10907540    10 Aug 2018
PI CN10907540    10 Aug 2018
UT DIIDW:2019014702
ER

PT P
PN CN109086778-A
TI Convolutional neural network based number plate de-fuzzy identifying method, involves inputting number plate data into de- fuzzy network, and inputting output image into recognition network for outputting number plate attribute content.
AU WU C
   LI F
   SHI Z
AE BEIJING YISA TECHNOLOGY CO LTD (BEIJ-Non-standard)
   QINGDAO YISA DATA TECHNOLOGY CO LTD (QING-Non-standard)
GA 201901471A
AB    NOVELTY - The method involves collecting number plate data. De-fuzzy process is performed on a training part corresponding to the number plate data. Fuzzy number plate data is identified as training input data. A number plate image is output by selecting the training input data as a fuzzy number plate image. Number plate attribute label is identified. The number plate is identified by using convolutional neural network and short-term memory network. Original number plate data is input into trained de-fuzzy network. An output image is input into a number plate recognition network for outputting the number plate attribute content in real-time.
   USE - Convolutional neural network based number plate de-fuzzy identifying method.
   ADVANTAGE - The method enables efficiently and effectively completing identification of color, character and type of the number plate through mutual cooperation of block functions so as to save manpower and material costs, thus improving identifying accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a number plate attribute label.
DC T01 (Digital Computers)
MC T01-E01B; T01-J10B2A; T01-J10B3B; T01-N01B3; T01-N01D2
IP G06K-009/46; G06K-009/34; G06N-003/04
PD CN109086778-A   25 Dec 2018   G06K-009/46   201914   Pages: 9   Chinese
AD CN109086778-A    CN10843719    27 Jul 2018
PI CN10843719    27 Jul 2018
UT DIIDW:201901471A
ER

PT P
PN CN109087691-A
TI Deep learning based system for recommending non-prescription medicine to patient, has user information receiving module for obtaining output result of recommendation model as recommendation result, which is associated with medical record.
AU XU Z
   YANG H
   ZHANG K
   SONG J
AE CSG SMART ROBOT TECHNOLOGY CO LTD (CSGS-Non-standard)
GA 2019014505
AB    NOVELTY - The system has a disease database for pre-storing multiple types of disease information. A symptom database stores symptoms characteristic of each disease information. A prescription drug library stores the medicine information corresponding to the disease information, and related information between the medicine information and the disease information. A data collecting module collects user information from an electronic medical record such that the user information is pre-processed and output, and connected with a depth learning data training module. A user information receiving module transmits user input information as input data to a medicine recommendation model, and obtains an output result of the medicine recommendation model as medicine recommendation result, which is associated with the electronic medical record.
   USE - Deep learning based system for recommending non-prescription medicine to a patient.
   ADVANTAGE - The system can recommend suitable medicine to the patient according to the type of disease symptoms and characteristic identification of the patient.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning based non-prescription medicine recommendation method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a deep learning based system for recommending non-prescription medicine to a patient. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC S05-G02G1; S05-G02G4; S05-G02G9; T01-J05B4B; T01-J06A1; T01-J30A; W04-W05A
IP G16H-020/10; G16H-050/70
PD CN109087691-A   25 Dec 2018   G16H-020/10   201914   Pages: 17   Chinese
AD CN109087691-A    CN10873485    02 Aug 2018
PI CN10873485    02 Aug 2018
UT DIIDW:2019014505
ER

PT P
PN CN109087317-A
TI Pulmonary nodule image segmentation method, involves binarizing pulmonary nodule area by using threshold process, and randomly selecting seed points in main area of pulmonary nodules, and segmenting lung nodule by regional growing process.
AU XUE J
   TAN Y
   LV K
   DONG J
AE UNIV CHINESE ACAD SCI (UYCH-Non-standard)
GA 2019014587
AB    NOVELTY - The method involves marking chest CT images to obtain a marked CT image data set. A pulmonary nodule detection convolutional neural network model is constructed to input the CT image data set into the pulmonary nodule detection convolutional neural network model. A hyper-parameter of the pulmonary nodule detection convolutional neural network is set. The CT image data set is input into a training model to output lung nodule location information after completing training. A pulmonary nodule area is binarized by using threshold process to determine a main area of the pulmonary nodule. Seed points are randomly selected in the main area of the pulmonary nodules. The lung nodule is segmented by using regional growing process.
   USE - Pulmonary nodule image segmentation method.
   ADVANTAGE - The method enables accurately and automatically segmenting pulmonary nodules so as to avoid difficulties of treatment.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a pulmonary nodule image segmentation method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-L02; T01-N01B3
IP G06T-007/11; G06T-007/136; G06N-003/04
PD CN109087317-A   25 Dec 2018   G06T-007/11   201914   Pages: 10   Chinese
AD CN109087317-A    CN11347355    13 Nov 2018
PI CN11347355    13 Nov 2018
UT DIIDW:2019014587
ER

PT P
PN CN109087703-A
TI Deep convolutional neural network based abdominal CT image peritoneal metastasis mark method, involves preprocessing and segmenting CT, predicting probability using trained candidate nodule model, and outputting determined mark in CT image.
AU XUE Y
   DU J
   LIU S
   GU Q
AE UNIV NANJING (UNAJ-C)
   NANJING DRUM TOWER HOSPITAL (NANJ-Non-standard)
GA 201901449T
AB    NOVELTY - The method involves preprocessing and segmenting a CT image. The segmenting image in candidate nodules is extracted. A deep convolutional neural network model is constructed and trained. Probability of a tumor nodule is predicted by using the trained candidate nodule model. A determined mark in the CT image is output. An abdominal CT image is read. The pre-processed CT image is divided by adopting watershed process to obtain particle images. Grain image is combined. Loss functions are set. CT image determination flag is output. CT value is set into a set gradation level.
   USE - Deep convolutional neural network based abdominal CT image peritoneal metastasis mark method.
   ADVANTAGE - The method enables finishing the automatic marking of abdominal CT image peritoneal metastasis to provide basis for malignant tumor diagnosis and treatment easy to understand automatic mark with simple implementation and suitable for massive abdominal CT image, thus increasing better expansibility and robustness and practicability of the CT image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep convolutional neural network based abdominal CT image peritoneal metastasis mark method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D02A1; S05-D06; T01-J10B2; T01-N01E1
IP G16H-050/20; G06K-009/62; G06N-003/04
PD CN109087703-A   25 Dec 2018   G16H-050/20   201914   Pages: 17   Chinese
AD CN109087703-A    CN10972458    24 Aug 2018
PI CN10972458    24 Aug 2018
UT DIIDW:201901449T
ER

PT P
PN CN109087294-A
TI Product defect detecting method, involves obtaining target area image of to-be-detected product, establishing recognition model according to preset depth learning, and detecting defects of to-be-detected product in target area image.
AU YAO J
   LI C
   FENG L
AE SHENZHEN COSMOSVISION INTELLIGENT TECHNO (SHEN-Non-standard)
GA 201901458V
AB    NOVELTY - The method involves obtaining an original image of a to-be-detected product. Image processing process is performed on an obtained original image. A target area image for an area image is obtained according to preset detection algorithm. The target area image is received by a server. A recognition model is established by the server according to preset depth learning. Defects identification process is performed on the target area image of the to-be-detected product to obtain an identification result, where the preset detection algorithm is linear section detecting algorithm.
   USE - Product defect detecting method.
   ADVANTAGE - The method enables improving continuity and real-time performance of product defect identification process and providing basis for industrial operation.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a product defect detection system
   (2) a computer-readable storage medium for storing a set of instructions for detecting product defect.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a product defect detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2A; T01-N01B3; T01-N02A3C; T01-S03; T04-D07A
IP G06T-007/00
PD CN109087294-A   25 Dec 2018   G06T-007/00   201914   Pages: 19   Chinese
AD CN109087294-A    CN10857307    31 Jul 2018
PI CN10857307    31 Jul 2018
UT DIIDW:201901458V
ER

PT P
PN CN109086773-A
TI Convolutional neural network based fault plane identifying method, involves training convolution neural network model, and identifying to-be-identified seismic amplitude data by using obtained convolutional neural network training model.
AU YAO X
   HUANG L
   HU G
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 201901471F
AB    NOVELTY - The method involves obtaining marked seismic amplitude data and to-be-identified seismic amplitude data from a three-dimensional (3D) seismic amplitude data volume. A convolution neural network model is established. The convolution neural network model is trained to obtain a complete convolution neural network training model by using the to-be-identified seismic amplitude data as a training set. The to-be-identified seismic amplitude data is identified by using the obtained convolutional neural network training model. The convolution neural network model is formed with multiple convolution layers for extracting earthquake amplitude image characteristics.
   USE - Convolutional neural network based fault plane identifying method.
   ADVANTAGE - The method enables realizing division of the three-dimensional earthquake amplitude data for fault identification in a quick manner by using the convolution neural network, and providing a connected layer to avoid loss of spatial information by using full convolution operation so as to obtain accurate fault segmentation and identification.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based fault plane identifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J05B4P; T01-J10B2; T01-N01B3
IP G06K-009/34; G06N-003/04; G06F-017/30
PD CN109086773-A   25 Dec 2018   G06K-009/34   201914   Pages: 13   Chinese
AD CN109086773-A    CN10995076    29 Aug 2018
PI CN10995076    29 Aug 2018
UT DIIDW:201901471F
ER

PT P
PN CN109086469-A
TI Recurrent neural network and preference information based aluminum electrolytic modeling and optimizing method, involves optimizing aluminum electrolytic cell by selection of electrolysis industry field according to decision variable.
AU YI J
   BAI J
   WU L
   CHEN X
   ZHOU W
   CHEN S
AE UNIV CHONGQING SCI & TECHNOLOGY (UYNQ-C)
GA 2019014790
AB    NOVELTY - The method involves determining control parameters of an aluminum electrolytic cell (S1). Aluminum electrolysis industry field is selected (S2). Four aluminum electrolytic cell manufacturing process models are established using a recurrent neural network. Strict partial order relationship between the aluminum electrolytic cell manufacturing process models are established (S3) by using a preference target differential evolution algorithm. The aluminum electrolytic cell is optimized (S4) by selection of the aluminum electrolysis industry field according to optimal decision variable.
   USE - Recurrent neural network and preference information based aluminum electrolytic modeling and optimizing method.
   ADVANTAGE - The method enables determining optimal value of process parameters in aluminum electrolysis production process to effectively improve current efficiency and reduce cell voltage and greenhouse gas emissions and energy consumption per ton of aluminum and satisfying preferences of decision makers so as to realize purpose of energy saving and emission reduction.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a recurrent neural network and preference information based aluminum electrolytic modeling and optimizing method. '(Drawing includes non-English language text)'
   Step for determining control parameters of an aluminum electrolytic cell (S1)
   Step for selecting aluminum electrolysis industry field (S2)
   Step for establishing strict partial order relationship between the aluminum electrolytic cell manufacturing process models by using a preference target differential evolution algorithm (S3)
   Step for optimizing the aluminum electrolytic cell by selection of the aluminum electrolysis industry field according to optimal decision variable (S4)
DC T01 (Digital Computers)
MC T01-J15X; T01-J16C4
IP G06F-017/50; G06N-003/00; G06N-003/04
PD CN109086469-A   25 Dec 2018   G06F-017/50   201914   Pages: 18   Chinese
AD CN109086469-A    CN10193122    09 Mar 2018
PI CN10193122    09 Mar 2018
UT DIIDW:2019014790
ER

PT P
PN CN109087327-A
TI Cascade full convolution neural network thyroid nodule ultrasound image segmenting method, involves restoring up-sampled characteristic pattern to original image resolution to supplement shallow detail information.
AU YING X
   WEI Z
   YU J
   ZHAO M
   XU T
   GAO J
AE UNIV TIANJIN (UTIJ-C)
GA 201901457X
AB    NOVELTY - The method involves constructing a simple full convolutional neural network based on U-Net. Thyroid ultrasound data in an ultrasonic image is semantically segmented based on the simple full convolutional neural network. Deep features of a region of interest of the simple full convolutional neural network are extracted by using a VGG19 network as a down-sampling layer for realizing automatic semantic segmentation process of thyroid nodules. Down-sampling process is performed in an input picture to process the deep features by the simple full convolutional neural network. Pixel classification process is performed in different type pixels neural networks. A down-sampled characteristic pattern is sequentially interpolated with an up-sampled characteristic pattern. The up-sampled characteristic pattern is restored to original image resolution to supplement shallow detail information.
   USE - Cascade full convolution neural network thyroid nodule ultrasound image segmenting method.
   ADVANTAGE - The method enables improving accuracy of nodules image and realizing identification of benign and malignant thyroid nodules so as to effectively ensure optimal auxiliary role in medical diagnosis process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a cascade full convolution neural network. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J05B2; T01-J10B1; T01-J10B2; T01-J10D; T01-J16C3; T01-N01E1
IP G06T-007/181; G06T-007/11; G06N-003/08; G06N-003/04
PD CN109087327-A   25 Dec 2018   G06T-007/181   201914   Pages: 13   Chinese
AD CN109087327-A    CN10772176    13 Jul 2018
PI CN10772176    13 Jul 2018
UT DIIDW:201901457X
ER

PT P
PN CN109086781-A
TI Deep learning based cabinet lamp state identifying method, involves initializing model parameter, selecting detecting whether number of lamp corresponding to threshold value, and identifying state of cabinet lamp.
AU YU G
   ZHANG Y
   ZHANG S
   NIU H
   ZHANG L
AE UNIV BEIHANG (UNBA-C)
GA 2019014717
AB    NOVELTY - The method involves initializing model parameter. Cabinet lamp state of a picture is obtained corresponding to the model parameter. A picture of a lamp group is identified corresponding to depth learning information. Different identification target is established according to different machine state set. A judgment is made to check whether abnormal threshold value is matched with preset threshold value. A profile searching algorithm is established. A lamp group number of an actual working cabinet lamp is obtained. A detection is made to check whether the number of the lamp is selected corresponding to the threshold value. State of the cabinet lamp is identified.
   USE - Deep learning based cabinet lamp state identifying method.
   ADVANTAGE - The method enables identifying cabinet lamp state in an automatic manner, and increasing working efficiency and precision.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning based cabinet lamp state identifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B3; T01-J10B2; T04-D03; T04-D04
IP G06K-009/46; G06K-009/62; G06N-003/08
PD CN109086781-A   25 Dec 2018   G06K-009/46   201914   Pages: 10   Chinese
AD CN109086781-A    CN10926063    15 Aug 2018
PI CN10926063    15 Aug 2018
UT DIIDW:2019014717
ER

PT P
PN CN109086134-A
TI Method for running deep learning operation in cluster, involves mapping hardware resources allocated by computing node to docker image, and running deep learning task by using hardware resources mapped to docker image.
AU YUAN S
AE ZHENGZHOU YUNHAI INFORMATION TECHNOLOGY (INEI-C)
GA 2019014878
AB    NOVELTY - The method involves receiving a user selection of resources required to run a deep learning job and a selection of a docker image for submitting a deep learning job. The deep learning job is scheduled according to usage and load conditions of a computing node. A docker image selected by the user is pushed from an image repository when the deep learning job is scheduled to the compute node. A docker container is created on each compute node in a cluster. Hardware resources allocated by the computing node are mapped to the docker image according to the deep learning job. Deep learning task is run by using the hardware resources mapped to the docker image and the docker container.
   USE - Method for running deep learning operation in a cluster.
   ADVANTAGE - The method enables reducing time and effort spent by the user to run the deep learning operation in the cluster.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for operating a deep learning operation in a cluster.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for running deep learning operation in cluster. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-F02C1; T01-F02C2; T01-F03; T01-F05G3; T01-J05B3; T01-J30A; W04-W05A
IP G06F-009/50; G06F-009/455; G06F-017/30; G06N-003/04; G06N-003/08
PD CN109086134-A   25 Dec 2018   G06F-009/50   201914   Pages: 10   Chinese
AD CN109086134-A    CN10793520    19 Jul 2018
PI CN10793520    19 Jul 2018
UT DIIDW:2019014878
ER

PT P
PN CN109086796-A
TI Method for identifying brand image by mobile terminal, involves inputting scale features to classifier network for identifying brand, and training classifier network corresponding to architecture by multiple size features.
AU ZHANG G
AE OPPO CHONGQING INTELLIGENT TECHNOLOGY CO (OPPO-Non-standard)
GA 201901470X
AB    NOVELTY - The method involves obtaining a mark image. The mark image is input to a residual network for extracting size characteristics of the mark image. The residual network is trained based on a convolutional neural network architecture and multiple symbol samples to obtain multiple brand samples, where each symbol sample comprises each brand sample. Scale features are input to a classifier network for identifying brand corresponding to the mark image. The classifier network is trained corresponding to the convolutional neural network architecture by multiple different size features. The residual network is selected as an nResNet-50 network.
   USE - Method for identifying a brand image by a mobile terminal (claimed).
   ADVANTAGE - The method enables feature extraction and object detection by a convolutional neural network for constructing a brand identifying system to help consumers to identify knowledge of a brand.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for identifying a brand image by a mobile terminal
   (2) a processor-readable storage medium for storing set of instructions for identifying a brand image by a mobile terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for identifying a brand image by a mobile terminal. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-N01B3; T01-S03; T04-D04
IP G06K-009/62; G06N-003/04
PD CN109086796-A   25 Dec 2018   G06K-009/62   201914   Pages: 18   Chinese
AD CN109086796-A    CN10679969    27 Jun 2018
PI CN10679969    27 Jun 2018
UT DIIDW:201901470X
ER

PT P
PN CN109086679-A
TI Method for detecting millimeter wave radar foreign matter using inspection instrument, involves determining anomaly type according to detection result of angle imaging image and multi-angle imaging result.
AU ZHANG X
AE XIAN HENGFAN ELECTRONIC TECHNOLOGY CO (XIAN-Non-standard)
GA 201901473S
AB    NOVELTY - The method involves designing a multi-layer convolution neural network based on deep learning. Multi-angle imaging result is collected by a millimeter wave radar inspection instrument body based on a GPU platform. A training sample data set is established according to a mark foreign VOC data format name and a position. The obtained training sample data set is sent to a target detection network. The multi-angle imaging result is sent to an object detection network. Anomaly type is determined according to a detection result of each angle imaging image and the multi-angle imaging result.
   USE - Method for detecting millimeter wave radar foreign matter using an inspection instrument.
   ADVANTAGE - The method enables accurately identifying and marking millimeter wave radar foreign matter by the inspection instrument.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for detecting millimeter wave radar foreign matter using an inspection instrument. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W06 (Aviation, Marine and Radar Systems)
MC T01-J04B2; T01-J10B2; T01-N01B3; W06-A04
IP G06K-009/00; G06K-009/62; G06N-003/04
PD CN109086679-A   25 Dec 2018   G06K-009/00   201914   Pages: 11   Chinese
AD CN109086679-A    CN10749762    10 Jul 2018
PI CN10749762    10 Jul 2018
UT DIIDW:201901473S
ER

PT P
PN CN109084613-A
TI Convolutional neural network and image recognition based condenser dust cleaning state monitoring and controlling system, has condenser row pipe base pipe whose outer side is connected with steam distributing pipe and condensed water tank.
AU ZHAO B
   REN R
   CAO S
   ZHOU Y
   GAO Y
   WANG X
   FAN S
   SUN T
   WANG G
   LV C
AE UNIV NORTHEAST DIANLI (UNEE-C)
GA 201901522K
AB    NOVELTY - The system has a steam turbine (1) whose output end is connected with an input end of a steam distributing pipe (3). An output end of the steam distributing pipe is connected with an input end of a condensed water pump (7) through a condenser row pipe base pipe (4). The condenser row pipe base pipe is connected with an input end of a condensing water tank (5). A first visible light camera (14) is fixed to an inner side of the condenser row pipe base pipe. A second visible light camera (15) is fixed into an inner side of the condenser row pipe base pipe. The first visible light camera and the second visible light camera are connected with an input end of an image collector (10). An outer side of the condenser row pipe base pipe is connected with the steam distributing pipe and the condensed water tank.
   USE - Convolutional neural network and image recognition based condenser dust cleaning state monitoring and controlling system.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a convolutional neural network and image recognition based condenser dust cleaning state monitoring and controlling method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a pneumatic diagram of a convolutional neural network and image recognition based condenser dust cleaning state monitoring and controlling system.
   Steam turbine (1)
   Steam distributing pipe (3)
   Condenser row pipe base pipe (4)
   Condensing water tank (5)
   Condensed water pump (7)
   Image collector (10)
   Visible light cameras (14, 15)
DC Q78 (Heat exchange in general (F28)); T01 (Digital Computers)
MC T01-J10B2A; T01-N01D1B; T01-N02B2B
IP F28G-015/00
PD CN109084613-A   25 Dec 2018   F28G-015/00   201914   Pages: 15   Chinese
AD CN109084613-A    CN11059723    12 Sep 2018
PI CN11059723    12 Sep 2018
UT DIIDW:201901522K
ER

PT P
PN CN109087302-A
TI Fundus image vessel segmentation method, involves training neural network model by using neural network training set, capturing target fundus image, and performing vessel segmentation operation on neural network model.
AU ZHAO L
   WANG S
AE BEIJING DAHENG PRUST MEDICAL TECHNOLOGY (BEIJ-Non-standard)
GA 201901458N
AB    NOVELTY - The method involves constructing a neural network training set for performing an eyeground image construction operation and a neural network training operation. An amplification operation of the neural network training set is performed. A neural network model is established for performing an eyebase image vascular segmentation operation based on a deep learning network. The neural network model is trained by using the neural network training set. A target fundus image is captured. A vessel segmentation operation is performed on the neural network model by training the target fundus image.
   USE - Fundus image vessel segmentation method.
   ADVANTAGE - The method enables simplifying a network structure, reducing loss of accuracy rate, and improving cutting speed.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a fundus image vessel segmentation device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a fundus image vessel segmentation method.
DC T01 (Digital Computers)
MC T01-J10B2; T01-N01B3
IP G06T-007/10
PD CN109087302-A   25 Dec 2018   G06T-007/10   201914   Pages: 13   Chinese
AD CN109087302-A    CN10885166    06 Aug 2018
PI CN10885166    06 Aug 2018
UT DIIDW:201901458N
ER

PT P
PN CN109086678-A
TI Pedestrian detecting method, involves training pedestrian detection network by adopting RMSprop learning strategy, and optimizing and accelerating forward inference stage of pedestrian detection network.
AU ZHAO M
   HE Y
   ZHENG Y
   HUANG Y
AE UNIV TIANJIN (UTIJ-C)
GA 201901473T
AB    NOVELTY - The method involves constructing an infrared pedestrian detection training set and a testing data set. A depth learning frame is constructed based on depth-supervised learning. A pedestrian detection network is trained by adopting RMSprop learning strategy. Forward inference stage of the pedestrian detection network to optimized and accelerated. The infrared pedestrian detection training set and the testing data set are processed by an Elektra Research Center. Infrared image of marked data is modified. A CVC-14 data set and a CVC-09 data set are merged into consistent data.
   USE - Deep-supervised learning extracted multi-level image features based pedestrian detecting method.
   ADVANTAGE - The method enables avoiding need for utilizing a pre-training model and improving pedestrian detection accuracy based on far infrared image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep-supervised learning extracted multi-level image features based pedestrian detecting method.
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-E01C; T01-J10B2; T01-N01B3A; T04-D07K; W04-W05A
IP G06K-009/00; G06N-003/04; G06N-003/08
PD CN109086678-A   25 Dec 2018   G06K-009/00   201914   Pages: 10   Chinese
AD CN109086678-A    CN10746609    09 Jul 2018
PI CN10746609    09 Jul 2018
UT DIIDW:201901473T
ER

PT P
PN CN109086648-A
TI Feature matching and target detecting fusion-target tracking method, involves determining feature of to-be-tracked target, and determining position of undetermined target with maximum similarity as tracked target in current video frame.
AU ZHAO X
   LI Z
AE UNIV TONGJI (UYTJ-C)
GA 201901474D
AB    NOVELTY - The method involves performing target tracking operation according to a convolutional neural network. A to-be-tracked target is obtained. A current video frame is detected by utilizing a trained object detection network. A frame position of an undetermined target is determined. Feature matching operation is performed. Feature of the to-be-tracked target is determined by utilizing a local feature descriptor matching network. The position of the undetermined target with maximum similarity is determined as the to-be-tracked target in the current video frame.
   USE - Feature matching and target detecting fusion-target tracking method.
   ADVANTAGE - The method enables increasing tracking precision and speed.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a feature matching and target detecting fusion-target tracking method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T04-D04; T04-D07D5
IP G06K-009/00; G06K-009/62
PD CN109086648-A   25 Dec 2018   G06K-009/00   201914   Pages: 10   Chinese
AD CN109086648-A    CN10509677    24 May 2018
PI CN10509677    24 May 2018
UT DIIDW:201901474D
ER

PT P
PN CN109086768-A
TI Convolutional neural network based semantic image segmentation method, involves processing segmentation of semantic image in network framework by combining part of hierarchical feature fusion and fully connected conditional random field.
AU ZHOU Q
   YANG W
   CONG D
   WANG Y
   LU J
AE UNIV NANJING POSTS & TELECOM (UNPT-C)
GA 201901471L
AB    NOVELTY - The method involves processing segmentation of a semantic image in a network framework by combining a part of hierarchical feature fusion and a fully connected conditional random field. A network architecture model consisting of a convolution layer, a pool, an up sampling layer and loss function is constructed. Step side of each layer in the network architecture model, size of a convolution core and number of feature maps output are customized according to specifications. A character image is output and divided to perform post-processing operation.
   USE - Convolutional neural network based semantic image segmentation method.
   ADVANTAGE - The method enables carrying feature fusion of different layers, fully utilizing information of each layer, combining fully connected CRF to post-process a network output result to improve final accuracy, and determining relationship between pixel points so as to ensure image segmentation result in accurate and smooth manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a convolutional neural network based semantic image segmentation structure. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B2; T01-J16C3; T04-D02; T04-D04
IP G06K-009/34; G06K-009/62
PD CN109086768-A   25 Dec 2018   G06K-009/34   201914   Pages: 8   Chinese
AD CN109086768-A    CN10768753    13 Jul 2018
PI CN10768753    13 Jul 2018
UT DIIDW:201901471L
ER

PT P
PN CN109063753-A
TI Convolutional neural network based three-dimensional point cloud model classification method, involves generating training set, and classifying convolutional neural network structure for two-dimensional image based on point cloud model.
AU BAI J
   SI Q
   LIU Z
AE UNIV NORTH MINZU (UYNM-Non-standard)
GA 2019006541
AB    NOVELTY - The method involves selecting Princeton ModelNet for ModelNet10 and ModelNet40 from a website. Desired number of a model is selected as training data and test data. Training set and data set are generated. A point cloud model is established to perform a feature analysis process. A classification frame constructing process is performed. A point cloud data ordering process is performed. A two-dimensional image of the ordered point cloud data is obtained. A service-oriented convolutional neural network (CNN) network of two-dimensional image is constructed. A CNN structure for the two-dimensional image is classified based on the point cloud model.
   USE - CNN based three-dimensional point cloud model classification method.
   ADVANTAGE - The method enables improving three-dimensional point cloud model classification feasibility and capturing point cloud of three-dimensional feature information in an effective manner, which is suitable for cloud model classification.
   DESCRIPTION OF DRAWING(S) - The drawing shows a process flow diagram illustrating a CNN based three-dimensional point cloud model classification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-E01A; T01-E01B; T01-J05B2; T01-J10B2; T01-N01B3A; T04-D04
IP G06K-009/62; G06N-003/04
PD CN109063753-A   21 Dec 2018   G06K-009/62   201914   Pages: 23   Chinese
AD CN109063753-A    CN10790133    18 Jul 2018
PI CN10790133    18 Jul 2018
UT DIIDW:2019006541
ER

PT P
PN CN109063568-A
TI Depth learning based figure skating video automatic scoring method, involves constructing loss function, training deep neural network model, and obtaining prediction technology total TES and program content score by neural network model.
AU FU Y
   XU C
   JIANG Y
   XUE X
AE UNIV FUDAN (UYFU-C)
GA 201900658V
AB    NOVELTY - The method involves collecting figure skating video data from different players and different high-grade events. An umpire professional scoring operation is performed. Video is pre-processed for extracting low signature sequence. A video-based signature sequence depth neural network of an attention module and a multi-scale convolution module is constructed to obtain higher order. Shorter weighted sequence is obtained. Jump memory network skip-long short-term memory (LSTM) treatment two sequence is obtained. Loss function is constructed. A deep neural network model is trained. Prediction technology total TES and program content score is obtained by the neural network model.
   USE - Depth learning based figure skating video automatic scoring method.
   ADVANTAGE - The method enables improving precision and robustness.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a multi-scale convolution module. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B2; T01-N01B3; T01-S03; T04-D04
IP G06K-009/00; G06K-009/62
PD CN109063568-A   21 Dec 2018   G06K-009/00   201914   Pages: 11   Chinese
AD CN109063568-A    CN10721097    04 Jul 2018
PI CN10721097    04 Jul 2018
UT DIIDW:201900658V
ER

PT P
PN CN109064514-A
TI Projection point coordinate regression-based six-degree-of-freedom pose estimation algorithm, has set of instructions to obtain result of image target instance to judge whether relation between points is determined according to camera pose.
AU JIANG Z
   ZHANG H
   ZHANG X
   ZHAO D
   XIE F
AE UNIV BEIHANG (UNBA-C)
GA 201900635P
AB    NOVELTY - The algorithm has set of instructions to input a RGB image of a target three-dimensional model to calculate target size information. Coordinates of a top point of an object three-dimensional boundary frame is obtained. RGB image predicting is performed by a convolutional neural network to obtain type of each position feature map of six different scales on category score. A projection point coordinates regression result is obtained. Target class-based score is measured. Two-dimensional bounding box non-maximum suppression is performed. A regression result of image target instance is obtained to judge whether relation between two-dimensional and three-dimensional points is determined according to six-degrees-of-freedom camera pose.
   USE - Projection point coordinate regression-based six-degree-of-freedom pose estimation algorithm.
   ADVANTAGE - The method enables reducing running time of algorithm.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a projection point coordinate regression-based six-degree-of-freedom pose estimation algorithm. '(Drawing includes non-English language text)'
DC T04 (Computer Peripheral Equipment)
MC T04-D08
IP G06T-007/73
PD CN109064514-A   21 Dec 2018   G06T-007/73   201914   Pages: 17   Chinese
AD CN109064514-A    CN10717442    03 Jul 2018
PI CN10717442    03 Jul 2018
UT DIIDW:201900635P
ER

PT P
PN CN109063712-A
TI Ultrasonic image based multi-model intelligent diffuse liver disease diagnosing method, involves performing final classification of diffuse liver disease based on model characteristics by using XGBoost classification algorithm.
AU LI D
   LI J
   LI X
   SHEN Y
AE HARBIN INST TECHNOLOGY (HAIT-C)
GA 2019006554
AB    NOVELTY - The method involves performing a liver ultrasonic image pre-processing process by using traditional histogram equalization algorithm. Depth application of a convolutional neural network is determined based on framework depth learning algorithm to achieve initial classification of diffuse liver disease. Image texture characteristics are extracted by convolutional neural network combining characteristics to obtain multiple model characteristics. Final classification of the diffuse liver disease is performed based on model characteristics by using XGBoost classification algorithm. An ultrasonic image inputting process is performed.
   USE - Ultrasonic image based multi-model intelligent diffuse liver disease diagnosing method.
   ADVANTAGE - The method enables applying the XGBoost algorithm to improve accuracy of classification algorithm and realize diffuse liver disease diagnosis process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an ultrasonic image based multi-model intelligent diffuse liver disease diagnosing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J10B2; T01-J16C2; T01-N01B3; T04-D03; T04-D04; T04-D07D3
IP G06K-009/46; G06K-009/62; G06N-003/04; G06N-003/08; G06T-007/41
PD CN109063712-A   21 Dec 2018   G06K-009/46   201914   Pages: 9   Chinese
AD CN109063712-A    CN10658611    22 Jun 2018
PI CN10658611    22 Jun 2018
UT DIIDW:2019006554
ER

PT P
PN CN109065072-A
TI Deep neural network based objective speech quality evaluation method, involves mixing different pure speech and ambient noise to generate noisy speech with different signal noise ratios, and obtaining different environmental noise samples.
AU LI G
   PENG R
   ZHENG C
   LI X
AE INST ACOUSTICS CHINESE ACAD SCI (CAAI-C)
GA 201902078F
AB    NOVELTY - The method involves establishing a deep neural network for generating data by using three speech features. Actual target speech is determined as output target of the deep neural network by using a PESQ score. The deep neural network is trained for inputting q-frame speech feature vector as input data. A quality evaluation score of the speech to-be evaluated is output. Different pure speech and ambient noise are mixed to generate noisy speech with different signal noise ratios. Different environmental noise samples are obtained.
   USE - Deep neural network based objective speech quality evaluation method.
   ADVANTAGE - The method enables realizing accurate evaluation of the target speech quality under different conditions without existence of a pure reference signal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep neural network based objective speech quality evaluation method. '(Drawing includes non-English language text)'
DC W04 (Audio/Video Recording and Systems)
MC W04-V01; W04-V04A; W04-V05
IP G10L-025/60; G10L-025/30
PD CN109065072-A   21 Dec 2018   G10L-025/60   201914   Pages: 15   Chinese
AD CN109065072-A    CN11154469    30 Sep 2018
PI CN11154469    30 Sep 2018
UT DIIDW:201902078F
ER

PT P
PN CN109060838-A
TI Machine vision based product surface scratch detecting method involves binarizing image using both high and low thresholds, and connecting edge of image with low threshold based on low threshold value.
AU LIN S
AE SANGOODS XIAMEN TECHNOLOGY CO LTD (SANG-Non-standard)
GA 201900726A
AB    NOVELTY - The method involves receiving a signal and collect image information. The non-classical repression suppression on the acquired image is performed, then the suppressed image is inputted into the trained convolutional neural network for texture judgment, using the convolutional neural network to texture the image. The image is dynamically iterated and suppressed to obtain a scratch image. The small range of fracture scratches is connected. The image is binarized using both high and low thresholds. The edge of the image is connected with a low threshold based on the low threshold value. The scratch image is refined.
   USE - Machine vision based product surface scratch detecting method.
   ADVANTAGE - The picture of the mobile phone partition board is rapidly acquired in real time, and a series of processing is carried out, so that the surface scratches can be detected efficiently and accurately, and a detection result can be displayed in real time.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a machine vision based product surface scratch detecting method. (Drawing includes non-English language text)
DC S03 (Scientific Instrumentation)
MC S03-E04F; S03-E04X
IP G01N-021/956
PD CN109060838-A   21 Dec 2018   G01N-021/956   201914   Pages: 11   Chinese
AD CN109060838-A    CN10814173    23 Jul 2018
PI CN10814173    23 Jul 2018
UT DIIDW:201900726A
ER

PT P
PN CN109063586-A
TI Candidate optimization based R-convolutional neural network driver detecting method, involves outputting detection network corresponding to driver detection improvement algorithm, and determining positioning effect of detection network.
AU LU X
   LU M
AE UNIV SOUTHEAST (UYSE-C)
GA 201900658D
AB    NOVELTY - The method involves filtering optimized driver candidate area by connecting a region optimization network to a candidate area generation network. Invalid redundant border frame is removed corresponding to the filtered optimized driver candidate area. Training process is performed on an R-convolutional neural network (CNN) driver detection network, where the R-CNN network and the region optimization network with a feature extraction network as fusion residual structure are separately trained. The trained R-CNN driver detection network is output corresponding to driver detection improvement algorithm. Driver detection accuracy and positioning effect of the R-CNN driver detection network are determined.
   USE - Candidate optimization based R-CNN driver detecting method.
   ADVANTAGE - The method enables improving driver identification precision and real-time property by designing ZF- Res network, selecting RGB image with the driver as a main image, filtering candidate areas with redundancy, performing subsequent classification and regression process to complete positioning detection of the driver, reducing a target frame without include the driver and improving accuracy of target driver detection so as to improve detection efficiency. The method enables improving application value during monitoring and identifying process of driver driving behavior so as to prevent traffic accidents.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a candidate optimization based R-CNN driver detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J10B1; T01-J10B2; T01-N01B3; T01-N02B2; T04-D04; T04-D07D5; T04-D08
IP G06K-009/00; G06K-009/62
PD CN109063586-A   21 Dec 2018   G06K-009/00   201914   Pages: 11   Chinese
AD CN109063586-A    CN10759174    11 Jul 2018
PI CN10759174    11 Jul 2018
UT DIIDW:201900658D
ER

PT P
PN CN109063842-A
TI Multi-algorithm frame compatible machine learning platform, has platform body mounted with resource scheduling model and user isolation model by constructing cluster compatible mainstream machine learning frameworks.
AU WANG F
AE WUXI XUELANG NUMERAL SYSTEM TECHNOLOGY (WUXI-Non-standard)
GA 201900651U
AB    NOVELTY - The platform has a platform body mounted with a resource scheduling model and a user isolation model by constructing a Hadoop (RTM: Open source Java-based programming framework) cluster compatible mainstream machine learning frameworks. The mainstream machine learning frameworks are constructed on the Hadoop (RTM: Open source Java-based programming framework) cluster for data storage. The user isolation model comprises a Spark ML (RTM: Open-source distributed general-purpose cluster-computing framework) and a learning frame in a parallel manner for depth learning. A feed mechanism selects added interception as data corresponding to the mainstream machine learning frameworks. Synchronization across nodes are arranged with each other for automatically performing model constructing process.
   USE - Multi-algorithm frame compatible machine learning platform.
   ADVANTAGE - The platform can finish heterogeneous computing framework integration and CPU and GPU bottom supports integration.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a multi-algorithm frame compatible machine learning platform. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-E01B; T01-F02C1; T01-F02C2; T01-J05B3
IP G06N-099/00; G06F-009/50; G06F-017/30
PD CN109063842-A   21 Dec 2018   G06N-099/00   201914   Pages: 7   Chinese
AD CN109063842-A    CN10738131    06 Jul 2018
PI CN10738131    06 Jul 2018
UT DIIDW:201900651U
ER

PT P
PN CN109063581-A; US2019130167-A1
TI Computer implemented gray scale input image capturing based human face detecting method, involves receiving gray scale input image by camera and face detection module and performing gray scale image conversion process.
AU WU Q
   WANG X
   GAO Y
   MA R
   LU Y
   NG H W
AE ALTUMVIEW SYSTEMS INC (ALTU-Non-standard)
   ALTUMVIEW SYSTEMS INC (ALTU-Non-standard)
GA 201900658J
AB    NOVELTY - The method involves receiving human face image training data set. A color image in a training data set is converted into a grayscale training image. A gray scale input image is received by a camera and a convolutional neural network human face detection module according to gray image training. Gray scale image conversion process is performed by the convolutional neural network human face detection module. Face detection operation is performed according to a received gray scale input image. Gradation lighting condition is provided with LED lighting.
   USE - Computer implemented gray scale input image capturing based human face detecting method.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for identifying approximate duplicate human face image and for selectively transmitting best pose human face image to a computer
   (2) a method for identifying approximate duplicate human face image and for selectively transmitting best pose human face image to a server.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a camera. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J10B3B; T01-J10D; T01-N01B3; T01-N01D1B; T01-N01D3; T04-D07F1; T04-D08
IP G06K-009/00; G06N-003/04; G06N-005/04; G06T-007/70
PD CN109063581-A   21 Dec 2018   G06K-009/00   201914   Pages: 31   Chinese
   US2019130167-A1   02 May 2019   G06K-009/00   201932      English
AD CN109063581-A    CN10747849    06 Jul 2018
   US2019130167-A1    US943728    03 Apr 2018
FD  US2019130167-A1 CIP of Application US796798
PI US789957    20 Oct 2017
   US796798    28 Oct 2017
   US943728    03 Apr 2018
UT DIIDW:201900658J
ER

PT P
PN CN109063572-A
TI Multi-scale and multi-convolution layer feature fusion based fingerprint activity detecting method, involves performing true and false identification on multi-scale fingerprint images, and testing multi-layer features of fingerprint images.
AU YUAN C
   YU P
   XIA Z
   FU Z
   SUN X
AE UNIV NANJING INFORMATION SCI & TECHNOLOG (UNAI-C)
GA 201900658S
AB    NOVELTY - The method involves performing multi-scale characteristic extracting and training operation. Fingerprint image collecting operation is performed. Convolution neural network training operation is performed to learn fingerprint image characteristics to obtain a convolutional neural network. Principal component analysis process is performed. Features of a network layer are extracted by using the trained convolutional neural network. The convolutional neural network is merged with the principal component analysis of the network layer. True and false identification is performed on multi-scale fingerprint images to identify true and false fingerprint images. Multi-layer features of the multi-scale fingerprint images are tested by using a support vector machine classifier.
   USE - Multi-scale and multi-convolution layer feature fusion based fingerprint activity detecting method.
   ADVANTAGE - The method enables simplifying feature extraction operation, and improving detection precision.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-scale and multi-convolution layer feature fusion based fingerprint activity detecting method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D01C5A; T01-J04B2; T01-J10B2; T01-N01B3A
IP G06K-009/00; G06K-009/62
PD CN109063572-A   21 Dec 2018   G06K-009/00   201914   Pages: 13   Chinese
AD CN109063572-A    CN10729204    04 Jul 2018
PI CN10729204    04 Jul 2018
UT DIIDW:201900658S
ER

PT P
PN CN109044323-A
TI Heart rate and blood oxygen saturation degree measuring device, has device main body provided with signal processing unit, where signal processing unit is provided with deep learning network model and signal is input to main body.
AU ZHANG J
   DING L
   CUI J
   LIU W
   LIU J
AE TIANJIN JINGFAN TECHNOLOGY CO LTD (TIAN-Non-standard)
GA 2019011377
AB    NOVELTY - The device has a device main body provided with a photoplethysmographic (PPG) signal processing unit. A signal processing unit is provided with a deep learning network model, where a PPG signal is input to the main body and heart rate and blood oxygen saturation index are output to the main body. Light to with two wavelengths is obtained using two PPG signal paths to obtain red light reflection or transmission of the PPG signal, where frequency of the PPG signal is about 100 Hz and sampling frequency is obtained 25 Hz. Five-layer convolutional neural networks are connected together.
   USE - Heart rate and blood oxygen saturation degree measuring device.
   ADVANTAGE - The device has better tolerance to noise in a resting state and a motion state and measures the heart rate and blood oxygen accurately.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a heart rate and blood oxygen saturation degree measuring device.
DC P31 (Diagnosis, surgery (A61B).); T01 (Digital Computers)
MC T01-N01B3; T01-N01D; T01-N01E
IP A61B-005/024; A61B-005/1455
PD CN109044323-A   21 Dec 2018   A61B-005/024   201914   Pages: 8   Chinese
AD CN109044323-A    CN11150628    29 Sep 2018
PI CN11150628    29 Sep 2018
UT DIIDW:2019011377
ER

PT P
PN CN109063728-A
TI Fire image depth learning mode identifying method, involves automatically extracting deep color feature by utilizing training sample network, and performing fire image recognition operation by utilizing convolutional network.
AU ZHANG X
   HOU D
   DONG X
AE UNIV YANSHAN (UYAN-C)
GA 201900654P
AB    NOVELTY - The method involves obtaining an image of fire on multi-stage in a complicated background picture from a baidu network by Python (RTM: High-level programming language) crawler technology. An interference image is obtained. Pre-processing operation is performed in an input image. The fire image is determined as a training sample and a testing sample. A multi-layer GoogleNet (RTM: Deep Learning Toolbox) convolutional neural network model is established. Deep color feature is automatically extracted by utilizing a training sample network. Fire image recognition operation is performed by utilizing a convolutional network.
   USE - Fire image depth learning mode identifying method.
   ADVANTAGE - The method enables increasing network training efficiency and performing identifying operation in a simple manner. The method enables reducing training time and increasing identifying stability and robustness. The method enables increasing recognition accuracy to 99.2%.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a fire image depth learning mode identifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2A; T01-J10B3B; T01-N01B3A; T01-N03A2; T04-D04; T04-D08
IP G06K-009/62
PD CN109063728-A   21 Dec 2018   G06K-009/62   201914   Pages: 13   Chinese
AD CN109063728-A    CN10638166    20 Jun 2018
PI CN10638166    20 Jun 2018
UT DIIDW:201900654P
ER

PT P
PN CN109065030-A
TI Convolutional neural network based environmental sound recognizing method, involves extending training distribution by linear interpolation and related target using characteristic of linear interpolation for increasing diversity of samples.
AU ZHANG Z
   XU S
   CAO S
   ZHANG S
AE UNIV SHANGHAI (USHN-C)
GA 201900623N
AB    NOVELTY - The method involves extracting energy spectrum from original audio mixing to obtain an energy spectrum characteristic sample library for training a convolutional neural network model. Environmental sounds are recognized by a trained convolutional neural network. Two samples are randomly selected from energy spectrum characteristic. The samples are mixed for constructing a virtual training sample according to ratio. A mixed ratio of the samples is obtained as a training target. Training distribution is extended by linear interpolation and related target using characteristic of linear interpolation for increasing diversity of training samples. FFT conversion is performed for frames of an original audio to obtain sound amplitude spectrum.
   USE - Convolutional neural network based environmental sound recognizing method.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a convolutional neural network based environmental sound recognizing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based environmental sound recognizing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-E01B; T01-J04B1; T01-N01B3; T01-N01D1A; W04-G; W04-V01
IP G10L-015/08; G10L-015/02; G10L-015/06; G06N-003/04
PD CN109065030-A   21 Dec 2018   G10L-015/08   201914   Pages: 10   Chinese
AD CN109065030-A    CN10862022    01 Aug 2018
PI CN10862022    01 Aug 2018
UT DIIDW:201900623N
ER

PT P
PN JP2018073103-A
TI Arithmetic circuit used for pattern recognition, has control unit which performs calculation of reference data and coefficient data of filter that are held at holding unit.
AU KATO M
   YAMAMOTO T
   NOMURA O
   ITO Y
   MORI K
AE CANON KK (CANO-C)
GA 201836191F
AB    NOVELTY - The arithmetic circuit has a first holding unit which holds a predetermined number of reference data transferred from a storage device. A second holding unit holds the coefficient data of a first filter and a second filter which are transferred from the storage device. A control unit (102) performs a calculation of the reference data and coefficient data of the second filter which are held at the first holding unit after making the calculator perform the calculation of the reference data and coefficient data of a first filter which are held at the first holding unit.
   USE - Arithmetic circuit used for pattern recognition, of image processing apparatus (claimed) such as pattern recognition apparatus.
   ADVANTAGE - The decline in the process efficiency of sum-of-products calculation is avoided by performing a filter calculation with the filter which differs from the one portion reference data held at the holding unit. The process order of an arithmetic circuit is optimized according to a structure and operating condition of the convolutional neural network (CNN) network. The reusability of reference data is improved and a memory access bottleneck is eliminated. The process order is controlled by increasing the number of the accumulation sum shift registers with respect to more calculation characteristic surfaces. The case where data transmission controls processing time is reduced, when transfer of reference data requires time, or when a kernel size is small and a kernel calculation area is short.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a control method for arithmetic circuit; and
   (2) a control program for arithmetic circuit.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the arithmetic circuit. (Drawing includes non-English language text)
   RAM (101)
   Control unit (102)
   Coefficient data buffer (103)
   Coefficient data shift register (104)
   Reference data buffer (105)
DC T01 (Digital Computers)
MC T01-D03; T01-H01C2; T01-J01; T01-J04; T01-J10B2A; T01-J16B; T01-J16C1; T01-M06A1; T01-N01D1B
IP G06F-017/10; G06N-003/063; G06T-001/40
PD JP2018073103-A   10 May 2018   G06F-017/10   201833   Pages: 23   Japanese
AD JP2018073103-A    JP211898    28 Oct 2016
PI JP211898    28 Oct 2016
UT DIIDW:201836191F
ER

PT P
PN JP2018066136-A
TI Controller for heater for snow melting, has control unit which changes capability for road heater to melt snow coverage, based on evaluation value output by convolutional neural network (CNN) section which evaluates snow coverage of place.
AU YOKOKAWA M
   TAKEDA K
   KAWAMURA H
AE HOKKAIDO GAS CO LTD (HOKK-Non-standard)
GA 201833457H
AB    NOVELTY - The controller (20) has camera (10) which photographs place of snow melting by road heater (2) and generates image which shows place of snow melting. The photographed image of place of snow melting is output to image decomposition unit (21) which decomposes image to block images, and outputs it to CNN section (22) which evaluates existence of snow coverage for every block image and outputs evaluation result. A control unit (24) changes capability for road heater to melt snow coverage, based on evaluation value.
   USE - Controller for heater for snow melting of snow coverage of place, such as road.
   ADVANTAGE - Controls snow melting of place of snow melting by appropriately reflecting condition of snow coverage, and reduces computational complexity without reducing precision of evaluation of required convolutional neural network.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a learning method for neural network used in snow melting control;
   (2) a snow melting control method; and
   (3) a snow melting control program for making function as snow melting control unit.
   DESCRIPTION OF DRAWING(S) - The drawing shows a functional block diagram of snow melting control system. (Drawing includes non-English language text).
   Road heater (2)
   Camera (10)
   Controller (20)
   Image decomposition unit (21)
   Convolutional neural network section (22)
   Control unit (24)
DC Q41 (Road, rail, bridge construction (E01)); S03 (Scientific Instrumentation); T01 (Digital Computers)
MC S03-E04X; T01-J10C; T01-J21
IP E01C-011/26; E01H-005/00; G06N-003/08; G06T-001/00
PD JP2018066136-A   26 Apr 2018   E01H-005/00   201833   Pages: 28   Japanese
AD JP2018066136-A    JP204053    18 Oct 2016
PI JP204053    18 Oct 2016
UT DIIDW:201833457H
ER

PT P
PN CN107958461-A
TI Method for tracking aircraft target based on binocular vision, involves training convolutional neural network (CNN) network to identify optical image with depth information using trained CNN network to obtain target recognition result.
AU ZHANG Y
   WANG J
AE AVIC XIAN AIRCRAFT DESIGN & RES INST (CHAV-C)
GA 201834800G
AB    NOVELTY - The method involves arranging (1) the binocular optical system at the maximum width of the aircraft platform to adjust the focal length to ensure that the target is within the overlapping range. The two images collected through the SIFT method is matched (2) to convert the disparity into depth information. The obtained depth information is added to one of the original images to obtain an optical image with depth information. The CNN network is trained (5) to identify (6) the optical image with depth information using the trained CNN network to obtain the target recognition result.
   USE - Method for tracking aircraft target based on binocular vision.
   ADVANTAGE - The method has small volume quality relative to laser radar. The aircraft does not need to perform energy radiation on the target to complete the distance measurement and enhance stealth capabilities. The small volume and quality of the laser radar can be achieved thus the faster imaging speed can be achieved.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of the method for tracking aircraft target based on binocular vision. (Drawing includes non-English language text)
   Step for arranging binocular optical system at maximum width of aircraft platform (1)
   Step for matching two images collected through SIFT method (2)
   Step for training CNN network (5)
   Step for identifying optical image with depth information using trained CNN network to obtain target recognition result (6)
DC T01 (Digital Computers); W06 (Aviation, Marine and Radar Systems)
MC T01-J07D1; T01-J10B2A; T01-N01B3; W06-B01B8
IP G06T-007/246; G06T-007/55
PD CN107958461-A   24 Apr 2018   G06T-007/246   201833   Pages: 6   Chinese
AD CN107958461-A    CN11123360    14 Nov 2017
PI CN11123360    14 Nov 2017
UT DIIDW:201834800G
ER

PT P
PN CN106778757-A
TI Scene text detecting method, involves cutting original image in image block according to connection rectangle, and dividing text line into word area to obtain final text detection result according to distance between text area and row.
AU WU X
   BU W
   TANG Y
AE HARBIN INST TECHNOLOGY (HAIT-C)
GA 201738606Y
AB    NOVELTY - The method involves constructing an initial text saliency detection convolutional neural network (CNN) model. A text significance image is obtained corresponding to the initial text saliency detection CNN model by using a large Otsu binarization process to obtain a binary image after the text significance image is obtained. An original image is cut in an image block according to minimal external connection rectangle. A text line is divided into a word area to obtain a final text detection result according to distance between a text area and a row.
   USE - Scene text detecting method.
   ADVANTAGE - The method enables effectively detecting scene in the text area, thus improving saliency detection performance during a scene text detection process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a photographic view a scene text detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2
IP G06K-009/20; G06K-009/34; G06N-003/08
PD CN106778757-A   31 May 2017   G06K-009/34   201749   Pages: 15   Chinese
AD CN106778757-A    CN11137890    12 Dec 2016
PI CN11137890    12 Dec 2016
CP CN106778757-A
      CN102163284-A   UNIV XIDIAN (UYXN)   WANG W, LI J, LU C, LIU X
      CN104573685-A   UNIV CENT SOUTH (UYCS)   CHEN Z, WU H, ZOU B, ZHAO Y
      CN105825238-A   UNIV JIANGSU (UYJS)   CAI Y, CHEN L, DAI L, JIANG H, WANG H, WANG S, XU X, YUAN C
      US20160098608-A1      
CR CN106778757-A
      SHEHZAD MUHAMMAD HANIF : "A cascade detector for text detection in natural scene images", 2008 19TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION,relevantClaims[1-7],relevantPassages[1-4]
      HYUNG IL KOO: "Scene text detection via connected component clustering and nontext filtering", IEEE TRANSACTIONS ON IMAGE PROCESSING,relevantClaims[1-7],relevantPassages[2296-2305]
      : "", ,relevantClaims[1-7],relevantPassages[95]
      : "MSER", ,relevantClaims[1-7],relevantPassages[1134-1140]
UT DIIDW:201738606Y
ER

PT P
PN CN106780546-A
TI Convolutional neural network based high-speed moving object motion fuzzy coding point identifying method, involves dividing blur coding mark point image by using neural network, and obtaining coding mark point identity identification.
AU ZHOU H
   ZHANG L
   CHEN M
AE UNIV NANJING AERONAUTICS & ASTRONAUTICS (UNUA-C)
GA 201738566Y
AB    NOVELTY - The method involves obtaining a motion blur code-point image-coding point identity sample by a really-shot camera. A convolutional neural network of a motion blur image is constructed. Training and testing processes of the obtained motion blur code-point image-coding point identity sample are performed. A motion blur coding mark point image is divided by using the neural network after training the identity sample to classify sub-graph. Corresponding coding mark point identity identification (ID) is obtained. An internal parameter matrix of the really-shot camera is determined.
   USE - Convolutional neural network based high-speed moving object motion fuzzy coding point identification method.
   ADVANTAGE - The method enables shooting a motion blur image to obtain corresponding identity of a coding mark point, providing quick and reliable machine vision measurement prophase data for the high-speed moving object and expanding application field of a machine visual testing process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a convolutional neural network based high-speed moving object.
DC T01 (Digital Computers)
MC T01-J10B2; T01-N01B3A
IP G06T-007/215; G06T-007/80
PD CN106780546-A   31 May 2017   G06T-007/215   201749   Pages: 13   Chinese
AD CN106780546-A    CN11109116    06 Dec 2016
PI CN11109116    06 Dec 2016
CP CN106780546-A
      CN104299006-A   UNIV CHINA COMMUNICATION (UYCH-Non-standard)   GONG W
      CN106096605-A   SHI F (SHIF-Individual)   SHI F, QIAO B, WANG B
      GB1170234-A      
      US9104914-B1   GOOGLE INC (GOOG)   VINCENT L, WU B, ABDULKADER A, ZENNARO M, BISSACCO A, ADAM H, CHEUNG K M, NEVEN H, FROME A L
      WO2014194345-A1   NEWSOUTH INNOVATIONS PTY LTD (NEWS)   AFSHAR S, HAMILTON T J
CR CN106780546-A
      PARAMANAND CHANDRAMOULI : "Inferring Image Transformation and Structure from Motion-Blurred Images", BMVC 2010,relevantClaims[1-5],relevantPassages[73.1-73.12]
      : "PCNN-", ,relevantClaims[1-5],relevantPassages[97-102]
UT DIIDW:201738566Y
ER

PT P
PN CN109100937-A
TI Active power filter based double hidden layer recurrent neural network sliding mode control method, involves designing adaptive law to verify stability of global sliding mode controller of active power filter.
AU CHU Y
   FEI J
   WANG H
   FENG Z
AE UNIV HOHAI CHANGZHOU CAMPUS (UYHO-C)
GA 201903012B
AB    NOVELTY - The method involves establishing a mathematical model of the active power filter. The global sliding mode controller of active power filter is established based on double hidden layer recurrent neural network, and the control law is designed and used it as the control input of active power filter. The adaptive law is designed to verify the stability of the global sliding mode controller of the active power filter based on the double hidden layer regression neural network based on the lyapunov function theory.
   USE - Active power filter based double hidden layer recurrent neural network sliding mode control method.
   ADVANTAGE - The approximation accuracy and generalization ability of the network are improved. The number of network parameters and weights, and speed up network training are reduced, and the ability to store more information is set with better approximation effects, so that the compensation current tracking accuracy and system robustness of the active power filter system in the presence of parameter perturbation and external interference are improved.
   DESCRIPTION OF DRAWING(S) - The drawing shows a circuit diagram of the active power filter based double hidden layer recurrent neural network sliding mode control process.
DC T01 (Digital Computers); T06 (Process and Machine Control)
MC T01-J04E; T06-A05; T06-A07B
IP G05B-013/04
PD CN109100937-A   28 Dec 2018   G05B-013/04   201913   Pages: 25   Chinese
AD CN109100937-A    CN10915690    13 Aug 2018
PI CN10915690    13 Aug 2018
UT DIIDW:201903012B
ER

PT P
PN CN109102037-A
TI Model training method, involves testing original handwriting recognition model and obtaining target handwriting recognition model, when test accuracy is greater than preset accuracy rate.
AU GAO L
   ZHOU G
AE PINGAN TECHNOLOGY SHENZHEN CO LTD (PING-C)
GA 201903403D
AB    NOVELTY - The method involves obtaining (S11) training handwritten images. The training handwritten images are divided (S12) into a training set and a test set according to a preset ratio. The training handwritten images in the training set are sequentially labeled (S13). The labeled training handwritten images are inputted into a convolutional neural network-long-short-time memory neural network for training. The convolutional neural network is used by a time series classification algorithm. The network parameters of the long and short time memory neural network are updated to obtain the original handwriting recognition model. The original handwriting recognition model is tested (S14) using the training handwritten image in the test set. The target handwriting recognition model is obtained, when the test accuracy is greater than the preset accuracy rate.
   USE - Model training method.
   ADVANTAGE - The high training efficiency and high recognition accuracy can be provided.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a character image recognition method;
   (2) a language model training device;
   (3) a character image recognition device;
   (4) a terminal device; and
   (5) a non-volatile storage medium for training model.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a model training method. (Drawing includes non-English language text)
   Step for obtaining training handwritten images (S11)
   Step for dividing training handwritten image into training set and test set (S12)
   Step for sequentially labeling training handwritten images in training set (S13)
   Step for testing original handwriting recognition model using training handwritten image in test (S14)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J10B2A; T01-N01B3A; T04-D04; T04-D07E
IP G06K-009/68; G06K-009/62; G06N-003/04
PD CN109102037-A   28 Dec 2018   G06K-009/68   201913   Pages: 28   Chinese
AD CN109102037-A    CN10563508    04 Jun 2018
PI CN10563508    04 Jun 2018
UT DIIDW:201903403D
ER

PT P
PN CN109102070-A
TI Pre-processing method for convolutional neural network data, involves determining convolution kernel according to operation data and converting average pooled layer to convolutional layer using convolution kernel.
AU GENG Y
   LUO H
AE HORIZON SHANGHAI ARTIFICIAL INTELLIGENCE TECHNOLOGY CO LTD (HORI-Non-standard)
GA 2019034037
AB    NOVELTY - The method involves determining (S110) an average pooling layer of the convolutional neural network. The operational data of the average pooling layer is determined (S120). The operation data is provided with a number of channels of the input tensor of the average pooled layer, a shape of a pooled core of the average pooled layer, the number of voxels is covered and the step size of the pooled core. A convolution kernel is determined (S130) according to the operation data. The average pooled layer is converted (S140) to the convolutional layer using the convolution kernel.
   USE - Pre-processing method for convolutional neural network data.
   ADVANTAGE - The convolution engine efficiently implements an average pooling operation. The average pooled layer in the convolutional neural network is converted into convolutional layer, thus omitting the average pooling operation.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a preprocessing apparatus for convolutional neural network data; and
   (2) a computer readable non-transitory storage medium storing program for convolutional neural network data.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating the pre-processing method for convolutional neural network data. (Drawing includes non-English language text)
   Step for determining average pooling layer of convolutional neural network (S110)
   Step for determined operational data of average pooling layer (S120)
   Step for determining convolution kernel according to operation data (S130)
   Step for converting average pooled layer to convolutional layer using convolutional kernel (S140)
DC T01 (Digital Computers)
MC T01-J04B2
IP G06N-003/04; G06N-003/063
PD CN109102070-A   28 Dec 2018   G06N-003/04   201913   Pages: 16   Chinese
AD CN109102070-A    CN10962564    22 Aug 2018
PI CN10962564    22 Aug 2018
UT DIIDW:2019034037
ER

PT P
PN CN109101932-A
TI Deep learning method involves configuring preliminary target detection classification and final target detection classification to redefine confidence score of target prediction frame.
AU HU J
   YANG H
AE UNIV SUN YAT-SEN FOSHAN SHUNDE RES INST (UYSY-C)
   SYSU-CMU SHUNDE INT JOINT RES INST (SYSU-Non-standard)
   UNIV SUN YAT-SEN (UYSY-C)
GA 2019033954
AB    NOVELTY - The method involves inputting a picture with a real frame initialized, extracting image features using a pre-trained convolutional neural network, and generating a target candidate frame. The image is passed through the region candidate network, and the target prediction frame is extracted. The target prediction frame is subjected to feature extraction through the convolution layer and feature pooling through the pooling layer. The preliminary detection result and the target candidate frame are merged into the ROI pooling layer, and the final frame regression and the final target detection classification are performed. The preliminary target detection classification and the final target detection classification are configured to redefine the confidence score of the target prediction frame by using an information relationship between a target prediction frame and other target prediction frames adjacent.
   USE - Multi-task and proximity information fusion on target detection based on deep learning method.
   ADVANTAGE - The method ensures the accuracy of the target detection while improving the speed of the target detection, and achieves the requirement of real-time target detection.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the network structure diagram of a deep learning algorithm for multi-task and proximity information fusion based on target detection. (Drawing includes non-English language text)
DC T01 (Digital Computers)
MC T01-F02; T01-J04B2; T01-J05B2; T01-J10B2; T01-J16C2; T01-N01B3
IP G06K-009/00; G06K-009/62; G06N-003/04; G06N-003/08
PD CN109101932-A   28 Dec 2018   G06K-009/00   201913   Pages: 8   Chinese
AD CN109101932-A    CN10947455    17 Aug 2018
PI CN10947455    17 Aug 2018
UT DIIDW:2019033954
ER

PT P
PN CN109101952-A
TI Mobile internet-based feature recognition method involves identifying each of extracted pictures according to pre-trained deep convolutional neural network (DCNN), and obtaining category information of species included in target area.
AU LIU J
   LI Z
   YOU Y
   LIU F
AE UNIV BEIJING POSTS & TELECOM (UBPT-C)
GA 201903403N
AB    NOVELTY - The method involves receiving (S101) multiple image data sent by a client device through a mobile internet. Any image data includes a picture about a target area and geographical position information of a preset position collected at the preset position. The preset position is located on a boundary of the target area. The different image data includes geographical location information of different preset locations. The geographic location information and pictures are extracted (S102) from the image data for each image data. An area of the target area is calculated (S103) based on the extracted geographic location information. Each of the extracted pictures is identified (S104) according to the pre-trained DCNN, and category information of the species included in the target area is obtained. The DCNN is trained through sample image and category information of species in the sample image.
   USE - Mobile internet-based feature recognition method applied to a server in a feature recognition system (all claimed).
   ADVANTAGE - The object recognition is quickly and conveniently performed, and the labor cost of the feature recognition is reduced.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a mobile internet-based feature recognition device; and
   (2) a feature recognition system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the mobile internet-based feature recognition method. (Drawing includes non-English language text)
   Step for receiving multiple image data sent by a client device through a mobile internet (S101)
   Step for extracting geographic location information and pictures from the image data for each image data (S102)
   Step for calculating area of the target area based on the extracted geographic location information (S103)
   Step for identifying each of the extracted pictures according to the pre-trained DCNN (S104)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B4F; T01-J10B2A; T04-D04
IP G06K-009/00; G06K-009/62; G06F-017/30
PD CN109101952-A   28 Dec 2018   G06K-009/00   201913   Pages: 17   Chinese
AD CN109101952-A    CN11025889    04 Sep 2018
PI CN11025889    04 Sep 2018
UT DIIDW:201903403N
ER

PT P
PN CN109101108-A
TI Method for optimizing intelligent cockpit human-computer interaction interface based on three decision-making involves obtaining optimal granularity by using weighted summation, and optimal granularity used as finest granularity.
AU LIU Q
   ZHANG G
   WANG R
AE UNIV CHONGQING POSTS & TELECOM (UCPT-C)
GA 201903008K
AB    NOVELTY - The method involves collecting (S1) a gesture video in a cockpit, pre-processing the gesture video, and obtaining a series of static gesture images. A segmentation process on the gesture and the background in the gesture image is performed (S2) to obtain a gesture area image. The multi-granularity expression for the gesture region image is performed (S3) from coarse-grained to fine-grained and multi-granularity features of the gesture region image are extracted by using a convolutional neural network. The semantic conversion is performed (S5) on the recognized gesture, and the corresponding operations are performed on the human-computer interactive interface according to the gesture recognition result after semantic conversion. The optimal granularity is obtained (S6) by using a weighted summation, and the optimal granularity is used as the finest granularity.
   USE - Method for optimizing intelligent cockpit human-computer interaction interface based on three decision-making.
   ADVANTAGE - The method not only can more accurately identify the gestures in the cockpit, execute the gesture commands, but also can reduce the interaction time of the cockpit human-computer interaction interface, and provide the user with a more comfortable interaction experience.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating method for optimizing intelligent cockpit human-computer interaction interface. (Drawing includes non-English language text)
   Step for collecting a gesture video in a cockpit, pre-processing the gesture video, and obtaining a series of static gesture images (S1)
   Step for performing a segmentation process on the gesture and the background in the gesture image to obtain a gesture area image (S2)
   Step for performing multi-granularity expression for the gesture region image from coarse-grained to fine-grained and extracting multi-granularity features of the gesture area image by using a convolutional neural network (S3)
   Step for performing semantic transformation on the recognized gesture, and performing corresponding operations on the human-computer interaction interface according to the gesture recognition result after semantic conversion (S5)
   Step for obtaining optimal granularity by using a weighted summation, and the optimal granularity is used as the finest granularity (S6)
DC T01 (Digital Computers)
MC T01-J10B2A; T01-J16C3
IP G06F-003/01; G06K-009/00
PD CN109101108-A   28 Dec 2018   G06F-003/01   201913   Pages: 11   Chinese
AD CN109101108-A    CN10823980    25 Jul 2018
PI CN10823980    25 Jul 2018
UT DIIDW:201903008K
ER

PT P
PN CN109102885-A
TI Automatic grading method for cataract based on convolutional neural network and random forest combination, involves obtaining feature vector of cataract eye image from first layer of preset convolutional neural network model.
AU NIU K
   HE Z
   RAN J
   DANG J
   WU S
AE UNIV BEIJING POSTS & TELECOM (UBPT-C)
GA 201903390V
AB    NOVELTY - The method involves collecting (101) an image of a cataract eye. The cataract eye image is entered (102) into a preset convolutional neural network model. The feature vector of the cataract eye image is obtained (103) from the first layer of the fully connected layer of the preset convolutional neural network model. The feature vector is inputted (104) into a preset random forest classification model to obtain a cataract grading result corresponding to the cataract eye image.
   USE - Automatic grading method for cataract based on convolutional neural network and random forest combination.
   ADVANTAGE - The method can easily determine the cataract grading result according to the cataract eye image through the preset convolutional neural network model and the preset random forest classification model.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a cataract automatic grading device based on a combination of a convolutional neural network and a random forest.
   DESCRIPTION OF DRAWING(S) - The drawing shows a floe chart illustrating the automatic grading method for cataract based on convolutional neural network and random forest combination. (Drawing includes non-English language text)
   Step for collecting an image of a cataract eye (101)
   Step for entering the cataract eye image into a preset convolutional neural network model (102)
   Step for obtaining the feature vector of the cataract eye image from the first layer of the fully connected layer of the preset convolutional neural network model (103)
   Step for inputting feature vector into a preset random forest classification model to obtain a cataract grading result corresponding to the cataract eye image (104)
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D06; S05-D08A; T01-J05B2; T01-J10B2; T01-N01E1
IP G16H-050/20; G16H-030/20; G06T-007/90; G06T-007/00; G06N-003/04
PD CN109102885-A   28 Dec 2018   G16H-050/20   201913   Pages: 25   Chinese
AD CN109102885-A    CN10948956    20 Aug 2018
PI CN10948956    20 Aug 2018
UT DIIDW:201903390V
ER

PT P
PN CN109101523-A
TI Image processing method, involves filtering original image when original image is determined to be sensitive image according to sensitive information detection area.
AU SHANG L
AE BEIJING SOGOU TECHNOLOGY DEV CO LTD (SOGU-C)
   SOGOU HANGZHOU INTELLIGENT TECHNOLOGY CO (SOGO-Non-standard)
GA 2019034015
AB    NOVELTY - The method involves obtaining (102) an original image of the display information and extracting features of the original image by using a convolutional neural network to obtain a feature image. A sensitive information detection area of the original image is determined (104) according to the feature image. The original image is filtered (106) when the original image is determined to be a sensitive image according to the sensitive information detection area.
   USE - Image processing method.
   ADVANTAGE - The number of times the convolutional neural network is reduced thus, improving the recognition efficiency of sensitive images. The recognition efficiency of sensitive images can be improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) an image processing apparatus;
   (2) a readable storage medium storing instructions for processing image; and
   (3) an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the image processing method. (Drawing includes non-English language text)
   Step for obtaining original image of display information and extracting features of original image by using convolutional neural network to obtain feature image (102)
   Step for determining sensitive information detection area of the original image according to the feature image (104)
   Step for filtering original image when original image is determined to be a sensitive image according to the sensitive information detection area (106)
DC T01 (Digital Computers)
MC T01-J05B2A; T01-J05B4P; T01-J10B1; T01-J10B2A; T01-S03
IP G06F-017/30; G06K-009/62; G06N-003/04
PD CN109101523-A   28 Dec 2018   G06F-017/30   201913   Pages: 23   Chinese
AD CN109101523-A    CN10615079    14 Jun 2018
PI CN10615079    14 Jun 2018
UT DIIDW:2019034015
ER

PT P
PN CN109102342-A
TI Data integration or application system based on block-chain and artificial intelligence, has service platform that performs artificial intelligence deep learning and trains consumption demand model based on first block information.
AU WANG J
AE GUANGDONG CGC DATA INTEGRATION CO LTD (GUAN-Non-standard)
GA 201903398M
AB    NOVELTY - The data integration or application system a service platform which is configured to acquire and store the first block information and second block information, and record a binding relationship between the first block information and second block information. The service platform performs clearing between the service platform, first user and second user according to the first predetermined rule. The service platform performs artificial intelligence deep learning and trains a consumption demand model based on the first block information, matches the product service information and intelligently generates a push message.
   USE - Data integration or application system based on block-chain and artificial intelligence.
   ADVANTAGE - The improved data integration or application system ensures that the generated push message is more accurate and effective and ensures the privacy and security of all parties.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a service method based on block-chain and artificial intelligence.
DC T01 (Digital Computers)
MC T01-J05A; T01-J12C; T01-J30A
IP G06Q-030/02; G06Q-030/06
PD CN109102342-A   28 Dec 2018   G06Q-030/02   201913   Pages: 9   Chinese
AD CN109102342-A    CN11084513    17 Sep 2018
PI CN11084513    17 Sep 2018
UT DIIDW:201903398M
ER

PT P
PN CN109101994-A
TI Method for transferring convolutional neural network used for diabetic retinopathy screening, involves improving last pool layer of first convolutional neural network to obtain second convolutional neural network.
AU WEI Q
   WANG H
   DING D
AE BEIJING VISTEL TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 201903403H
AB    NOVELTY - The method involves improving last pool layer of first convolutional neural network to obtain second convolutional neural network, so that the resolution of the input image of the second convolutional neural network is greater than the resolution of the input image of the first convolutional neural network. The dimensions of length and width of the last pool layer of the first convolutional neural network is enlarged according to the input image. The input image of the second convolutional neural network is a fundus image.
   USE - Method for transferring convolutional neural network used for diabetic retinopathy screening.
   ADVANTAGE - The computing resources consumed by developing specific convolutional neural network is saved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) fundus image screening method;
   (2) a convolutional neural network transferring device;
   (3) a fundus image screening device;
   (4) a electronic device; and
   (5) a computer-readable storage medium storing program for performing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the fundus image screening method. (Drawing includes non-English language text)
   Step for acquiring the fundus image (S11)
   Step for using trained convolutional neural network to detect whether multiple pixels or groups of pixels of the fundus image are screening pixels or screening pixel groups, and screening type of the screening pixel or screening pixel set includes screening type (S12)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J10B3A; T01-J10D; T01-N01D1B; T01-S03; T04-D04
IP G06K-009/62; G06T-007/00; G06N-003/08; G06N-003/04
PD CN109101994-A   28 Dec 2018   G06K-009/62   201913   Pages: 18   Chinese
AD CN109101994-A    CN10732805    05 Jul 2018
PI CN10732805    05 Jul 2018
UT DIIDW:201903403H
ER

PT P
PN CN109101931-A
TI Method for recognizing scene in terminal device, involves confirming scene category of to-be-identified picture as preset scene category, if picture collection parameter meets preset parameter range.
AU ZHANG G
AE OPPO GUANGDONG MOBILE COMMUNICATION CO (GDOP-C)
GA 2019033955
AB    NOVELTY - NOVELTY B The method involves obtaining (S101) a picture to be identified, and performing (S102) scene classification on a to-be-identified picture by using a trained convolutional neural network model. A picture collection parameter of the picture to be identified is obtained (S103), if the convolutional neural network model identifies that the scene category of the to-be-identified picture is a preset scene category. Determination (S104) is made whether the picture collection parameter meets a preset parameter range corresponding to the preset scene category. The scene category of the to-be-identified picture is confirmed (S105) as the preset scene category, if the picture collection parameter meets the preset parameter range.
   USE - Method for recognizing scene in terminal device (claimed).
   ADVANTAGE - ADVANTAGE B The recognition accuracy of the scene category of the pseudo scene can be improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a scene recognition device; and
   (2) a computer-readable storage medium storing computer program for performing scene recognition process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the process for recognizing scene. (Drawing includes non-English language text)
   Step for obtaining picture to be identified (S101)
   Step for performing scene classification on to-be-identified picture (S102)
   Step for obtaining picture collection parameter of picture to be identified (S103)
   Step for determining whether picture collection parameter meets preset parameter range (S104)
   Step for confirming scene category of to-be-identified picture as preset scene category (S105)
DC T01 (Digital Computers)
MC T01-J05B2; T01-J10B2A; T01-S03
IP G06K-009/00; G06K-009/62
PD CN109101931-A   28 Dec 2018   G06K-009/00   201913   Pages: 18   Chinese
AD CN109101931-A    CN10947235    20 Aug 2018
PI CN10947235    20 Aug 2018
UT DIIDW:2019033955
ER

PT P
PN CN109101933-A
TI Method of visual analysis of emotional behavior in electronic device, involves using convolutional neural network to perform data interpretation on visualization report to obtain data interpretation result, and performing teaching diagnosis.
AU ZHANG K
AE CHONGQING LEJIAO TECHNOLOGY CO LTD (CHON-Non-standard)
GA 201903403V
AB    NOVELTY - The method involves obtaining (S21) image data and voice data collected by the photographing device in real time and storing the image data. A teaching model is obtained (S22) and inputted into a convolutional neural network which is used to mine and analyze the image data and the voice data to obtain a teacher-student emotion analysis result and a teacher-student behavior analysis result and that is stored. A visual report is generated (S23) according to the teacher-student emotion analysis result and the teacher-student behavior analysis result and stored. The convolutional neural network is used to perform (S24) data interpretation on the visualization report to obtain a data interpretation result and store it, and perform teaching diagnosis and early warning according to the data interpretation result.
   USE - Method of visual analysis of emotional behavior in electronic device connected to photographing device in classroom.
   ADVANTAGE - The ductility of multimodal sentiment analysis technology and the ductility of convolutional neural networks are improved.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the process for visual analysis of emotional behavior based on artificial intelligence. (Drawing includes non-English language text)
   Step for obtaining image data and voice data collected by the photographing device in real time and storing the image data (S21)
   Step for obtaining a teaching model, inputting the teaching model into a convolutional neural network, and using the convolutional neural network to mine and analyze the image data and the voice data to obtain a teacher-student emotion analysis result and a teacher-student behavior analysis result and stored (S22)
   Step for generating a visual report according to the teacher-student emotion analysis result and the teacher-student behavior analysis result and storing (S23)
   Step for using the convolutional neural network to perform data interpretation on the visualization report to obtain a data interpretation result and store it, and perform teaching diagnosis and early warning according to the data interpretation result (S24)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J10B2; T01-J10E; T01-N01B3; W04-V01; W04-V04A4; W04-V05; W04-W05A
IP G06K-009/00; G10L-025/30; G10L-025/63
PD CN109101933-A   28 Dec 2018   G06K-009/00   201913   Pages: 13   Chinese
AD CN109101933-A    CN10947643    21 Aug 2018
PI CN10947643    21 Aug 2018
UT DIIDW:201903403V
ER

PT P
PN CN109086883-A
TI Deep learning accelerator based sparse calculation realizing method, involves storing target input current characteristic value in pre-configured data cache, and transmitting information of input characteristic value to multiplier array.
AU CHEN S
   YANG C
   LI B
   CHEN H
   HU X
   ZHANG J
   CHEN W
AE UNIV PLA NAT DEFENCE TECHNOLOGY (UNDT-C)
GA 201901468S
AB    NOVELTY - The method involves starting scoring processing operation when input data is received from a channel of an appointed number. The input data is compressed to filter neuron value and obtain the compressed input data. An input characteristic value of the compressed input data is obtained to judge an order of the channel. Required data is detected if the characteristic value of the of the compressed input data is detected as a target input current characteristic value. The target input current characteristic value is stored in a pre-configured data cache. Information of the input characteristic value in the cache is transmitted to an accelerator multiplier array for performing sparse calculation.
   USE - Deep learning accelerator based sparse calculation realizing method.
   ADVANTAGE - The method enables realizing sparse calculation function in a simple manner, reducing sparse calculation cost and energy consumption, and improving calculation efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning accelerator based sparse calculation realizing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-E01A; T01-H03A; T01-J05A1; T01-J30A
IP G06N-003/063; G06N-003/04
PD CN109086883-A   25 Dec 2018   G06N-003/063   201913   Pages: 14   Chinese
AD CN109086883-A    CN10803430    20 Jul 2018
PI CN10803430    20 Jul 2018
UT DIIDW:201901468S
ER

PT P
PN CN109087634-A
TI Audio classification based tone setting method, involves drawing approximate sound style curve to adjust Dolby effects with bass and surround sound, and determining default standard mode parameters according to types of voice data.
AU GAO L
AE SICHUAN CHANGHONG ELECTRIC CO LTD (SCCE-C)
GA 2019014518
AB    NOVELTY - The method involves classifying speech data by an audio classification module using deep learning convolutional neural network (CNN)-MobileNet classification network. A low frequency part is attenuated for speech data of a music class by using audio optimizer function of Dolby sound effect to increase vocal corresponding frequency band. Vocal part effects are determined with Dolby sound clarity. An approximate sound style curve is drawn according to Dolby Sound Intelligent EQ function to adjust Dolby effects with bass and surround sound. Default standard mode parameters are determined according to multiple types of voice data.
   USE - Audio classification based tone setting method.
   ADVANTAGE - The method enables realizing intelligence degree of Android (RTM: Linux-based operating system) intelligent TV, and improving use experience of a user.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an audio classification based tone setting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W03 (TV and Broadcast Radio Receivers); W04 (Audio/Video Recording and Systems)
MC T01-J05B2; T01-N01B3; W03-A12B3; W03-A15; W03-A16C5K; W03-A18A; W03-C05; W04-R01A; W04-R01C5; W04-V01; W04-V05
IP G10L-015/08; G10L-025/18; G10L-025/45
PD CN109087634-A   25 Dec 2018   G10L-015/08   201913   Pages: 6   Chinese
AD CN109087634-A    CN11278861    30 Oct 2018
PI CN11278861    30 Oct 2018
UT DIIDW:2019014518
ER

PT P
PN CN109086878-A
TI Rotation invariance based convolutional neural network model training method, involves determining output of characteristic vector after performing splicing process when window is placed at position coordinates of original feature vector.
AU HAO Z
   ZHANG S
   YANG Q
   WANG Y
   ZHAO W
   TANG J
   XIU H
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 201901468U
AB    NOVELTY - The method involves calculating length and rotation angle of an outer layer (S3). Determination (S4) is made to check whether rotating operation time of the convolution window is equal to rotation time. A current layer feature vector of the convolution window is selected (S5). The convolution window is obtained (S6) after performing rotation transformation process on a feature vector position coordinate. An output of a characteristic vector is determined (S7) after performing splicing process based on a feature vector when the convolution window is placed at position coordinates of an original feature vector.
   USE - Rotation invariance based convolutional neural network model training method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a rotation invariance based convolutional neural network model training method. '(Drawing includes non-English language text)'
   Step for calculating length and rotation angle of outer layer (S3)
   Step for determining whether rotating operation time of convolution window is equal to rotation time (S4)
   Step for selecting current layer feature vector of convolution window (S5)
   Step for obtaining convolution window (S6)
   Step for determining output of characteristic vector after performing splicing process (S7)
DC T01 (Digital Computers)
MC T01-J04B2; T01-N01B3
IP G06N-003/04
PD CN109086878-A   25 Dec 2018   G06N-003/04   201913   Pages: 22   Chinese
AD CN109086878-A    CN11219320    19 Oct 2018
PI CN11219320    19 Oct 2018
UT DIIDW:201901468U
ER

PT P
PN CN109087633-A
TI Method for evaluating voice by electronic device, involves obtaining voice segment of target object, and inputting voice segment of target object into speech evaluation model to obtain voice evaluation result of target object.
AU LI B
   ZHONG G
   QIAO D
AE BEIJING ORION STAR TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 2019014519
AB    NOVELTY - The method involves obtaining voice segments to-be-detected. Voice segments to-be-detected is processed by a speech classification model to obtain voice segment of a target object. Voice segment of the target object is inputted into a speech evaluation model to obtain voice evaluation result of the target object. Feature vector is extracted corresponding to multiple training samples, where multiple training samples comprise voice signal of the target object and voice signal of a non-target object. A speech learning model is established by utilizing deep learning algorithm to determine the speech classification model.
   USE - Method for evaluating voice by an electronic device (claimed).
   ADVANTAGE - The method enables improving accuracy of child speech evaluation.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for evaluating voice by an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for evaluating voice by an electronic device. '(Drawing includes non-English language text)'
DC W04 (Audio/Video Recording and Systems)
MC W04-V01; W04-V04A; W04-V05
IP G10L-015/06; G10L-025/51
PD CN109087633-A   25 Dec 2018   G10L-015/06   201913   Pages: 18   Chinese
AD CN109087633-A    CN10969582    23 Aug 2018
PI CN10969582    23 Aug 2018
UT DIIDW:2019014519
ER

PT P
PN CN109087258-A
TI Depth learning based rain image removing method, involves performing refinement processing on decoded feature image, and summarizing rain image and raindrop negative residual information to obtaining high quality rainless image.
AU LIN J
   LI G
   ZHANG Y
   HE X
   WANG Q
AE UNIV SUN YAT-SEN (UYSY-C)
GA 201901459T
AB    NOVELTY - The method involves generating a shallow feature map of rain images by utilizing a shallow convolutional neural network (S1). The obtained shallow feature map is inputted (S2) to a multi-layer encoder for encoding. A decoding operation is performed (S3) on an encoded feature image by the multi-layer decoder. A refinement processing is performed (S4) on the decoded feature image to predict raindrop negative residual information of the rain image. The rain image and the raindrop negative residual information are summarized (S5) to obtaining a high quality rainless image, where the multi-layer decoder includes an upstream encoder structure and a level-upsampled.
   USE - Depth learning based rain image removing method.
   ADVANTAGE - The method enables retaining the scene detail information when removing the rain strip information.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a depth learning based rain image removing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a depth learning based rain image removing method. '(Drawing includes non-English language text)'
   Step for generating shallow feature map of rain images by utilizing shallow convolutional neural network (S1)
   Step for inputting obtained shallow feature map (S2)
   Step for performing decoding operation (S3)
   Step for performing refinement processing on decoded feature image (S4)
   Step for summarizing rain image and raindrop negative residual information to obtaining high quality rainless image (S5)
DC T01 (Digital Computers)
MC T01-J10B1; T01-J10D; T01-N01B3
IP G06N-003/04; G06T-005/00
PD CN109087258-A   25 Dec 2018   G06T-005/00   201913   Pages: 14   Chinese
AD CN109087258-A    CN10843575    27 Jul 2018
PI CN10843575    27 Jul 2018
UT DIIDW:201901459T
ER

PT P
PN CN109087655-A
TI Traffic road sound monitoring and abnormal sound identifying system, has communication network for providing transmission data to data processing module, and convolutional neural network for recognizing output type of abnormal sound.
AU LUO L
   QIN H
   WANG M
   ZHOU Z
   DENG X
   LIU Z
   WEI J
AE UNIV GUILIN ELECTRONIC TECHNOLOGY (UYGE-C)
GA 201901450T
AB    NOVELTY - The system has a sound collecting end provided with a sound pickup, a sound card, a GPS locating module, a data processing module and a wireless communication module. The sound pickup collects a sound signal. The sound card performs analog-digital conversion to a sound signal. The GPS locating module obtains positioning information of a satellite. The data processing module collects the sound signal. A communication network provides transmission data to the data processing module. A deep convolutional neural network (CNN) recognizes output type of abnormal sound and generates the abnormal sound.
   USE - Traffic road sound monitoring and abnormal sound identifying system.
   ADVANTAGE - The system greatly increases data transmission quantity, adopts CNN for identification and classification of sound, greatly improves training efficiency and identifying precision, and effectively identifies abnormal sound generated in the traffic road.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a traffic road sound monitoring and abnormal sound identifying system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T07 (Traffic Control Systems); W04 (Audio/Video Recording and Systems)
MC T01-C08; T01-D02; T01-J05B2; T01-J18; T01-J21C; T01-N01D1A; T01-N02B2; T07-A01; T07-A05; W04-V01; W04-V04A; W04-V05
IP G10L-019/02; G10L-019/26; G10L-025/24; G10L-025/30; G10L-025/45; G10L-025/51; G08G-001/01; G08G-001/13
PD CN109087655-A   25 Dec 2018   G10L-019/02   201913   Pages: 13   Chinese
AD CN109087655-A    CN10851609    30 Jul 2018
PI CN10851609    30 Jul 2018
UT DIIDW:201901450T
ER

PT P
PN CN109086885-A
TI Method for establishing depth learning model by utilizing electronic device, involves performing deep learning operation in current data by utilizing complex model when confidence rate is less than or equal to predetermined threshold value.
AU TANG L
   SHI R
   ZHANG Z
   HUANG C
   ZHANG D
AE BEIJING QIANXIN TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 201901468R
AB    NOVELTY - The method involves establishing a current data single depth learning model during depth learning process. A confidence rate of the current data single depth learning model is calculated. Deep learning operation is performed in current data by utilizing the current data single depth learning model for reducing execution time of deep learning operation when the confidence rate is greater than a predetermined threshold value. Deep learning operation is performed in the current data by utilizing a complex model when the confidence rate is less than or equal to the predetermined threshold value.
   USE - Method for establishing depth learning model by utilizing an electronic device (claimed).
   ADVANTAGE - The method enables increasing depth learning accuracy and processing large data by utilizing the current data single depth learning model.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for establishing depth learning model by utilizing an electronic device
   (2) a non transitory computer-readable storage medium for storing a set of instructions for establishing depth learning model by utilizing an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for establishing depth learning model by utilizing an electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J30A; T01-S03; W04-W05A
IP G06N-003/08
PD CN109086885-A   25 Dec 2018   G06N-003/08   201913   Pages: 9   Chinese
AD CN109086885-A    CN10873655    02 Aug 2018
PI CN10873655    02 Aug 2018
UT DIIDW:201901468R
ER

PT P
PN CN109087236-A
TI Multi-depth learning platform industrial image detection control method, involves completing network configuration file to model load, loading industrial image in model, performing image recognition process, and obtaining detection result.
AU WANG J
   XU X
AE GOERTEK INC (QDGR-C)
GA 201901460D
AB    NOVELTY - The method involves obtaining model configuration center according to an image characteristic recognition application scene, where the model configuration center comprises an algorithm platform, a model input, a network structure, software, and a hardware dependent algorithm model. Precondition checking process is performed on the model configuration center. A model load is downloaded according to parameters of a model. A network configuration file is completed to the model load. An industrial image is loaded in the model. Image characteristic recognition process is performed. Detection result is obtained.
   USE - Multi-depth learning platform industrial image detection control method.
   ADVANTAGE - The method enables transferring inconvenient caused by a platform.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a multi-depth learning platform industrial image detection control system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-depth learning platform industrial image detection control method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-F01B; T01-F02A; T01-J10B2A; T01-N01B3; T01-N01D1B; T01-N01D2; T01-N03
IP G06T-001/20; G06T-007/00; G06F-009/445
PD CN109087236-A   25 Dec 2018   G06T-001/20   201913   Pages: 8   Chinese
AD CN109087236-A    CN10643803    21 Jun 2018
PI CN10643803    21 Jun 2018
UT DIIDW:201901460D
ER

PT P
PN CN109086884-A
TI Gradient reverse confrontation sample recovery based neural network optimizing and training method, involves optimizing sample set by detecting confrontation sample from sample set, and generating anti-samples in sample set.
AU YI P
   HU J
   ZHANG H
   NI J
   HE Z
AE UNIV SHANGHAI JIAOTONG (USJT-C)
GA 201903704G
AB    NOVELTY - The method involves optimizing a sample set by detecting a confrontation sample from the sample set when the sample is restored to a normal sample by an attack mode. Anti-samples are generated in the sample set by using a FGSM (fast gradient sign method) algorithm, C and W algorithm, deep fool algorithm and jacobian saliency mapping algorithm. The attack mode is included with a quick gradient descent algorithm. Anti-sample distance calculating process, confused depth learning process. Greedy matching algorithm is obtained based on jacobian matrix (JSMA). Minimum disturbance distance is calculated as shortest distance from a current inputting point to a segmentation plane.
   USE - Gradient reverse confrontation sample recovery based neural network optimizing and training method.
   ADVANTAGE - The method enables improving universality for various sample and multiplexing degree of a system, realizing gradient reversing process to know in advance a generating mode and increasing disturbing and crossing efficiency of a decision boundary to reduce the decision boundary to the normal sample.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a gradient reverse confrontation sample recovery based neural network optimizing and training method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-N01B3
IP G06N-003/08; G06K-009/62
PD CN109086884-A   25 Dec 2018   G06N-003/08   201913   Pages: 6   Chinese
AD CN109086884-A    CN10781467    17 Jul 2018
PI CN10781467    17 Jul 2018
UT DIIDW:201903704G
ER

PT P
PN CN109044339-A
TI Limit convolutional neural network based ECG signal diagnosing method, involves determining sample data of ECG waveform in database, and performing diagnosis process on ECG signals by using extreme learning machine.
AU AO W
   HE S
   YU Q
   WANG Y
   CHEN X
AE UNIV CHONGQING TECHNOLOGY & BUSINESS (UYSS-C)
GA 201901136Q
AB    NOVELTY - The method involves establishing a three-layer convolutional neural network (S1). Characteristic extraction process of ECG waveforms is performed (S2) in a database by using the three-layer convolutional neural network. Characteristic output parameters of the convolutional neural network is input (S3) into an extreme learning machine. Parameters of the extreme learning machine are determined (S4). Sample data of the ECG waveform in the database is determined. Diagnosis process is performed (S5) on the ECG signals by using the extreme learning machine.
   USE - Limit convolutional neural network based ECG signal diagnosing method.
   ADVANTAGE - The method enables realizing purpose of diagnosing ECG signals, avoiding artificial monitoring signals for ECG diagnosis, and utilizing a neural network to perform signal diagnosis so as to realize better particularly effect.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a limit convolutional neural network based ECG signal diagnosing method. '(Drawing includes non-English language text)'
   Step for establishing a three-layer convolutional neural network (S1)
   Step for performing characteristic extraction process of ECG waveforms in a database by using the three-layer convolutional neural network (S2)
   Step for inputting characteristic output parameters of the convolutional neural network into an extreme learning machine (S3)
   Step for determining parameters of the extreme learning machine (S4)
   Step for performing diagnosis process on the ECG signals by using the extreme learning machine (S5)
DC B04 (Natural products and polymers. Including testing of body fluids (other than blood typing or cell counting), pharmaceuticals or veterinary compounds of unknown structure, testing of microorganisms for pathogenicity, testing of chemicals for mutagenicity or human toxicity and fermentative production of DNA or RNA. General compositions.); P31 (Diagnosis, surgery (A61B).); S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC B11-C08B; B11-C11; B12-K04G2B; S05-D01A1; S05-G02G3; T01-J05B4P; T01-N01E; T01-N02B2
IP A61B-005/0402; A61B-005/00; G06N-003/04
PD CN109044339-A   21 Dec 2018   A61B-005/0402   201913   Pages: 6   Chinese
AD CN109044339-A    CN10916000    13 Aug 2018
PI CN10916000    13 Aug 2018
UT DIIDW:201901136Q
ER

PT P
PN CN109062897-A
TI Deep neural network based sentence aligning method, involves calculating probability of alignment of two sentences, judging whether sentences are aligned when probability is greater than set value, and extracting sentences from documents.
AU DING Y
   LI J
   ZHOU G
AE UNIV SOOCHOW (USWZ-C)
GA 2019006766
AB    NOVELTY - The method involves generating a word list and word in the word list based on a training language material. A word embedding layer is generated for finding the corresponding word in a word table. A sentence is encoded by using a bidirectional circular neural network layer. Context information of the word is determined to obtain a hidden state of the context information. A result of the bidirectional circular neural network layer is input into a sensor through a sensor layer to obtain abstract representation for determining sentence alignment. Probability of alignment of two sentences is calculated. Judgment is made to check whether the sentences are aligned when the probability is greater than a set value. The aligned sentences are extracted from two documents.
   USE - Deep neural network based sentence aligning method.
   ADVANTAGE - The method enables obtaining a word hidden state including the context information through the bidirectional circulating neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep neural network based sentence aligning method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04A; T01-J11A1; T01-N01B3
IP G06F-017/27; G06N-003/04
PD CN109062897-A   21 Dec 2018   G06F-017/27   201913   Pages: 23   Chinese
AD CN109062897-A    CN10834431    26 Jul 2018
PI CN10834431    26 Jul 2018
UT DIIDW:2019006766
ER

PT P
PN CN109062910-A
TI Deep neural network based sentence aligning method, involves calculating probability of sentences to judge whether sentences are aligned when probability is greater than set value for extracting aligned sentences in documents.
AU DING Y
   LI J
   ZHOU G
AE UNIV SOOCHOW (USWZ-C)
GA 201900675V
AB    NOVELTY - The method involves pre-processing training language material to generate a word in a word list. Generated word in sentence is determined from a word table. Sentence encoding is performed by using a bidirectional circular neural network layer. Semantic information and context information of the word are obtained in a hidden state. Determination is made to check whether semantic relationship between information of the hidden state of each word is determined by the bidirectional circulating neural network layer. Determination is made to check whether similarity between words from linear relation and non-linear relation is determined by a door associated network. Probability of two sentences are calculated to judge whether the sentences are aligned when the probability is greater than a set value for extracting aligned sentences in documents.
   USE - Deep neural network based sentence aligning method.
   ADVANTAGE - The method enables avoiding need of additional dictionary information to capture semantic relationship between features so as to obtain a word similarity matrix.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep neural network based sentence aligning method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-D02; T01-J04A; T01-J11A1; T01-J14; T01-J16C3; T01-N01B3
IP G06F-017/28; G06F-017/27; G06N-003/04
PD CN109062910-A   21 Dec 2018   G06F-017/28   201913   Pages: 23   Chinese
AD CN109062910-A    CN10835723    26 Jul 2018
PI CN10835723    26 Jul 2018
UT DIIDW:201900675V
ER

PT P
PN CN109065032-A
TI Deep convolutional neural network and external corpus-based speech recognition method, involves establishing neural network model corresponding to voice data input, and obtaining output recognized text data.
AU FU X
   ZHANG G
AE UNIV HANGZHOU DIANZI (UYHH-C)
GA 201900623L
AB    NOVELTY - The method involves acquiring voice annotation data. Voice label data and telephone recording data are obtained. Character sequence, pinyin sequence and phoneme sequence are obtained corresponding to the voice annotation data. Voice label feature is extracted based on standardization data. A neural network model is established corresponding to voice data input. Output recognized text data is obtained. Average energy data is obtained. Average energy of voice signal data is obtained. Phoneme output layer node number is 70.
   USE - Deep convolutional neural network and external corpus-based speech recognition method.
   ADVANTAGE - The method enables improving recognition rate of whole sentence and improving large scale lifting sentence recognition accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep convolutional neural network and external corpus-based speech recognition method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-C08A; T01-J16B; W04-V01
IP G10L-015/16; G10L-015/18
PD CN109065032-A   21 Dec 2018   G10L-015/16   201913   Pages: 9   Chinese
AD CN109065032-A    CN10777097    16 Jul 2018
PI CN10777097    16 Jul 2018
UT DIIDW:201900623L
ER

PT P
PN CN109044651-A
TI Unknown environment natural gesture instruction based intelligent wheelchair controlling method, involves establishing convolutional neural network training regression model, and sending navigation commands to subsystem.
AU GAO Q
   ZHANG J
AE UNIV SOOCHOW (USWZ-C)
GA 2019011292
AB    NOVELTY - The method involves controlling motion of an intelligent wheelchair by a motion control subsystem according to a navigation instruction. Color image information and depth image information are obtained. A hand region image is obtained by a removing arm part. Hand area characteristic is determined by using a support vector machine. Gesture recognition result is obtained, where the recognition result comprises pointing gesture. A hand area centroid value is obtained. A convolutional neural network training regression model is established. Navigation commands are sent to the motion control subsystem.
   USE - Unknown environment natural gesture instruction based intelligent wheelchair controlling method.
   ADVANTAGE - The method enables simplifying human body natural gesture identifying process in a convenient manner so as to realize intelligent wheelchair user intent target point analysis and self-planned route.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an unknown environment natural gesture instruction based intelligent wheelchair controlling system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an unknown environment natural gesture instruction based intelligent wheelchair controlling method. '(Drawing includes non-English language text)'
DC P33 (Medical aids, oral administration (A61G, H, J).); T01 (Digital Computers)
MC T01-J10B2A; T01-J10B3B; T01-J21; T01-N01B3; T01-N01D1B; T01-N01E
IP A61G-005/04; G06F-003/01; G06K-009/00
PD CN109044651-A   21 Dec 2018   A61G-005/04   201913   Pages: 12   Chinese
AD CN109044651-A    CN10590953    09 Jun 2018
PI CN10590953    09 Jun 2018
UT DIIDW:2019011292
ER

PT P
PN CN109063203-A
TI Method for expanding query word based on personalized model, involves constructing user personalized model based on multi-tag, and selecting final ranking value with candidate expansion term, and selecting words as expansion words.
AU GOU Z
   HAN L
   YUAN B
   LIU Y
   ZHANG Q
AE UNIV HOHAI (UYHO-C)
GA 201900668D
AB    NOVELTY - The method involves constructing a user personalized model based on a multi-tag, where weight of each tag reflects user preference for the tag. A word vector in a large corpus is trained by using deep learning process. A corpus word vector is generated. An inquiry is initiated by a user. An inquiry word vector is generated. Similarity between each word and the inquiry word vector is calculated. A final sorting value of each candidate extension word is calculated according to similarity. The final ranking value is selected from big to small with candidate expansion term. The words are selected as expansion words.
   USE - Method for expanding a query word based on a personalized model.
   ADVANTAGE - The method enables combining the query word based on the personalized model so as to improve retrieving precision.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for expanding a query word based on a personalized model. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-E01B; T01-E03; T01-J04C; T01-J05B4P; T01-J30A
IP G06F-017/30
PD CN109063203-A   21 Dec 2018   G06F-017/30   201913   Pages: 9   Chinese
AD CN109063203-A    CN11073589    14 Sep 2018
PI CN11073589    14 Sep 2018
UT DIIDW:201900668D
ER

PT P
PN CN109067688-A
TI Data model-driven based OFDM receiving method, involves performing hard decision with preset threshold to obtain depth neural network output and decision result, and obtaining estimation of bit stream corresponding to decision result.
AU JIN S
   GAO X
   ZHANG J
   WEN C
AE UNIV SOUTHEAST (UYSE-C)
GA 201900559Q
AB    NOVELTY - The method involves training a deep neural network within a receiver channel estimation module and a signal detecting module. Input frequency domain pilot frequency and local frequency domain pilot frequency are received by a channel estimation module to obtain a channel estimation result. A zero-forcing equalization result is obtained during zero-forcing equalization process corresponding to the channel estimation result. Hard decision with a preset threshold is performed to obtain a depth neural network output and a decision result. Estimation of a bit stream is obtained corresponding to the decision result.
   USE - Data model-driven based OFDM receiving method.
   ADVANTAGE - The method enables optimizing a neural network to ensure low network training time consumption and better detecting performance so as to quickly obtain channel information.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a data model-driven based OFDM receiving method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W01 (Telephone and Data Transmission Systems); W02 (Broadcasting, Radio and Line Transmission Systems)
MC T01-N01B3; W01-A06A3; W01-A08X; W01-A09D; W02-K07C
IP H04L-027/26; H04L-025/02
PD CN109067688-A   21 Dec 2018   H04L-027/26   201913   Pages: 10   Chinese
AD CN109067688-A    CN10743534    09 Jul 2018
PI CN10743534    09 Jul 2018
UT DIIDW:201900559Q
ER

PT P
PN CN109045664-A
TI Depth learning based diving scoring method, involves performing diving scoring model training process, inputting synchronized video data to training completion of diving scoring model, and outputting synchronized value.
AU LI Y
   DU C
   LIN J
   GAN T
   SONG X
   NIE L
AE UNIV SHANDONG (USHA-C)
GA 2019011057
AB    NOVELTY - The method involves constructing a diving scoring model, where the diving scoring model includes a C3DNet model, a pose-net model and a support vector regression model. The C3DNet model and the pose-net model are connected with each other in a parallel manner. The support vector regression model is connected with the C3DNet model based on Intel caffe frame optimization process. Synchronized video data and a corresponding diving value are utilized. Diving scoring model training process is performed. The synchronized video data is input to training completion of the diving scoring model. A synchronized value is output. Action analysis module training process is performed.
   USE - Depth learning based diving scoring method.
   ADVANTAGE - The method enables avoiding manual interference of a diving fraction value, and improving accuracy of the diving fraction value.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a depth learning based diving scoring server
   (2) a depth learning based diving scoring system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a depth learning based diving scoring method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J10B2; T01-J30A; T01-N02A3C; T04-D04; W04-W05A; W04-X01A; W04-X01C
IP A63B-071/06; A63B-024/00; G06K-009/62
PD CN109045664-A   21 Dec 2018   A63B-071/06   201913   Pages: 11   Chinese
AD CN109045664-A    CN11030493    05 Sep 2018
PI CN11030493    05 Sep 2018
UT DIIDW:2019011057
ER

PT P
PN CN109061705-A
TI Method for processing training data in memory and service terminal based on nerve network model, involves obtaining training set according to input tensor-flow convolution neural network model to obtain training data.
AU LIN L
AE QIANXUN LOCATION NETWORK CO LTD (QIAN-Non-standard)
GA 201900704D
AB    NOVELTY - The method involves extracting track information of a user. A map is plotted on a plane based on the extracted track information to form a corresponding set of track pictures. A training set is obtained according to input tensor-flow convolution neural network model to obtain training data. User data is obtained, where the user data comprises expected trajectory data and the real track data. Track information is extracted from anticipated trajectory data. A track picture set is stored into a test set and a training set. The test set and the training set are input to a tensor-flow convolutional neural network model.
   USE - Method for processing training data in a memory and a service terminal (all claimed) based on a tensor-flow convolution nerve network model.
   ADVANTAGE - The method enables improving data processing accuracy.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a differential locating platform
   (2) a Tensor-flow convolution nerve network model based terminal for processing training data in a memory and a service terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for processing training data in a memory and a service terminal based on tensor-flow convolution nerve network model. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W06 (Aviation, Marine and Radar Systems)
MC T01-J04B2; T01-J10E; T01-N01B3A; W06-A03A5A; W06-A03A5C
IP G01S-019/46; G06N-003/04; G06N-003/08
PD CN109061705-A   21 Dec 2018   G01S-019/46   201913   Pages: 13   Chinese
AD CN109061705-A    CN10686584    27 Jun 2018
PI CN10686584    27 Jun 2018
UT DIIDW:201900704D
ER

PT P
PN CN109048903-A
TI Mechanical arm inserting method, involves obtaining final position of gesture prediction model, and controlling grabbing end pose prediction model based on final position of gesture prediction model along moving direction.
AU LU C
   DING J
   WANG S
   ZHONG S
AE SHANGHAI FLEXIV ROBOTICS LTD (SHAN-Non-standard)
GA 201902310H
AB    NOVELTY - The method involves obtaining correct insertion gesture and picture of an inserting hole. Random sampling in a block region of a grid is performed to obtain a grasping end of a mechanical arm in sampling state. A sampling position of a lattice point is determined. An inserting shaft of the inserting hole is sampled to obtain a light stream image. Flow characteristic is obtained through a convolutional neural network. Optical characteristic is added to a database. A final position of a gesture prediction model is obtained. A grabbing end pose prediction model is controlled based on the final position of the gesture prediction model along a moving direction.
   USE - Mechanical arm inserting method.
   ADVANTAGE - The method enables automatically realizing insertion of a mechanical arm with high precision, improving predicting precision by sampling with high-universality and high-automation degree.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a mechanical arm inserting device
   (2) a computing-based device comprising set of instructions to execute a mechanical arm inserting method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a mechanical arm inserting method. '(Drawing includes non-English language text)'
DC P62 (Hand tools, cutting (B25, B26).); T01 (Digital Computers)
MC T01-J05B4P
IP B25J-009/16
PD CN109048903-A   21 Dec 2018   B25J-009/16   201913   Pages: 16   Chinese
AD CN109048903-A    CN10943152    17 Aug 2018
PI CN10943152    17 Aug 2018
UT DIIDW:201902310H
ER

PT P
PN CN109064674-A
TI Storage cabinet management based human face detecting method, involves drawing article by user, matching characteristic data with storage list, opening storage lattice, deleting user data, and stopping storing operation.
AU SHI J
   LIN Z
AE HUAHUISHI TECHNOLOGY TIANJIN CO LTD (HUAH-Non-standard)
GA 201900631P
AB    NOVELTY - The method involves detecting a human face by using a deep learning face detection algorithm. Face characteristic data is extracted. The characteristic data is stored. Articles are stored by a user. A storage button is clicked on a touch screen. Free storage space is indicated if idle grid entry storing process is performed. Face recognition process is performed. Grid number is stored in a database. The article is drawn by the user. The characteristic data is matched with a storage list. A storage lattice is opened. User data is deleted. Storing operation is stopped. Drawing process is performed on the face characteristic data.
   USE - Storage cabinet management based human face detecting method.
   ADVANTAGE - The method enables adopting face recognition technology for realizing management of a storage cabinet, storing and drawing articles credentials by the storage cabinet, and improving user experience.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a storage cabinet management based human face detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); T05 (Counting, Checking, Vending, ATM and POS Systems)
MC T01-J05B4M; T01-J10B2A; T01-J30A; T04-D07F1; T04-F02A2; T05-D01B; T05-G03; T05-H05C; T05-H08C
IP G07F-017/12; G07C-009/00
PD CN109064674-A   21 Dec 2018   G07F-017/12   201913   Pages: 5   Chinese
AD CN109064674-A    CN10547899    31 May 2018
PI CN10547899    31 May 2018
UT DIIDW:201900631P
ER

PT P
PN CN109063741-A
TI Depth learning and Hilbert curve transformation based energy spectrum analysis method, involves constructing nuclide identification of depth learning algorithm to analyze deep learning classifier by determining threshold value and ROC curve.
AU TANG X
   GONG P
   ZHANG J
   LI H
   LIANG D
AE UNIV NANJING AERONAUTICS & ASTRONAUTICS (UNUA-C)
   NAT OCEAN TECHNOLOGY CENT (GJHY-C)
GA 201900654B
AB    NOVELTY - The method involves obtaining detection spectrum and simulated spectrum to carry out a pre-treatment process (1). One-dimensional (1D) energy spectrum is converted into two-dimensional (2D) image identification to performing deep learning process of full-spectrum based on the energy spectrum (2). A fast nuclide identification of a depth learning algorithm is constructed to analyze a deep learning classifier by determining classification threshold value and ROC curve (3). Natural background energy spectrum is detected by a detector to obtain radioactive nuclide energy spectrum data. A Monte-Carlo program is installed with MORSE, MCNP, EGS, GEANT4-type, FLUKA, SuperMC and Phits or GADRAS.
   USE - Depth learning and Hilbert curve transformation based energy spectrum analysis method.
   ADVANTAGE - The method enables improving recognition rate, stability and adaptability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a process flow diagram illustrating a depth learning and Hilbert curve transformation based energy spectrum analysis method. '(Drawing includes non-English language text)'
   Step for obtaining detection spectrum and simulated spectrum to carry out pre-treatment process (1)
   Step for converting 1D energy spectrum into 2D image identification to performing deep learning process of full-spectrum based on energy spectrum (2)
   Step for constructing fast nuclide identification of depth learning algorithm to analyze deep learning classifier by determining classification threshold value and ROC curve (3)
DC S03 (Scientific Instrumentation); T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC S03-G02; T01-E04; T01-J05B2; T01-J10B2A; T01-J16C2; T01-J30A; T04-D04
IP G06K-009/62; G01T-001/36
PD CN109063741-A   21 Dec 2018   G06K-009/62   201913   Pages: 15   Chinese
AD CN109063741-A    CN10732197    05 Jul 2018
PI CN10732197    05 Jul 2018
UT DIIDW:201900654B
ER

PT P
PN CN109064449-A
TI Method for detecting bridge surface disease, involves collecting sample of surface disease of bridge, and subjecting disease segmentation images for outputting combined calculation result as disease detection result.
AU WANG B
   RUAN X
   JING G
   WANG Z
   ZHAO X
   WANG X
   CHAI X
   MA C
   YI J
   ZHANG D
AE CHINA RAILWAY MAJOR BRIDGE SCI RES INST (CREN-C)
   CHINA RAILWAY MAJOR BRIDGE ENG CO LTD (CREN-C)
GA 201900637D
AB    NOVELTY - The method involves collecting a sample of a surface disease of a bridge. A network model of disease semantic segmentation is obtained by using a deep learning algorithm to train the sample. Semantics is segmented according to disease. A segmented image is converted into a binarized image by using network model segments of a disease image. A single binarized image is identified for calculating area of connected region in the binarized image. A disease segmentation images is sequentially subjected for outputting a combined calculation result as a disease detection result.
   USE - Method for detecting bridge surface disease.
   ADVANTAGE - The method enables comparing total area and diseases maximum area with current technology for improving detection accuracy and bridge disease.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for detecting bridge surface disease. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04A; T01-J10B2; T01-J10D; T01-J16C2; T01-J16C3; T01-N01B3
IP G06T-007/00; G06T-007/11
PD CN109064449-A   21 Dec 2018   G06T-007/00   201913   Pages: 8   Chinese
AD CN109064449-A    CN10726585    04 Jul 2018
PI CN10726585    04 Jul 2018
UT DIIDW:201900637D
ER

PT P
PN CN109062177-A
TI Neural network based mechanical device fault diagnosis method, involves performing image data training process, performing image data training process by using depth neural network classifier, and obtaining fault diagnosis result.
AU WANG C
   YI Y
   JIA Z
AE WUXI YITONG PRECISION MACHINERY CO LTD (WUXI-Non-standard)
GA 201900693Q
AB    NOVELTY - The method involves obtaining mechanical device fault attribute by a fault diagnosis expert system. Fault attribute expansion process is performed. A complete characteristic data set is obtained. The complete characteristic data set is processed. A mechanical equipment vibration signal is collected according to the complete characteristic data set. Image data training process is performed by using a depth neural network classifier. A fault diagnosis result is obtained. An attribute expansion neural network is established based on a deep neural network Adam algorithm. A characteristic node is determined.
   USE - Neural network based mechanical device fault diagnosis method.
   ADVANTAGE - The method enables converting knowledge of expert system knowledge base into neural network trainable data so as to obtain training result of a large sample in an effective manner, and improving mechanical equipment fault diagnosis accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a neural network based mechanical equipment fault diagnosis system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a neural network based mechanical device fault diagnosis system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T06 (Process and Machine Control)
MC T01-J10B2; T01-J16A; T01-N01B3; T06-A08
IP G05B-023/02
PD CN109062177-A   21 Dec 2018   G05B-023/02   201913   Pages: 11   Chinese
AD CN109062177-A    CN10713197    29 Jun 2018
PI CN10713197    29 Jun 2018
UT DIIDW:201900693Q
ER

PT P
PN CN109062692-A
TI Human face recognition depth learning training platform optimizing method, involves testing GPU card number, and optimizing configuration of predetermined platform, component parameters and component connection according to tested results.
AU WANG P
AE ZHENGZHOU YUNHAI INFORMATION TECHNOLOGY (INEI-C)
GA 201900681D
AB    NOVELTY - The method involves establishing a predetermined human face recognition model for testing a usage state of a CPU. Read-write state of IOPS data of a disk is tested. Usage state of GPU memory bandwidth and usage state of a GPU memory core are tested. GPU card number is tested. Configuration of a predetermined platform, component parameters and component connection are optimized according to tested results. Determination is made to check whether storage and reading of data in the disk affects training efficiency of the predetermined platform or not by analyzing read-write speed of the disk.
   USE - Human face recognition depth learning training platform optimizing method.
   ADVANTAGE - The method enables solving problems of low cost performance, waste cost limitation caused by simply relying on stacked hardware and changing configuration on a server, saving training platform optimizing cost, and effectively improving training speed, training stability and production efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face recognition depth learning training platform optimizing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face recognition depth learning training platform optimizing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-F02C2; T01-J10B2A; T01-M06S; T01-N01B3A; W04-W05A
IP G06F-009/50; G06K-009/00
PD CN109062692-A   21 Dec 2018   G06F-009/50   201913   Pages: 19   Chinese
AD CN109062692-A    CN10819416    24 Jul 2018
PI CN10819416    24 Jul 2018
UT DIIDW:201900681D
ER

PT P
PN CN109063247-A
TI Deep belief network based landslide disaster forecasting method, involves optimizing DBN network by using genetic algorithm, and classifying output result into landslide levels for predicting probability of landslide occurrence.
AU WEN Z
   CHENG S
   LI L
   LIU D
AE UNIV XIAN POLYTECHNIC (UYXP-C)
GA 2019006678
AB    NOVELTY - The method involves establishing a landslide monitoring pre-warning system for collecting large number of disaster-induced factor. The disaster-induced factor is divided into a test sample, a training sample and an evolutionary sample according to specific proportion. A landslide disaster forecasting model is established based on a deep belief network. A network parameter is updated by pre-training a RBM. A DBN network is optimized by using a genetic algorithm. An output result is classified into landslide levels to predict probability of landslide occurrence. A main control chip STM32 is connected with a Universal Serial Bus (USB) interface, a General Packet Radio Service (GPRS) module and a ZigBee module and a RS485 module.
   USE - Deep belief network based landslide disaster forecasting method.
   ADVANTAGE - The method enables improving convergence speed by extracting the disaster-induced factor so as to prevent local landslide disaster forecasting optimal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep belief network based landslide disaster forecasting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-C07C4A; T01-J15X; T01-J16C4; T01-N01B3A; T01-N02B2B
IP G06F-017/50; G06N-003/04; G06N-003/08
PD CN109063247-A   21 Dec 2018   G06F-017/50   201913   Pages: 17   Chinese
AD CN109063247-A    CN10667868    26 Jun 2018
PI CN10667868    26 Jun 2018
UT DIIDW:2019006678
ER

PT P
PN CN109063658-A
TI Method for replacing faces of persons in video of multi-mobile terminal based on deep learning, involves training generated model, and synthesizing multi-person face-replacing video with generated model and reference video.
AU WU P
AE WU P (WUPP-Individual)
GA 201902078V
AB    NOVELTY - The method involves acquiring picture subsets of different faces in a reference video containing multiple faces. A picture set is divided into multiple subsets according to different faces. A set of to-be-replaced face pictures is uploaded and generated by a mobile terminal. A to-be-replaced face picture set is generated according to a corresponding video or picture uploaded by the mobile terminal. A generated model is trained. A multi-person face-replacing video is synthesized with the generated model and the reference video containing multi-faces.
   USE - Method for replacing faces of persons in a video of a multi-mobile terminal based on deep learning in internet artificial intelligent video processing field.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for replacing faces of persons in a video of a multi-mobile terminal based on deep learning. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J10B1; T01-J10B2; T01-N01B3; T01-N01D1B; W04-W05A
IP G06K-009/00; G06T-005/50
PD CN109063658-A   21 Dec 2018   G06K-009/00   201913   Pages: 7   Chinese
AD CN109063658-A    CN10897882    08 Aug 2018
PI CN10897882    08 Aug 2018
UT DIIDW:201902078V
ER

PT P
PN CN109064461-A
TI Deep learning network based steel rail surface defect detecting method, involves changing parameters of depth learning neural network model, and completing test process when test accuracy is equal to or greater than desired value.
AU ZHANG H
   SONG Y
   LIU L
   ZHONG H
   LIANG Z
AE UNIV CHANGSHA SCI & TECHNOLOGY (UYSG-C)
GA 2019006374
AB    NOVELTY - The method involves collecting a rail image. A rail defect position is manually marked in a part of the rail image. A residual part in the rail image is utilized as a test sample set. A deep learning neural network model is trained. A rail defect training set is input to the trained deep learning neural network model. Training process is completed when depth learning loss of the neural network model reaches a preset value. Samples of the rail defect training set and the test sample set are tested. Parameters of the depth learning neural network model are changed if accuracy of the testing process is lesser than a desired value. The test process is completed when the test accuracy is equal to or greater than the desired value.
   USE - Deep learning network based steel rail surface defect detecting method.
   ADVANTAGE - The method enables accurately identifying defects so as to effectively improve steel rail surface defect detection and identification rate.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a deep learning network based steel rail. '(Drawing includes non-English language text)'
DC S03 (Scientific Instrumentation); T01 (Digital Computers)
MC S03-E04F; T01-J10B2; T01-N01B3A
IP G06T-007/00; G06T-007/12; G06T-007/181; G01N-021/88
PD CN109064461-A   21 Dec 2018   G06T-007/00   201913   Pages: 11   Chinese
AD CN109064461-A    CN10885506    06 Aug 2018
PI CN10885506    06 Aug 2018
UT DIIDW:2019006374
ER

PT P
PN CN109063887-A
TI Criminal hot spot prediction method, involves establishing convolution neural network model, obtaining common characteristics of kde-spot pattern, and establishing hotspot graph based on collaborative crime information.
AU ZHANG Z
   YI W
   SHI Y
   LIANG B
   ZHAO G
AE CHRTC TECHNOLOGY GROUP CO LTD (CHRT-Non-standard)
GA 201900650T
AB    NOVELTY - The method involves acquiring source data (S1). Kde-point map of different areas is established based on source data (S2). A convolution neural network model is established. The convolutional neural network model is trained (S3). Common characteristics of kde-spot pattern are obtained. Hotspot graph is established based on collaborative crime information, where the source data comprises hotspot pattern. Multiple criminal spot distribution types are determined. A criminal hotspot graph is established. Convolution kernel number is obtained.
   USE - Criminal hot spot prediction method.
   ADVANTAGE - The method enables solving the technical problem of how to discover collaborative cases in different regions and improving criminal hot spot prediction probability.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a criminal hot spot prediction system
   (2) a storage medium for storing a set of instructions for executing a criminal hot spot prediction process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a criminal hot spot prediction method. '(Drawing includes non-English language text)'
   Step for acquiring source data (S1)
   Step for establishing de-point map of different areas based on source data (S2)
   Step for training convolutional neural network model (S3)
DC T01 (Digital Computers)
MC T01-J04B2; T01-N01A2; T01-N01D
IP G06Q-010/04; G06Q-050/26
PD CN109063887-A   21 Dec 2018   G06Q-010/04   201913   Pages: 13   Chinese
AD CN109063887-A    CN10619599    13 Jun 2018
PI CN10619599    13 Jun 2018
UT DIIDW:201900650T
ER

PT P
PN CN109061558-A
TI Deep learning based voice collision detection and sound source positioning method, involves centralizing depth learning model according to training set when training result is obtained at optimal sound source position.
AU ZHONG Y
   WEI T
   JIANG J
   LUO X
AE UNIV GUILIN ELECTRONIC TECHNOLOGY (UYGE-C)
GA 2019007088
AB    NOVELTY - The method involves establishing a voice collision detection and sound source location system, where the sound collision detection and sound source locating system is provided with a microphone array, a sound source, a noise source and an acoustic sensing device. The sound source and the noise source are attached together by the microphone array and the acoustic sensing device. Sound is generated by the noise source. Sound data is determined at a sound source position. A depth learning model is centralized according to training set when the training result is obtained at an optimal sound source position.
   USE - Deep learning based voice collision detection and sound source positioning method.
   ADVANTAGE - The method enables ensuring better sound locating detection function and improving quick detection speed and real-time performance in an effective manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based voice collision detection and sound source positioning method. '(Drawing includes non-English language text)'
DC W06 (Aviation, Marine and Radar Systems)
MC W06-A02E; W06-A03F
IP G01S-003/80; G01S-005/18
PD CN109061558-A   21 Dec 2018   G01S-003/80   201913   Pages: 9   Chinese
AD CN109061558-A    CN10640649    21 Jun 2018
PI CN10640649    21 Jun 2018
UT DIIDW:2019007088
ER

PT P
PN CN108738444-A
TI Depth learning system based tractor farming method, involves sending real-time agricultural land feedback to automatic learning system by speed sensor, and lifting plow arm when tractor working state is stopped.
AU WAN Z
   HU X
   LI Y
AE LUOYANG ZHONGKE LONGWANG INNOVATION TECH (LUOY-Non-standard)
GA 201888478U
AB    NOVELTY - The method involves providing a positioning system feedback to a tractor for sensing a ground boundary. The ground boundary is identified when a camera is connected with a vehicular computer. Boundary information and position information of the tractor are obtained. The ground boundary is provided with an infrared sensor. Thickness of a fertile soil layer is determined. Grade depth is adjusted by a plow arm. A real-time agricultural land feedback is sent to an automatic learning system by a speed sensor. The plow arm is lifted when the tractor working state is stopped.
   USE - Depth learning system based tractor farming method.
   ADVANTAGE - The method enables simplifying tractor farming operation so as to improve working efficiency, crops yield, popularization rate and application range.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a depth learning system based tractor farming method. '(Drawing includes non-English language text)'
DC P11 (Soil working, planting (A01B, C).); ; T01 (Digital Computers); T06 (Process and Machine Control); X22 (Automotive Electrics); X25 (Industrial Electric Equipment)
MC T01-J07D1A; T01-J30A; T06-A05; T06-D01; X22-E06B; X22-E09A; X22-G; X22-P09; X22-X06A; X22-X06X; X25-N01A
IP A01B-079/00; F16H-059/36; G05B-013/02
PD CN108738444-A   06 Nov 2018   A01B-079/00   201913   Pages: 8   Chinese
AD CN108738444-A    CN10172134    01 Mar 2018
PI CN10172134    01 Mar 2018
UT DIIDW:201888478U
ER

PT P
PN US2019034761-A1; TW636404-B1; CN109325583-A
TI Deep neural network for recognizing and classifying media data as one of multiple pre-determined data classes, has predictor which is configured to recognize and classify media data as one of multiple pre-determined data classes.
AU HUANG M
   LAI C
AE IND TECHNOLOGY RES INST (ITRI-C)
   IND TECHNOLOGY RES INST (ITRI-C)
GA 2019139793
AB    NOVELTY - The network has a main path in a sequential order of an input layer for receiving a media data, a pooling layer for downsampling an output from the X groups of layers in the main path, and a classification layer for computing a class for each of multiple pre-determined data classes for the media data through the main path. A predictor is configured to recognize and classify the media data as one of multiple pre-determined data classes corresponding to a final class. The deep neural network directs the media data sequentially through alternative path and the main path until the final class is outputted, and outputs either a highest class of a first-ever one of the alternative path and the main path that reaches or exceeds a corresponding class threshold or a highest fused class from the fusion layer based on the class of several of any of the main path and the alternative path that the media data is directed through as the final class.
   USE - Deep neural network for recognizing and classifying media data as one of multiple pre-determined data classes.
   ADVANTAGE - The media data does not have to go through the entire/complete main path, which significantly reduces the computation time and therefore enhances the recognition and classification efficiency. Since the class of paths are fused to calculate the final class, the collaborative decision making mechanism can improve recognition and classification accuracy of the deep neural network.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a method for recognizing and classifying media data; and
   (2) a non-transitory computer-readable medium storing instructions for recognizing and classifying media data.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the deep neutral network.
DC T01 (Digital Computers)
MC T01-J05B2; T01-J07B; T01-J10B2A; T01-S03
IP G06K-009/62; G06N-003/02; G06N-003/04
PD US2019034761-A1   31 Jan 2019   G06K-009/62   201913   Pages: 43   English
   TW636404-B1   21 Sep 2018   G06N-003/02   201913      Chinese
   CN109325583-A   12 Feb 2019   G06N-003/04   201914      Chinese
AD US2019034761-A1    US793086    25 Oct 2017
   TW636404-B1    TW146091    27 Dec 2017
   CN109325583-A    CN10156489    24 Feb 2018
FD  US2019034761-A1 Provisional Application US538811P
PI US538811P    31 Jul 2017
   US793086    25 Oct 2017
UT DIIDW:2019139793
ER

PT P
PN US2017206470-A1
TI Computer-based method for determining patterns for social network platform, involves providing output pattern-identification data to output-interface of social-media-data pattern-identification system.
AU MARCULESCU R
   PENG H
AE UNIV CARNEGIE MELLON (UYCM-C)
GA 2017495200
AB    NOVELTY - The method (100) involves receiving (105) the time-series social-media data set by a social-media-data pattern-identification system. A deep-learning algorithm is applied (110) to the time-series social-media data set. The time-series social-media data set is analyzed for patterns across multiple times and pattern-identification data is outputted by deep-learning algorithm. The output pattern-identification data is provided (115) to an output-interface of the social-media-data pattern-identification system. The deep-learning algorithm comprises a convolutional Bayesian model (CBM).
   USE - Computer-based method for determining patterns with time-series social-media data set for social network platforms e.g. TWITTER (RTM: online news and social networking service). Uses include but are not limited to SNAPCHAT (RTM: image messaging and multimedia mobile application), FACEBOOK (RTM: online social media and social networking service), INSTAGRAM (RTM: mobile, desktop, and internet-based photo-sharing application and service) and YOUTUBE (RTM: video-sharing website).
   ADVANTAGE - The abnormal behaviors in multiple temporal resolutions are detected. The cost associated with the required intervention U is minimized and an ideal outcome Vref is achieved. The reward associated with the outcome V is maximized. The profit is maximized. The mismatch is reduced.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computer-based method for engineering outcome dynamics of a dynamic system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method of identifying patterns within social media data at multiple time scales.
   Method of identifying patterns within social media data at multiple time scales (100)
   Step for receiving time-series social-media data set (105)
   Step for applying deep-learning algorithm (110)
   Step for providing output pattern-identification data (115)
DC T01 (Digital Computers)
MC T01-J16A; T01-N01A2D; T01-N01B4
IP G06N-099/00; G06Q-030/02; G06Q-050/00; H04L-012/58; H04L-029/08
PD US2017206470-A1   20 Jul 2017   G06N-099/00   201749   Pages: 46   English
AD US2017206470-A1    US406268    13 Jan 2017
FD  US2017206470-A1 Provisional Application US388074P
PI US388074P    15 Jan 2016
   US406268    13 Jan 2017
UT DIIDW:2017495200
ER

PT P
PN JP2017004350-A
TI Image processing apparatus such as digital camera, has candidate region preparation assembly that produces candidate region image data which shows candidate regions based on first output data.
AU HIKITA S
AE RICOH KK (RICO-C)
GA 201702397G
AB    NOVELTY - The apparatus has a recognition assembly that recognizes the category according to input image data. The candidate region preparation assembly (125) produces one or more candidate region image data which shows candidate regions based on first output data which shows the output result of the predetermined layer of convolutional neural network. The apparatus recognizes the category according to candidate region image data which is produced by candidate region preparation assembly.
   USE - Image processing apparatus such as digital camera and portable information terminal.
   ADVANTAGE - The apparatus assists the reduction of the processing time of a recognition process.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) image processing method; and
   (2) program for processing image.
   DESCRIPTION OF DRAWING(S) - The drawing shows the block diagram of the image processing apparatus.
   Process unit (110)
   Pooling process unit (113)
   Candidate region creation process unit (120)
   Candidate region preparation assembly (125)
   Output unit (140)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J10B2A; T01-J16C1; T01-M06A1A; W04-M01B1; W04-M01D6
IP G06T-007/00
PD JP2017004350-A   05 Jan 2017   G06T-007/00   201706   Pages: 25   Japanese
AD JP2017004350-A    JP119147    12 Jun 2015
PI JP119147    12 Jun 2015
UT DIIDW:201702397G
ER

PT P
PN CN106228314-A
TI Depth learning enhancing based workflow scheduling method, involves obtaining actual execution environment information, performing task state DAG workflow acyclic graph scheduling process, and inputting scheduling result to sample cell.
AU DUAN H
   MIN J
   ZHANG J
   WANG J
AE UNIV CHINA ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 201680112H
AB    NOVELTY - The method involves obtaining actual execution environment information. DQN neural network training process is performed. A neural network parameter matrix is formed by using depth neural network formula. An executing task is obtained. An output value of the neural network parameter matrix is calculated. A Markov decision process (MDP) model is established to generate an initial task state by using the depth neural network formula. Task state DAG workflow directed acyclic graph scheduling process is performed. Scheduling result is inputted to a sample cell.
   USE - Depth learning enhancing based workflow scheduling method.
   ADVANTAGE - The method enables improving algorithm generalization performance and realizing automatic learning policy scheduling process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a depth learning enhancing based workflow scheduling method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-E03; T01-J04C; T01-J16C1; T01-N01A2E; T01-N01B3
IP G06N-003/08; G06Q-010/06
PD CN106228314-A   14 Dec 2016   G06Q-010/06   201704   Pages: 14   Chinese
AD CN106228314-A    CN10656579    11 Aug 2016
PI CN10656579    11 Aug 2016
CP CN106228314-A
      CN103226759-A   UNIV SUN YAT SEN (UYSY)   CHEN W, YIN L, ZHANG J
      CN103412792-A   CHENGDU GUOKE HAIBO COMPUTER SYSTEM CO (CHEN-Non-standard);  UNIV CHINA ELECTRONIC SCI & TECHNOLOGY (UEST)   YANG J, LIU J, CHEN P, WU L, WU D, HUANG T
      CN104657221-A   GUANGDONG PETROCHEMICAL RES INST (UNGP)   DONG S, SUN H, SHU L, ZUO L
CR CN106228314-A
      VOLODYMYR MNIH: "Playing Atari with Deep Reinforcement Learning", 19 December 2013,relevantClaims[1-5],relevantPassages[1-24]
      : "Q", ,relevantClaims[1-5],relevantPassages[1-2]
UT DIIDW:201680112H
ER

PT P
PN CN109101584-A
TI Depth learning and mathematical analysis combined sentence classification enhancing method, involves calculating number of iterations, and inputting pre-classified sentence data set into classification model to acquire classification result.
AU KWON C
   WANG J
   LIU Y
   LIN X
   LI C
AE UNIV HUNAN (UYHU-C)
GA 201902997H
AB    NOVELTY - The method involves obtaining a sentence classification data set. The data set is combined to a training word vector to form an original word vector table. An AWF weight table is generated by using word frequency to enhance the word vector table. A sentence classification model is constructed, where the model comprises an AWF word vector to represent a layer LSTM network. Number of iterations is calculated to obtain a final classification model. A pre-classified sentence data set is input into a final classification model to acquire a classification result.
   USE - Depth learning and mathematical analysis combined sentence classification enhancing method.
   ADVANTAGE - The method enables increasing reliability of sentence modeling to obtain better sentence semantic feature representation so as to improve accuracy of sentence classification.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a sentence classification model. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J04A; T01-J04C; T01-J05B2; T01-J05B4P; T01-J16C3; T01-N01B3; W04-W05A
IP G06F-017/30
PD CN109101584-A   28 Dec 2018   G06F-017/30   201912   Pages: 16   Chinese
AD CN109101584-A    CN10812774    23 Jul 2018
PI CN10812774    23 Jul 2018
UT DIIDW:201902997H
ER

PT P
PN CN109104731-A
TI Residential area scene category classification model establishing method, involves performing deep learning network model training process performed according to scene category to determine scene category classification model.
AU LI Q
   TANG Y
   MO J
   LIU J
AE GUANGDONG HGICREATE TECHNOLOGY CO LTD (GUAN-Non-standard)
GA 201902928X
AB    NOVELTY - The method involves reducing communication behavior field correlation lower than a threshold value of field data obtaining behavior to obtain characteristic data. Main component characteristic data analyzing process is performed to obtain a behavior characteristic value. Communication action oscillogram screening behavior is represented as a waveform sample. Deep learning network model training process is performed according to scene category to determine a scene category classification model.
   USE - Residential area scene category classification model establishing method.
   ADVANTAGE - The method enables improving scene category updating efficiency and realizing customization of a plan development scheme.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a residential area scene category classification model establishing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a residential area scene category classification model establishing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W01 (Telephone and Data Transmission Systems); W04 (Audio/Video Recording and Systems)
MC T01-J04B2; T01-J05B1; T01-J05B2; T01-J10B2; T01-J10B3A; T01-N01B3; T04-D04; W01-A06A3; W01-A06C4; W01-A06D; W01-A06E; W04-W05A
IP H04W-016/22; H04W-024/02; G06K-009/62
PD CN109104731-A   28 Dec 2018   H04W-016/22   201912   Pages: 23   Chinese
AD CN109104731-A    CN10721971    04 Jul 2018
PI CN10721971    04 Jul 2018
UT DIIDW:201902928X
ER

PT P
PN CN109102126-A
TI Depth transfer learning based theoretical line loss rate prediction method, involves simulating load data and nonlinear mapping relationship between bus voltage data, power supply data and line loss rate to predict line loss rate.
AU LU Z
   YANG Y
   DING Y
   GU Y
AE UNIV YANSHAN (UYAN-C)
GA 2019029840
AB    NOVELTY - The method involves establishing a DBN deep belief network formed by stacked multiple RBM models. An output layer of a DBN depth confidence network is connected to an input layer of a DNN model to form a DBN-DNN depth learning model. The DBN of the DBN-DNN depth learning model is delayed in the depth network. Distribution distance between source data and task prediction data is measured by MMD process. The data is mitigated in a data source sample. Load data is simulated during power grid operation and nonlinear mapping relationship between bus voltage data, power supply data and a line loss rate to predict a line loss rate.
   USE - Depth transfer learning based theoretical line loss rate prediction method.
   ADVANTAGE - The method enables selecting power grid operation data as an input model to predict line loss rate, and avoiding requirements to power grid strong, efficient operation, energy-saving and environment-friendly and construction of intelligent network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a depth transfer learning based theoretical line loss rate prediction method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-N01A2; T01-N01B3; T01-N01D
IP G06Q-010/04; G06Q-050/06; G06N-003/08
PD CN109102126-A   28 Dec 2018   G06Q-010/04   201912   Pages: 17   Chinese
AD CN109102126-A    CN10999797    30 Aug 2018
PI CN10999797    30 Aug 2018
UT DIIDW:2019029840
ER

PT P
PN CN109101638-A
TI Dam deformation monitoring continuity missing data complementing method, involves constructing artificial neural network model, and finishing dam distortion monitoring continuity missing data completion process through deep neural network.
AU MAO Y
   ZHANG J
   GAO J
   CHEN H
   PING P
   WANG L
AE UNIV HOHAI (UYHO-C)
GA 2019029962
AB    NOVELTY - The method involves predicting continuity missing of dam deformation monitoring data by using reverse distance weighting interpolation and bidirectional index smooth interpolation. Spatial similarities between dam deformation monitoring points is calculated by adopting reverse distance weighting interpolation method. Dam deformation monitoring missing data is obtained by performing global spatial interpolation. An artificial neural network model is constructed. Dam distortion monitoring continuity missing data completion process is finished by training nonlinear fusion through a deep neural network.
   USE - Dam deformation monitoring continuity missing data complementing method.
   ADVANTAGE - The method reducing dam distortion monitoring continuity missing data completion problem in an effective manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a dam deformation monitoring continuity missing data complementing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B4P; T01-N01B3; T01-N02B2
IP G06F-011/07
PD CN109101638-A   28 Dec 2018      201912   Pages: 17   Chinese
AD CN109101638-A    CN10946824    20 Aug 2018
PI CN10946824    20 Aug 2018
CP CN109101638-A
      CN104217002-A   UNIV BEIJING AERONAUTICS & ASTRONAUTICS (UNBA)   DU B, LV W, ZHANG X, DU N, XIE Y
      CN105225486-A   HARBIN INST TECHNOLOGY SHENZHEN GRADUATE (HAIT)   LIU L, ZHAO Y
      CN105893610-A   CAS INFORMATION ENG INST (CAIE)   WANG S, YUN X, ZHANG L
      CN106776484-A   UNIV NANJING AERONAUTICS & ASTRONAUTICS (UNUA)   ZHUANG Y, LI J, ZHANG S, SUN J, ZHANG Q, CUI H, HUANG T
      CN107680377-A   UNIV ZHEJIANG GONGSHANG (UYZG)   YANG B, SUN S, LIN X, SONG C
      CN107833153-A   GUANGZHOU POWER SUPPLY CO LTD (CSPG);  UNIV ZHEJIANG (UYZH)   HUA H, DONG S, WANG L, WANG X, LIU Y, WU R, CAI Y, CENG S
      CN107992536-A   UNIV SUN YAT-SEN (UYSY)   HE Z, ZHONG J
      CN108010320-A   UNIV BEIJING TECHNOLOGY (UYBT)   SHI Y, WANG Y, ZHANG Y, YIN B
CR CN109101638-A
      : "", ,relevantClaims[1-7],relevantPassages[1-69]
UT DIIDW:2019029962
ER

PT P
PN CN109101483-A
TI Electrical power inspection text error identification method, involves determining multiple qualitative nodes by using breadth-first search algorithm, performing inspection process, and outputting qualitative description information.
AU WANG H
   LIU Z
AE UNIV ZHEJIANG (UYZH-C)
GA 201902999U
AB    NOVELTY - The method involves establishing a neural network language model in a depth learning field. Euclidean distance between word vectors are determined corresponding to each word. Key information is obtained according to part-of-speech tagging result. Multiple phenomenon and state nodes are determined by using a breadth-first search algorithm. State information is obtained by multiple sub-nodes of a node. Multiple qualitative description nodes are determined by using the breadth-first search algorithm. Inspection process is performed. Qualitative description information is outputted.
   USE - Electrical power inspection text error identification method.
   ADVANTAGE - The method enables improving better interpretation, avoiding influence of data skew, and reducing error identification of a power inspection text.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an electrical power inspection text error identification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B4P; T01-J11A1; T01-N01B3; T01-N03A2
IP G06F-017/27; G06F-017/30; G06N-099/00
PD CN109101483-A   28 Dec 2018   G06F-017/27   201912   Pages: 16   Chinese
AD CN109101483-A    CN10723197    04 Jul 2018
PI CN10723197    04 Jul 2018
UT DIIDW:201902999U
ER

PT P
PN CN109102529-A
TI Deep convolutional neural network based end-to-end hyperspectral image change detecting method, involves inputting affinity matrix to trained network model to obtain end-to-end two-phase hyperspectral image change detecting result.
AU WANG Q
   LI X
   YUAN Z
AE UNIV NORTHWESTERN POLYTECHNICAL (UNWP-C)
GA 201902974V
AB    NOVELTY - The method involves performing radiation correction, geometric correction and image registration preprocess on input two-phase hyperspectral image to obtain preprocessed two-phase hyperspectral image by using ENVI remote sensing image processing software. Pixel in high spectrum image is obtained. The pixel in time-phase hyperspectral image is processed obtain a nonlinear abundance map. The pixel in a hyperspectral image is selected as training data set. An affinity matrix is input to a trained network model to obtain an end-to-end two-phase hyperspectral image change detecting result.
   USE - Deep convolutional neural network based end-to-end hyperspectral image change detecting method.
   ADVANTAGE - The method enables effectively processing high-dimensional information of the hyperspectral image to improve robustness and accuracy of end-to-end hyperspectral image change detection.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep convolutional neural network based end-to-end hyperspectral image change detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-E01B; T01-J10B1; T01-J10B2; T01-N01B3; T01-N03
IP G06T-007/246
PD CN109102529-A   28 Dec 2018   G06T-007/246   201912   Pages: 10   Chinese
AD CN109102529-A    CN10929252    15 Aug 2018
PI CN10929252    15 Aug 2018
UT DIIDW:201902974V
ER

PT P
PN CN109100710-A
TI Convolutional neural network based underwater target identifying method, involves inputting tested data to underwater target identification function of convolutional neural network to obtain identification result of each graph.
AU WU Q
   XU P
   FANG S
AE UNIV SOUTHEAST (UYSE-C)
GA 201903017S
AB    NOVELTY - The method involves obtaining a target tracing beam image for adding a doubling different target label. Data enhancement and size scaling and cutting process are performed in a training sample set and a test sample set. A training sample with a label is input to a convolutional neural network for performing supervised learning to obtain each layer parameter of the convolutional neural network. Underwater target identification function of the convolutional neural network is obtained by each layer parameter of the convolutional neural network. A navigation target of radiation noise is acquired by a towed array. The tested data is input to the underwater target identification function of the convolutional neural network to obtain an identification result of each graph. An identified target is selected as a final identification result.
   USE - Convolutional neural network based underwater target identifying method.
   ADVANTAGE - The method enables identifying high ocean background noise under the underwater target with high precision and speed identification.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based underwater target identifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W06 (Aviation, Marine and Radar Systems)
MC T01-D03; T01-J10B2; T01-J21; T01-N01B3A; T04-D03A; T04-D04; T04-E; W06-A05C3
IP G01S-007/527; G01S-007/536; G01S-007/539; G06K-009/00; G06K-009/62
PD CN109100710-A   28 Dec 2018   G01S-007/527   201912   Pages: 14   Chinese
AD CN109100710-A    CN10669600    26 Jun 2018
PI CN10669600    26 Jun 2018
UT DIIDW:201903017S
ER

PT P
PN CN109102515-A
TI Multi-row deep convolutional neural network based cell counting method, involves performing mathematical integration in cell image to obtain final cell number, and determining average distance between cells to obtain parameter of cell.
AU YAN D
   WANG C
   LI P
   ZHANG M
   JIANG M
   YAN C
AE ZHEJIANG HANGZHOU IRON & STEEL COLD ROLL (HZIS-C)
   UNIV HANGZHOU DIANZI (UYHH-C)
GA 2019029757
AB    NOVELTY - The method involves training a multi-row deep convolutional neural network. A cell image is input to the trained multi-row deep convolutional neural network to output a density image. A multi-row deep convolutional neural network frame is constructed to determine number of columns and number of layers. Image features are extracted from the multi-row deep convolutional neural network by processing the cell image in a corresponding density map. Mathematical integration is performed in the cell image to obtain final cell number. Average distance between cells is determined to obtain parameter of the cell.
   USE - Multi-row deep convolutional neural network based cell counting method.
   ADVANTAGE - The method enables improving robustness and adaptive capability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a multi-row deep convolutional neural network based cell counting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-N01B3
IP G06T-007/12; G06T-007/60; G06N-003/04; G06N-003/08
PD CN109102515-A   28 Dec 2018   G06T-007/12   201912   Pages: 9   Chinese
AD CN109102515-A    CN10856908    31 Jul 2018
PI CN10856908    31 Jul 2018
UT DIIDW:2019029757
ER

PT P
PN CN109102491-A
TI Automatic gastroscope image collecting system, has result output module for recording recognition result, where part classification confidence and picture clear and confidence are weighted and ordered for outputting maximum ranked image.
AU YU H
   HU S
   WU L
AE UNIV WUHAN RENMIN HOSPITAL (UYWU-C)
GA 201902975S
AB    NOVELTY - The system has a convolutional neural network model for classifying a pre-processed image to obtain a position and a focus characteristic. A temporal memory network model performs sequence fitting on a classified result to obtain a recognition result, where the recognition result includes part classification corresponding to a picture and part classification confidence and picture clear and confidence. An image display module displays the recognition result of a video identifying module. A result output module records the recognition result, where the part classification confidence and the picture clear and confidence are weighted and ordered for outputting a maximum ranked image based on the recognition result of the video identifying module.
   USE - Automatic gastroscope image collecting system.
   ADVANTAGE - The system realizes determining length of time of the memory network model of gastroscope video acquisition in real time and avoids losing of target characteristic in the image for optimizing a characteristic-type.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an automatic gastroscope image collecting method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of an automatic gastroscope image collecting system. '(Drawing includes non-English language text)'
DC P31 (Diagnosis, surgery (A61B).); T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J10B2A; T01-N01E; T04-D04; T04-D07D3
IP G06T-007/00; G06K-009/62; A61B-001/273
PD CN109102491-A   28 Dec 2018   G06T-007/00   201912   Pages: 9   Chinese
AD CN109102491-A    CN10690051    28 Jun 2018
PI CN10690051    28 Jun 2018
UT DIIDW:201902975S
ER

PT P
PN CN109104441-A
TI Deep learning based encrypted malicious traffic detection system, has website submission module for receiving traffic PCAP packet uploaded by user on self-built server, and feedback display module for displaying final flow rate.
AU ZOU F
   XU W
   MA Z
   GAO Y
   LI L
AE UNIV SHANGHAI JIAOTONG (USJT-C)
GA 2019029350
AB    NOVELTY - The system has a website submission module for receiving a traffic PCAP packet uploaded by a user on a self-built server. A traffic analyzing and storing module analyzes and stores the traffic PCAP packet uploaded by using HAMBRO-type flow analyzing software. A core analysis module is connected to the traffic analyzing and storing module for pre-processing data by using a recognition model. A feedback display module receives a final identification result generated by the core analysis module to judge whether malicious traffic is detected. The feedback display module displays final flow rate according to a white filtering list.
   USE - Deep learning based encrypted malicious traffic detection system.
   ADVANTAGE - The system can judge maliciousness of the traffic without knowing to-be-decrypted content of the traffic so as to analyze encrypted malicious traffic.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning based encrypted malicious traffic detecting method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based encrypted malicious traffic detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-D01; T01-N01B3; T01-N01D2; T01-N01D3; T01-N02B3; T01-N03
IP H04L-029/06
PD CN109104441-A   28 Dec 2018   H04L-029/06   201912   Pages: 9   Chinese
AD CN109104441-A    CN11244932    24 Oct 2018
PI CN11244932    24 Oct 2018
UT DIIDW:2019029350
ER

PT P
PN CN109087667-A
TI Method for identifying voice fluency, involves determining voice fluency of lower one of each voice fluency as fluency to be detected when voice fluency determined by sequence of consecutive speech frames in detected speech is different.
AU CAI Y
   CHENG N
   WANG J
   XIAO J
AE PINGAN TECHNOLOGY SHENZHEN CO LTD (PING-C)
GA 201902382G
AB    NOVELTY - The method involves constructing (110) a speech recognition model by a sequence-to-sequence deep learning network. The detected speech is pre-processed (120) to obtain a continuous sequence of speech frames. The continuous sequence of speech frames is inputted into the speech recognition model. A voice fluency corresponding to the continuous sequence of speech frames is determined (130) according to the speech recognition model. The determination (140) is made on whether the obtained voice fluency is the same. The voice fluency as the fluency of the client corresponding to the voice to be detected is determined (150) when the voice fluency determined by the sequence of consecutive speech frames in the to-be-detected speech is the same. The voice fluency of the lower one of each of the voice fluency is determined (160) as the fluency of the voice to be detected when the voice fluency determined by the sequence of consecutive speech frames in the to-be-detected speech is different.
   USE - Method for identifying voice fluency.
   ADVANTAGE - The method achieves smarter and more accurate fluency judgment of the customer service voice based on the deep learning network neural that is realized. The accuracy of recognition is self-improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a customer service voice fluency identification device;
   (2) a computing device; and
   (3) a non-transitory computer medium readable storing program for identifying voice fluency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for identifying voice fluency. (Drawing includes non-English language text)
   Step for constructing a speech recognition model by a sequence-to-sequence deep learning network (110)
   Step for pre-processing the detected speech to obtain a continuous sequence of speech frames (120)
   Step for determining a voice fluency corresponding to the continuous sequence of speech frames (130)
   Step for determining whether the obtained voice fluency is the same (140)
   Step for determining voice fluency as the fluency of the client corresponding to the voice to be detected when the voice fluency determined by the sequence of consecutive speech frames in the to-be-detected speech is the same (150)
   Step for determining the voice fluency of the lower one of each of the voice fluency as the fluency of the voice to be detected when the voice fluency determined by the sequence of consecutive speech frames in the to-be-detected speech is different (160)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-C08A; T01-L02; T01-N01B3; W04-V01; W04-V04A; W04-V05
IP G10L-025/30; G10L-025/60; G10L-015/06; G06N-003/04; G06N-003/08
PD CN109087667-A   25 Dec 2018   G10L-025/30   201912   Pages: 16   Chinese
AD CN109087667-A    CN11093169    19 Sep 2018
PI CN11093169    19 Sep 2018
UT DIIDW:201902382G
ER

PT P
PN CN109086868-A
TI Abstract image emotion recognizing method, involves performing migration combination regulation process utilizing natural image emotion data set and abstract image emotion data set to establish abstract image emotion recognition model.
AU CHEN L
   YANG Z
AE UNIV NANJING POSTS & TELECOM (UNPT-C)
GA 2019014694
AB    NOVELTY - The method involves pre-training a convolutional neural network based on a natural image emotion recognition data set. Style characteristic extracting process is performed on the natural image emotion recognition data set and an abstract image emotion recognition data set based on a pre-trained convolutional neural network. Style difference is calculated between samples in the natural image emotion recognition data set and the abstract image emotion recognition data set. Two-layer migration combination regulation process is performed by utilizing the natural image emotion recognition data set and the abstract image emotion recognition data set to establish an abstract image emotion recognition model.
   USE - Abstract image emotion recognizing method.
   ADVANTAGE - The method enables improves the recognition accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an abstract image emotion recognizing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2A; T04-D04
IP G06N-003/04; G06N-003/08; G06K-009/62
PD CN109086868-A   25 Dec 2018   G06N-003/04   201912   Pages: 14   Chinese
AD CN109086868-A    CN10743180    09 Jul 2018
PI CN10743180    09 Jul 2018
UT DIIDW:2019014694
ER

PT P
PN CN109086712-A
TI Statistical method for working time of construction machinery, involves counting working time of construction machinery based on video inter-frame pixel difference information, according to detection results.
AU CHEN W
   WAN H
   XIANG J
   ZHU Z
   CHEN Y
   YIN F
   LI Y
AE SHENZHEN ONE IOT WORLD TECHNOLOGY CO LTD (SHEN-Non-standard)
GA 201902199V
AB    NOVELTY - The method involves collecting (S101) training data of construction machinery. The training data are labeled (S102) and the labeled documents are obtained. The in-depth learning process is utilized (S103) to train the annotated documents and to establish the detection model. The video files of the construction machinery work are obtained (S104). The video file is detected (S105) based on the detection model. The working time of the construction machinery is counted (S106) based on the video inter-frame pixel difference information, according to the detection results.
   USE - Statistical method for working time of construction machinery.
   ADVANTAGE - The image of construction machinery in various physical states is detected by using in-depth learning process to establish detection model. The accuracy of image detection is improved, and the accuracy of working time statistics is improved. The small portion of the video frames is analyzed by analyzing the detected video files based on the inter-frame pixel difference information. The computational cost is small, and the computational resources are saved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a statistical device for working time of construction machinery.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating the statistical method for working time of construction machinery. (Drawing includes non-English language text)
   Step for collecting training data of construction machinery (S101)
   Step for labeling training data and obtaining labeled documents (S102)
   Step for utilizing in-depth learning process to train the annotated documents and to establish the detection model (S103)
   Step for obtaining video files of the construction machinery work (S104)
   Step for detecting video file based on the detection model (S105)
   Step for counting working time of construction machinery based on video inter-frame pixel difference information, according to detection results (S106)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J03; T01-J05A1; T01-J10B1; T01-J10B2; T01-J30A; W04-W05A
IP G06K-009/00; G06K-009/62
PD CN109086712-A   25 Dec 2018   G06K-009/00   201912   Pages: 13   Chinese
AD CN109086712-A    CN10847612    27 Jul 2018
PI CN10847612    27 Jul 2018
UT DIIDW:201902199V
ER

PT P
PN CN109087298-A
TI Alzheimer MRI image classifying method, involves optimizing network parameters to establish test model, inputting test data in test set into test model, and inputting output data into KNN classifier to obtain classification result.
AU CHENG J
   ZHOU J
   SU Y
   GUO H
   LIN L
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 201901458R
AB    NOVELTY - The method involves collecting sufficient number of MRI images from three groups of AD, MCI and NC users (S1). The MRI images are preprocessed (S2) to obtain multiple 2D form in three directions of axial, sagittal and coronal. A deep learning model is established (S3). A sub-network in the deep learning model is trained, where the sub-network in the deep learning model is selected as a convolutional neural network. Network parameters are optimized (S4) by using a counter-propagating and stochastic gradient process to establish a test model. Test data in a test set is input (S5) into the test model. Output data is input into a KNN classifier to obtain classification result.
   USE - Alzheimer MRI image classifying method.
   ADVANTAGE - The method enables avoiding difficult to find suitable data set for pre-training and over-fitting of a 3D convolutional neural network by retaining 3D information to certain extent and realizing separability of characteristics extracted by the deep learning model so as to obtain better classification results of the MRI images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an Alzheimer MRI image classifying method. '(Drawing includes non-English language text)'
   Step for collecting sufficient number of MRI images of three groups of AD, MCI and NC users (S1)
   Step for preprocessing MRI images to obtain multiple 2D form of slices in three directions of axial, sagittal and coronal (S2)
   Step for establishing a deep learning model (S3)
   Step for optimizing network parameters by using a counter-propagating and stochastic gradient process to establish a test model (S4)
   Step for inputting test data in a test set into the test model (S5)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J10B2; T01-N01B3A; T04-D04
IP G06T-007/00; G06K-009/62
PD CN109087298-A   25 Dec 2018   G06T-007/00   201912   Pages: 9   Chinese
AD CN109087298-A    CN10941074    17 Aug 2018
PI CN10941074    17 Aug 2018
UT DIIDW:201901458R
ER

PT P
PN CN109086715-A
TI Navigation and telemetry data middleware system has remote sensing module which preprocesses remote sensing data at target area, performs image semantic segmentation and extracts vector data and raster data to obtain analysis result.
AU DU X
   WANG D
   YANG J
AE TIANTU SOFTWARE TECHNOLOGY CO LTD (TIAN-Non-standard)
GA 2019023812
AB    NOVELTY - The system has a navigation module (10) which is configured to collect location information of a target area and provide a high-precision navigation service. A communication module (20) is configured to acquire location information collected by the navigation module and other information collected by a sensor at the location, and collect remote sensing data according to the location information. A remote sensing module (30) is configured to preprocess the remote sensing data at the target area and perform image semantic segmentation based on the deep learning method on the processed remote sensing data. The remote sensing module extracts the vector data and raster data that are subjected to spatial statistical analysis to obtain an analysis result.
   USE - Navigation and telemetry data middleware system.
   ADVANTAGE - The sensor information communication can be performed under complex conditions. The positioning accuracy of satellite movement is achieved. The satellite information can be easily extracted.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a navigation and telemetry data middleware based data processing method; and
   (2) a computing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the navigation and telemetry data middleware system.
   Navigation module (10)
   Communication module (20)
   Sensing module (30)
DC T01 (Digital Computers)
MC T01-F05E1; T01-J03; T01-J05B4F; T01-J10B2; T01-J16C3; T01-J21; T01-J30A
IP G06K-009/00
PD CN109086715-A   25 Dec 2018   G06K-009/00   201912   Pages: 19   Chinese
AD CN109086715-A    CN10857887    31 Jul 2018
PI CN10857887    31 Jul 2018
UT DIIDW:2019023812
ER

PT P
PN CN109086664-A
TI Method for recognizing dynamic and static fusion polymorphic gesture, involves performing normalized size processing on dynamic and static fused sample sequence to obtain and to input training sample for identifying recognition result.
AU FENG Z
   ZHOU X
AE UNIV JINAN (UJIN-C)
GA 2019022004
AB    NOVELTY - The method involves continuously capturing a sequence of gestures made by the user by using Kinect (RTM: Line of motion sensing input device produced by Microsoft). The dynamic and static fusion processing is performed on the captured gesture sequence to obtain a sample sequence of dynamic and static fusion. A normalized size processing is performed on the dynamic and static fused sample sequence to obtain a training sample. The training sample is inputted into the trained deep learning model to identify and to obtain the recognition result.
   USE - Method for recognizing dynamic and static fusion polymorphic gesture.
   ADVANTAGE - The recognition rate and robustness of the polymorphic gesture are improved. The intelligent teaching system is better served by using entire interactive teaching interface.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating the dynamic and static fusion polymorphic gesture recognition process. (Drawing includes non-English language text)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J10B2A; T01-J30A; T04-D02; T04-D04; W04-W05A
IP G06K-009/00; G06K-009/62; G06K-009/34; G06F-003/01
PD CN109086664-A   25 Dec 2018   G06K-009/00   201912   Pages: 28   Chinese
AD CN109086664-A    CN10681989    27 Jun 2018
PI CN10681989    27 Jun 2018
UT DIIDW:2019022004
ER

PT P
PN CN109087646-A
TI Method for introducing artificial intelligence ultra-deep learning for voice image recognition, involves connecting CORTEX-A75 processor to cloud server data through network controller.
AU HAN Q
AE WUHAN TUORUICHUANQI TECHNOLOGY CO LTD (WUHA-Non-standard)
GA 2019023759
AB    NOVELTY - The method involves connecting an image recognition module to a camera. A voice recognition module is electrically connected to a microphone. The image recognition module and the voice recognition module are respectively connected to a CORTEX-A75 processor data. The CORTEX-A75 processor is connected to a program memory and a random storage data. The processing and storage of image data and voice data are implemented. The CORTEX-A75 processor is connected to the cloud server data through a network controller.
   USE - Method for introducing artificial intelligence ultra-deep learning for voice image recognition.
   ADVANTAGE - The personnel can quickly identify the corresponding information, and in the process of re-identification, the voice signal can be denoised and filtered, the accuracy of the recognition is improved, and the data is continuously updated and uploaded to the cloud.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the process for introducing artificial intelligence ultra-deep learning for voice image recognition. (Drawing includes non-English language text)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-C08A; T01-J05B2A; T01-J10B2A; T01-N01B3; T01-N02A3C; T04-D04; W04-V01; W04-V05
IP G10L-015/22; G10L-015/02; G10L-021/0208; H04L-029/08; G06K-009/62
PD CN109087646-A   25 Dec 2018   G10L-015/22   201912   Pages: 10   Chinese
AD CN109087646-A    CN11249231    25 Oct 2018
PI CN11249231    25 Oct 2018
UT DIIDW:2019023759
ER

PT P
PN CN109086653-A
TI Handwritten model training method for identifying relatively scribble non-canonical character, involves updating weighting and offset of Chinese handwriting recognition model by using gradient descent-based backward propagation algorithm.
AU HUANG C
   ZHOU G
AE PINGAN TECHNOLOGY SHENZHEN CO LTD (PING-C)
GA 2019022008
AB    NOVELTY - The method involves acquiring (S10) standard Chinese character training sample, and standardized Chinese character training sample is batched according to a preset batch. A convolutional neural network is initialized (S20). The batched standardized Chinese character training samples are input (S30) into the convolutional neural network for training. An error word is obtained whose recognition result does not match the real result, and using all the erroneous words as the erroneous word training sample. The erroneous word training sample is input (S70) into the adjusted Chinese handwriting recognition model for training, and the weighting and offset of the Chinese handwriting recognition model are updated and updated by using a batch gradient descent-based backward propagation algorithm to obtain the target Chinese handwriting identify the model.
   USE - Handwritten model training method for identifying relatively scribble non-canonical character.
   ADVANTAGE - The word sample gets the error word training sample based on the batch gradient descent backward propagation algorithm, the erroneous word training sample is used to update the weight and offset of the Chinese handwriting recognition model to obtain the target Chinese handwriting recognition model. The target Chinese handwriting recognition model with high recognition rate of handwriting is obtained, by adopting the handwritten model training method.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a handwriting recognition device; and
   (2) a computer readable storage medium storing computer program for handwriting recognition.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a handwritten model training method. (Drawing includes non-English language text)
   Step for acquiring standard Chinese character training sample (S10)
   Step for initializing convolutional neural network (S20)
   Step for inputting batched standardized Chinese character training samples (S30)
   Step for acquiring a non-standard Chinese character training sample (S40)
   Step for inputting erroneous word training sample (S70)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2A; T01-N01B3; T01-S03; T04-D04; T04-D07A; T04-D07E; T04-F04
IP G06K-009/00; G06K-009/62; G06N-003/04; G06N-003/08
PD CN109086653-A   25 Dec 2018   G06K-009/00   201912   Pages: 26   Chinese
AD CN109086653-A    CN10564050    04 Jun 2018
PI CN10564050    04 Jun 2018
UT DIIDW:2019022008
ER

PT P
PN CN109086817-A
TI Depth belief network based high-voltage circuit breaker fault diagnosing method, involves training deep belief network fault diagnosis model to classify test set samples to obtain fault classification result of high-voltage circuit breaker.
AU HUANG X
   WANG N
AE UNIV XIAN POLYTECHNIC (UYXP-C)
GA 201901470B
AB    NOVELTY - The method involves selecting experiment data samples of a high-voltage circuit breaker. Standardized sample data is divided into test samples and training samples according to specific ratio. A deep belief network (DBN) fault diagnosis model of the high-voltage circuit breaker is established. The DBN fault diagnosis model is pre-trained by using a layer-by-layer unsupervised greedy learning algorithm. Parameters of the DBN fault diagnosis model are optimized by using a genetic algorithm. The DBN fault diagnosis model of the high-voltage circuit breaker is trained to classify test set samples to obtain a fault classification result of the high-voltage circuit breaker.
   USE - DBN based high-voltage circuit breaker fault diagnosing method.
   ADVANTAGE - The method enables obtaining large data volume samples to realize high-voltage circuit breaker fault diagnosis function in an easy manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a DBN based high-voltage circuit breaker fault diagnosing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-E01B; T01-J05B2; T01-J10B2; T01-J16C2; T01-J16C4; T01-N01B3A
IP G06K-009/62; G06N-003/04; G06N-003/08
PD CN109086817-A   25 Dec 2018   G06K-009/62   201912   Pages: 13   Chinese
AD CN109086817-A    CN10827990    25 Jul 2018
PI CN10827990    25 Jul 2018
UT DIIDW:201901470B
ER

PT P
PN CN109086779-A
TI Convolutional neural network based attention object identifying method, involves performing reverse propagation operation by utilizing cross entropy loss function and smooth function, and optimizing parameter of convolution kernel.
AU JI Z
   KONG Q
   LI S
AE UNIV TIANJIN (UTIJ-C)
GA 2019014719
AB    NOVELTY - The method involves obtaining a characteristic of an image through a convolution neural network to obtain a characteristic map. Characteristic map enhancement process is performed. Characteristic attention operation is executed. Convolution operation is performed for performing sigmoid activation function activating operation. Characteristic map elements are obtained. Target object candidate frames are classified by coordinate regression process. Reverse propagation operation is performed by utilizing cross entropy loss function and smooth function. A parameter of convolution kernel is optimized.
   USE - Convolutional neural network based attention object identifying method.
   ADVANTAGE - The method enables performing target detection operation to maintain information of a small target property map and large target information in deep feature so as to improve target object capability, thus ensuring target detection efficiency, and improving object detection precision.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a convolutional neural network. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B2; T04-D03
IP G06K-009/46; G06N-003/04; G06N-003/08
PD CN109086779-A   25 Dec 2018   G06K-009/46   201912   Pages: 8   Chinese
AD CN109086779-A    CN10849427    28 Jul 2018
PI CN10849427    28 Jul 2018
UT DIIDW:2019014719
ER

PT P
PN CN109086870-A
TI Memristor-based three-dimensional convolutional neural network realizing method, involves performing training process until training precision reaches standard, and obtaining required three-dimensional convolutional neural network.
AU LEE J
   LIU J
   TANG Y
   LI J
AE CHONGQING YINPULE TECHNOLOGY CO LTD (CHON-Non-standard)
GA 2019014692
AB    NOVELTY - The method involves establishing a to-be trained three-dimensional (3D) convolutional neural network based on memristor array, where 3D convolutional neural network comprise a first convolution pooling layer, a second convolution pooling layer and a fully-connected layer. Original video information is input to obtain an output value of the fully-connected layer. The 3D convolutional neural network is trained by performing inverse propagation function according to deviation of the output value of the fully-connected layer and standard information. Judgment is made to check whether training precision reaches standard when set number of trainings is reached. Training process is performed until training precision reaches the standard. A required three-dimensional convolutional neural network is obtained.
   USE - Memristor-based three-dimensional convolutional neural network realizing method.
   ADVANTAGE - The method enables guaranteeing training precision, reducing preparation difficulty and preparation time, and processing the video information by using the 3D convolutional neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a memristor-based three-dimensional convolutional neural network realizing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J10B3A; T01-N01B3
IP G06N-003/04
PD CN109086870-A   25 Dec 2018   G06N-003/04   201912   Pages: 12   Chinese
AD CN109086870-A    CN10845015    27 Jul 2018
PI CN10845015    27 Jul 2018
CP CN109086870-A
      CN105160401-A   UNIV ELECTRONIC SCI & TECHNOLOGY (UEST)   DENG J, CAI J, HU Q, LI C, LIU X, YANG C, YU Y, ZHANG R, MEN Y
      CN105224986-A   UNIV TSINGHUA (UYQI)   DENG L, PEI J, SHI L, LI G
      CN105701541-A   HARBIN INST TECHNOLOGY SHENZHEN GRADUATE (HAIT)   TANG Y, WANG M, ZHAO Q, HUANG J, LIU J, SONG B
      CN106203283-A   UNIV CHONGQING TECHNOLOGY (UYCH-Non-standard)   LIU Z, LI B, FENG X, GE Y, ZHANG L, ZHANG J
      CN108009640-A   UNIV TSINGHUA (UYQI)   ZHANG Q, WU H, YAO P, ZHANG W, GAO B, QIAN H
      US20170083810-A1      
      WO2018016176-A1   DENSO CORP (NPDE)   KATAEVA I, OTSUKA S, AKITA H
UT DIIDW:2019014692
ER

PT P
PN CN109086663-A
TI Convolutional neural network based self-adaptive scale natural scene text detecting method, involves adjusting size of rectangular receptive field, and obtaining text characteristics to increase text detecting precision.
AU LI H
   YUAN Q
   ZHANG B
   WANG Z
   LIU H
AE UNIV DALIAN TECHNOLOGY (UYDA-C)
GA 2019014746
AB    NOVELTY - The method involves learning scale factor of text in an image. A scale self-adaptive frame is designed to increase computational efficiency and text detecting accuracy. Additional scale regression layers are introduced in a VGG-16 basic network to generate scale factor graphs. Loss function prediction box parameter representation is performed under chain rules to update scale regression layer gradient information returned by derivative function. Size of a rectangular receptive field is adjusted. Text characteristics are obtained to increase text detecting precision.
   USE - Convolutional neural network based self-adaptive scale natural scene text detecting method.
   ADVANTAGE - The method enables increasing robust rate, accurately and rapidly improving text location efficiency and ensuring high actual application value.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a scale self-adaptive frame. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-N01B3
IP G06K-009/00; G06N-003/04
PD CN109086663-A   25 Dec 2018   G06K-009/00   201912   Pages: 8   Chinese
AD CN109086663-A    CN10675506    27 Jun 2018
PI CN10675506    27 Jun 2018
UT DIIDW:2019014746
ER

PT P
PN CN109087255-A
TI Mixing loss-based lightweight depth image de-noising method for use in mobile device, involves inputting noisy image by using deep convolutional neural network after depth image denoising network model is learned.
AU LI Y
   MIAO Z
   WANG J
   ZHANG R
   WU B
   ZHANG Y
AE UNIV PLA ARMY ENG (UYPL-Non-standard)
GA 201902359C
AB    NOVELTY - The method involves acquiring (S1) a noise-free image, and constructing a data set for training the de-noising network using a computer to add random noise to the noise-free image, where the data set includes a pair of noisy images and a noise-free image. A lightweight deep convolutional neural network is constructed (S2) as a de-noising network, where the deep convolutional neural network converts the input noisy image into a noiseless image. A mixing loss function including pixel-level recovery loss and the peak signal to noise ratio (PSNR) loss of the image is constructed (S3), and the deep convolutional neural network is trained by using the mixed loss function to obtain a depth image de-noising network model parameter. The noisy image is inputted (S4) by using the deep convolutional neural network after the depth image de-noising network model is learned, where the output of the network is the image after de-noising.
   USE - Mixing loss-based lightweight depth image de-noising method for use in mobile device and embedded device.
   ADVANTAGE - The lightweight depth image de-noising process is realized, and the high quality de-noising effect is achieved in mobile devices and embedded devices with limited computing resources.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the mixing loss-based lightweight depth image de-noising process. (Drawing includes non-English language text)
   Step for acquiring noise-free image (S1)
   Step for constructing lightweight deep convolutional neural network as denoising network (S2)
   Step for constructing mixing loss function (S3)
   Step for inputting noisy image by using deep convolutional neural network (S4)
DC T01 (Digital Computers)
MC T01-J10B1; T01-L02; T01-N01B3
IP G06T-005/00
PD CN109087255-A   25 Dec 2018   G06T-005/00   201912   Pages: 8   Chinese
AD CN109087255-A    CN10794951    18 Jul 2018
PI CN10794951    18 Jul 2018
UT DIIDW:201902359C
ER

PT P
PN CN109086892-A
TI Common dependency tree based visual problem inference model, has convolution layer arranged on long-term memory network and residual combined module, and connecting layer arranged with input part of residual module.
AU LIN J
   LI B
   WANG Q
   LIANG X
AE UNIV SUN YAT-SEN (UYSY-C)
GA 201901468K
AB    NOVELTY - The model has a convolution layer arranged on a long-term memory network in a parallel manner. A recurrent neural network access multilayer perception parts. A topological structure is connected with a recurrent neural network to provide general syntax analysis tree generated guidance. The recurrent neural network is provided with multiple sub-networks, where each sub-network is propped against an attention module and a residual combination module in a parallel manner. A full connection layer is arranged with a bilinear fusion layer in a parallel manner. A convolution layer is arranged on a residual combined module that is provided with two full-connection layers and a bilinear fusion layer in a parallel manner. A connecting layer is arranged with an input part of a residual module.
   USE - Common dependency tree based visual problem inference model.
   ADVANTAGE - The model has high visualization accuracy and degree.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a common dependency tree based visual problem inference system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a common dependency tree based visual problem inference model. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2
IP G06N-003/04; G06N-003/08; G06N-005/04
PD CN109086892-A   25 Dec 2018   G06N-005/04   201912   Pages: 10   Chinese
AD CN109086892-A    CN10623776    15 Jun 2018
PI CN10623776    15 Jun 2018
UT DIIDW:201901468K
ER

PT P
PN CN109087256-A
TI Method for de-blurring target image by using electronic device based deep learning, involves obtaining preset neural network according to fuzzy image sample and clear image sample after training process is performed.
AU LIU Y
   DONG Y
   BAI H
   XIONG F
AE BEIJING FACEALL TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 201901459V
AB    NOVELTY - The method involves obtaining a target image to be blurred. The target image is preprocessed. The pre-processed target image is input to a preset neural network. A de-blurred image is obtained corresponding to the target image according to an output result of the preset neural network. The preset neural network is obtained according to a fuzzy image sample and a clear image sample after training process is performed. Size of the target image to prescribed size is adjusted. The pre-processed target image is down-sampled by using a down-sampling layer. A feature vector is output corresponding to the target image.
   USE - Method for de-blurring target image by using an electronic device (claimed) based deep learning.
   ADVANTAGE - The method enables automatically de-blurring the blurred image by using the preset neural network to obtain the de-blurring image corresponding to the blurred image so as to obtain an accurate fuzzy kernel in advance with wide universality, thus improving efficiency and accuracy of image processing, recognition and application of the de-blurring image.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a non-transitory computer-readable storage medium for storing set of instructions for de-blurring target image by using an electronic device based deep learning
   (2) a system for de-blurring target image by using electronic device based deep learning.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for de-blurring target image by using electronic device based deep learning. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B1; T01-J10B2A; T01-N01B3; T01-S03; T04-D03
IP G06T-005/00; G06N-003/04
PD CN109087256-A   25 Dec 2018   G06T-005/00   201912   Pages: 13   Chinese
AD CN109087256-A    CN10796502    19 Jul 2018
PI CN10796502    19 Jul 2018
UT DIIDW:201901459V
ER

PT P
PN CN109086879-A
TI Implementation method of densely connected neural network based on field programmable gate array (FPGA), involves designing data transmission and reception logic according to characteristics of each layer of densely connected neural network.
AU LU S
   PANG W
   LI Y
   ZHOU S
   FAN X
   XIANG J
AE UNIV SOUTHEAST (UYSE-C)
   UNIV SOUTHEAST WUXI INTEGRATED CIRCUIT (UYSE-C)
GA 201902380D
AB    NOVELTY - The method involves dividing the entire convolutional neural network into several dense connection blocks. A FPGA-side convolution operation module is designed. The data transmission and reception logic of the whole neural network is designed. The size of the storage area required for the input feature map, output feature map, and dense block buffer is designed, such that the send buffer and receive buffer are designed according to the block size and the parallelism of the convolution unit. The data transmission and reception logic are designed according to the characteristics of each layer of the densely connected neural network.
   USE - Implementation method of densely connected neural network based on field programmable gate array (FPGA).
   ADVANTAGE - The width of each layer of the network under the premise of ensuring the accuracy of the algorithm can be reduced such that the number of parameters is reduced, the data transmission efficiency is improved, and the running speed of the neural network is improved.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the data transceiving logic of the whole neural network. (Drawing includes non-English language text)
DC T01 (Digital Computers); U13 (Integrated Circuits)
MC T01-F06; T01-J04B2; T01-J16C1; T01-N01D; U13-C04C
IP G06N-003/06; G06N-003/04
PD CN109086879-A   25 Dec 2018   G06N-003/06   201912   Pages: 12   Chinese
AD CN109086879-A    CN10729915    05 Jul 2018
PI CN10729915    05 Jul 2018
UT DIIDW:201902380D
ER

PT P
PN CN109086886-A
TI Extreme learning machine based convolutional neural network learning algorithm, has set of instruction for outputting CONV parameter according to input character, and classifying pool by using classification algorithm.
AU MA H
   FEI Q
   LI N
   KE L
AE GONGJI BEIJING INTELLIGENT TECHNOLOGY CO (GONG-Non-standard)
GA 201901468Q
AB    NOVELTY - The algorithm has a set of instruction for outputting a CONV parameter according to an input character. Input feature is standardized and filtered by a filter. Standard deviation of data is determined to form a desired target. An input weight is randomly generated. A hidden matrix is constructed. An output weight is calculated. A filter matrix is re-constructed. An input feature map is input. Calculation hidden matrix extracting character is output. A convolution characteristic value is obtained. A pool is classified by using a classification algorithm.
   USE - Extreme learning machine based convolutional neural network learning algorithm.
   ADVANTAGE - The algorithm enables avoiding the need to extract feature by an entire classification model so as to improve training speed and competitiveness of a result.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating an operation of an extreme learning machine based convolutional neural network learning algorithm. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-E03; T01-J04B2; T01-J04C; T01-J05B2; T01-J16C2
IP G06N-003/08
PD CN109086886-A   25 Dec 2018   G06N-003/08   201912   Pages: 14   Chinese
AD CN109086886-A    CN10874817    02 Aug 2018
PI CN10874817    02 Aug 2018
UT DIIDW:201901468Q
ER

PT P
PN CN109086872-A
TI Convolutional neural network based natural earthquake wave judging method, involves performing batch training in process of training by using mini batch process to speed up training, and establishing seismic wave identification model.
AU REN T
   YUAN X
   WANG H
   XIA F
   FU R
   LIU L
   GAO M
AE UNIV CHINESE NORTHEASTERN (UYDB-C)
GA 2019014690
AB    NOVELTY - The method involves collecting model training data. Position of a p-wave start point in seismic data is found by STA/LTA. 169 seconds or 16900 data points are intercepted from the p-wave start point as training data of single station single seismic event to set convolution size and number of different convolution channels by using an eight-convolution layer and a three-coherent convolutional neural network. Batch training is performed in process of training by using mini batch process to speed up training such that number of training samples in a single batch is 256. A seismic wave identification model is established.
   USE - Convolutional neural network based natural earthquake wave judging method.
   ADVANTAGE - The method enables transmitting earthquake data to the network in form of three-component three-channel so as to effectively perform continuous training and debugging, and training a network model for testing so as to adjust composition ratio of the training data, number of iterations and amount of data according to results, thus achieving seismic wave identification with an accuracy of 97.17%.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a convolutional neural network. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J10B2; T01-N01B3A
IP G06K-009/62; G06N-003/04; G06N-003/08
PD CN109086872-A   25 Dec 2018   G06N-003/04   201912   Pages: 8   Chinese
AD CN109086872-A    CN10851740    30 Jul 2018
PI CN10851740    30 Jul 2018
UT DIIDW:2019014690
ER

PT P
PN CN109088407-A
TI Deep belief network pseudo-measurement modeling based power distribution network state estimating method, involves separating virtual measurement process from total measurement to process linear constraint, and calculating node voltage.
AU SUN G
   QIAN Q
   WEI Z
   ZANG H
AE UNIV HOHAI (UYHO-C)
GA 201901434M
AB    NOVELTY - The method involves inputting load history value and weather information to a deep belief network for performing pseudo measurement. Network parameter is obtained for power flow calculation to obtain measurement data in real-time manner. Node voltage initial value and three-phase voltage amplitude value are calculated. Equivalent current value is calculated according to the node voltage initial value. Virtual measurement process is separated from total measurement to process linear constraint. Node voltage is calculated. Weather prediction information of a power distribution network is obtained.
   USE - Deep belief network pseudo-measurement modeling based power distribution network state estimating method.
   ADVANTAGE - The method enables improving power distribution network state estimating accuracy. The method ensures zero power injection of a node so as to avoid condition information matrix problem.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep belief network pseudo-measurement modeling based power distribution network state estimating method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); X12 (Power Distribution/Components/Converters)
MC T01-N01D; X12-H01; X12-H05
IP H02J-003/00
PD CN109088407-A   25 Dec 2018   H02J-003/00   201912   Pages: 17   Chinese
AD CN109088407-A    CN10885249    06 Aug 2018
PI CN10885249    06 Aug 2018
UT DIIDW:201901434M
ER

PT P
PN CN109086267-A
TI Method for Chinese word segmentation based on deep learning, involves incrementally connecting candidate word with previous word segmentation history.
AU WANG C
   SHI Y
   LI Z
AE UNIV NANJING POSTS & TELECOM (UNPT-C)
GA 2019017097
AB    NOVELTY - The method involves performing literal word frequency statistics on the large-scale corpus. Each word in the corpus is initialized to a literal vector, and the obtained literal vector is pressed. The index is saved to the dictionary based on the continuous word bag model and the hierarchical normalization training method. The training corpus is transformed into a fixed-length vector, feeding it into the deep learning model, and refining the literal vector in the dictionary to obtain a feature vector carrying the context semantics and a vector containing the character features. All candidate words ending with the current target word are segmented according to the preset maximum word length. The refined feature vector is merged into the word vector of each candidate word, and the candidate word is incrementally connected with the previous word segmentation history, and the cluster word search method is used for dynamic word segmentation.
   USE - Method for Chinese word segmentation based on deep learning.
   ADVANTAGE - The word segmentation task is freed from the cumbersome feature engineering. The better system performance is obtained by extracting richer feature information, and the complete segmentation history is used for modeling, with sequence-level word segmentation ability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the Chinese word segmentation method based on deep learning. (Drawing includes non-English language text)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-E01C; T01-J03; T01-J05B1; T01-J05B3; T01-J07B; T01-J11A1; T01-J16C3; T01-J30A; W04-W05A
IP G06F-017/27; G06N-003/02
PD CN109086267-A   25 Dec 2018   G06F-017/27   201912   Pages: 30   Chinese
AD CN109086267-A    CN10756452    11 Jul 2018
PI CN10756452    11 Jul 2018
UT DIIDW:2019017097
ER

PT P
PN CN109087545-A
TI Virtual reality and depth learning based training method, involves ensuring actual scene of training game by virtual reality technology, and determining virtual scene implementation of training programs.
AU WANG F
AE WANG F (WANG-Individual)
GA 2019014535
AB    NOVELTY - The method involves ensuring an actual scene of a training game by virtual reality technology. A professional skill practical training competition is determined. The virtual reality technology is utilized for realizing a training scene. Data interaction feedback is obtained for training and competition. A virtual reality training game scene is established for single-player training game or a multi-person training game. A virtual scene implementation of training programs is determined for sports training competitions or professional skills operation competitions.
   USE - Virtual reality and depth learning based training method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a virtual reality and depth learning based training method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J10B2; T01-J30D; T01-J40A; T01-J40B; T04-D04; W04-W07A; W04-W07E; W04-X01
IP G09B-009/00; G06K-009/62
PD CN109087545-A   25 Dec 2018   G09B-009/00   201912   Pages: 7   Chinese
AD CN109087545-A    CN10431397    08 May 2018
PI CN10431397    08 May 2018
UT DIIDW:2019014535
ER

PT P
PN CN109086792-A
TI Fine-grained image classification method, involves establishing neural network structure based on convolutional neural network feature extraction function, and determining characteristics gradients to establish end-to-end network.
AU WANG Y
   ZHANG X
   YU Y
   MA L
AE UNIV SHANGHAI SCI & TECHNOLOGY (USHS-C)
GA 2019014712
AB    NOVELTY - The method involves processing standard fine image data set by using an object detection algorithm. The processed standard fine image data set is input to train and classify a dual-linear convolution neural network to obtain fine-grained image classification result. A bilinear convolution neural network structure is established based on a convolutional neural network feature extraction function. Characteristics gradients are determined to establish an end-to-end network. A bounding box is predicted by using a prior frame. Judgment is made to check whether an object is appeared in an i-th grid.
   USE - Network architecture detecting and identifying process based fine-grained image classification method.
   ADVANTAGE - The method enables avoiding to-be identified object background interference and unrelated information without affecting classification result, obtaining a detected image to classify a linear convolution neural network by a dual-grained classification algorithm, filtering classification of a fine-grained image to extract a dual-linear convolutional neural network, classifying label information in a training process without manual annotation information, enhancing practicability of the dual-grained classification algorithm and increasing recognition rate.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a network architecture detecting and identifying process based fine-grained image classification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J05B2; T01-J10B1; T01-J10B2A
IP G06K-009/62; G06N-003/04
PD CN109086792-A   25 Dec 2018   G06K-009/62   201912   Pages: 11   Chinese
AD CN109086792-A    CN10674058    26 Jun 2018
PI CN10674058    26 Jun 2018
UT DIIDW:2019014712
ER

PT P
PN CN109086802-A
TI Octal number convolutional neural network based image classification method, involves inputting training image, constructing and training number convolutional neural network model, and calculating recognition rate.
AU WU J
   XU L
   KONG Y
   YANG G
   ZHANG P
   YANG C
   JIANG L
   SHU H
AE UNIV SOUTHEAST (UYSE-C)
GA 201901470R
AB    NOVELTY - The method involves inputting a training image. The training image is expresses as an octal matrix. A number convolutional neural network model is constructed and trained. The check image is adjusted to optimal network parameters. A test image is tested. Classify results are obtained. A recognition rate is calculated. Real number batch normalization is performed on an input feature map. A two-dimensional sliding window convolution operation is performed. A parameter optimization process is obtained to best network parameter.
   USE - Octal number convolutional neural network based image classification method.
   ADVANTAGE - The method enables realizing classification task, and obtaining higher classification accuracy of the image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an octal number convolutional neural network based image classification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J05B2; T01-J10B2A; T01-N01B3A; T04-D04
IP G06K-009/62; G06N-003/04
PD CN109086802-A   25 Dec 2018   G06K-009/62   201912   Pages: 22   Chinese
AD CN109086802-A    CN10748292    10 Jul 2018
PI CN10748292    10 Jul 2018
UT DIIDW:201901470R
ER

PT P
PN CN109086836-A
TI Convolutional neural network-based cancer pathological image automatic discriminating device, has detection device for detecting pathological image and inputting to-be-diagnosed image into network to discriminate result of malignant tumor.
AU XIANG L
   LI G
   MA J
AE HUAIYIN TECHNOLOGY INST (HUIA-C)
GA 201901469U
AB    NOVELTY - The device has an image collecting module for collecting esophageal tumor tissue pathology images to construct a cancer pathological slice image base. An image processing module performs rotation, zooming, mirror and combining operation of the esophageal tumor tissue pathology images in a database. A discrimination module scans an esophageal cancer pathological slice image on a trained network and inputs a test set image into the trained network. A detection device detects the pathological image and inputs a to-be-diagnosed image into the trained network to discriminate a discriminating result of malignant tumor.
   USE - Convolutional neural network-based automatic cancer pathological image discriminating device.
   ADVANTAGE - The device avoids over-fitting problem of esophageal pathological image data set in training sample depth learning process, thus improving image recognition rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a convolutional neural network-based automatic cancer pathological image discriminating method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network-based cancer pathological image automatic discriminating device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B4F; T01-J10B1; T01-J10B2A; T01-J10B3A; T04-D04
IP G06K-009/62; G06N-003/04
PD CN109086836-A   25 Dec 2018   G06K-009/62   201912   Pages: 15   Chinese
AD CN109086836-A    CN11022872    03 Sep 2018
PI CN11022872    03 Sep 2018
UT DIIDW:201901469U
ER

PT P
PN CN109086807-A
TI Convolutional neural network based semi-supervised flow learning method, involves transmitting non-tag data to network input end, and obtaining test input and image by using trained model corresponding to light flow output.
AU XIANG X
   CHANG J
   ZHAI M
   LV N
   GUO X
   WANG S
   YU Z
   ZHANG Y
AE UNIV HARBIN ENGINEERING (UHEG-C)
GA 201901470L
AB    NOVELTY - The method involves constructing (S1) optical flow learning sub- networks. An expanding part is provided with two layers of an anti-convolution layer (S2). Two stack networks are constructed (S3). The sub-network is connected with the stack network. Deformation technology is adopted to capture an image. Different resolution of the stack network is output along forward flow and reverse flow directions (S4). An input sensing layer shielding operation is performed (S5). Non-tag data is transmitted to a network input end (S6). Test input and the image are obtained by using a trained model corresponding to the light flow output (S7).
   USE - Convolutional neural network based semi-supervised flow learning method.
   ADVANTAGE - The method enables improving optical flow estimation precision, realizing shielding sensing loss function, and designing a stacked network structure, thus improving network performance.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based semi-supervised flow learning method. '(Drawing includes non-English language text)'
   Step for constructing optical flow learning sub- networks (S1)
   Step for connecting expanding part with anti-convolution layer (S2)
   Step for constructing two stack networks (S3)
   Step for outputting different resolution of stack network along forward flow and reverse flow directions (S4)
   Step for performing input sensing layer shielding operation (S5)
   Step for transmitting non-tag data to network input end (S6)
   Step for obtaining test input and the image (S7)
DC T01 (Digital Computers)
MC T01-J10B2; T01-J10D; T01-N01B3A; T01-N01D1B
IP G06K-009/62; G06N-003/04; G06N-003/08
PD CN109086807-A   25 Dec 2018   G06K-009/62   201912   Pages: 11   Chinese
AD CN109086807-A    CN10779483    16 Jul 2018
PI CN10779483    16 Jul 2018
UT DIIDW:201901470L
ER

PT P
PN CN109086649-A
TI Method for satellite remote sensing image identification, involves obtaining stack self-encoding neural network parameters and accurately extracting water information.
AU YANG L
   ZHAO M
   WANG X
   HU H
   HE W
   FENG L
   WANG H
   MA B
   XU X
   LIU Q
   LI Y
   LI Z
   HU M
   WANG K
   JING K
   WANG B
   ZHANG X
AE STATE GRID XINJIANG ELECTRIC POWER CO (SGCC-C)
   STATE GRID CORP CHINA (SGCC-C)
GA 201902200B
AB    NOVELTY - The method involves calculating corresponding feature values of each pixel in the relevant set according to a given feature algorithm and formula. The average value of the feature values of all adjacent pixels is calculated. The feature averages are obtained as new features of the target pixel points to form a new feature set. The water body information is accurately extracted. Multiple feature values expanded in the third step are inputted as a model. The deep learning model is trained. The stack self-encoding neural network parameters are obtained. The water information is accurately extracted.
   USE - Method for satellite remote sensing image identification.
   ADVANTAGE - The improved method performs co-training on the deep learning model to achieve accurate extraction of water information.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the method for satellite remote sensing image identification. (Drawing includes non-English language text)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B1; T01-J10B2; T01-N01B3; T04-D04
IP G06K-009/00; G06K-009/62
PD CN109086649-A   25 Dec 2018   G06K-009/00   201912   Pages: 21   Chinese
AD CN109086649-A    CN10532639    29 May 2018
PI CN10532639    29 May 2018
UT DIIDW:201902200B
ER

PT P
PN CN109086777-A
TI Global pixel feature based saliency map refinement method, involves extracting depth characteristics based on ultra-pixel, and performing convolutional neural network classifying process to obtain refined final saliency map.
AU YANG M
   ZHANG S
AE UNIV NANJING NORMAL (UNNO-C)
GA 201901471B
AB    NOVELTY - The method involves extracting depth characteristics based on ultra-pixel (S1). A saliency detection model is established to generate an initial saliency map. An input image pre-processing process is performed (S2), where the initial saliency map comprises image spectrum. An RGB image is converted into a BGR image. An image global pixel extracting process is performed by a convolutional neural network depth feature map (S3). A depth characteristic graph is spliced to obtain current characteristics (S4). A convolutional neural network classifying process is performed to obtain a refined final saliency map (S5).
   USE - Global pixel feature based saliency map refinement method.
   ADVANTAGE - The method enables accurately finishing image significant object detection task in a large scale.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a global pixel feature based saliency map refinement method. '(Drawing includes non-English language text)'
   Step for extracting depth characteristics based on ultra-pixel (S1)
   Step for performing input image pre-processing process (S2)
   Step for performing image global pixel extracting process by convolutional neural network depth feature map (S3)
   Step for splicing depth characteristic graph to obtain current characteristics (S4)
   Step for performing convolutional neural network classifying process to obtain refined final saliency map (S5)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J10B3; T01-J10D; T04-D03; T04-D04; T04-D08; T04-E
IP G06K-009/46; G06K-009/62; G06T-007/11
PD CN109086777-A   25 Dec 2018   G06K-009/46   201912   Pages: 10   Chinese
AD CN109086777-A    CN10743616    09 Jul 2018
PI CN10743616    09 Jul 2018
UT DIIDW:201901471B
ER

PT P
PN CN109086742-A
TI Scene recognition method involves performing scene recognition on image to be processed to obtain scene label matrix, and determining scene category of image to be processed according to scene label matrix.
AU ZHANG G
AE OPPO GUANGDONG MOBILE COMMUNICATION CO (GDOP-C)
GA 201902380Y
AB    NOVELTY - The method involves obtaining (S101) the image to be processed. The scene recognition is performed (S102) on the image to be processed to obtain a scene label matrix including multiple scene labels, where the scene label indicates a scene category. The scene category of the image to be processed is determined (S103) according to the scene label matrix. The image to be processed is input to the trained neural network, and a scene label matrix including multiple scene labels output by the convolutional neural network is obtained.
   USE - Scene recognition method.
   ADVANTAGE - The problem that the information in the image is easily lost when the scene is recognized, and the reduction of the scene recognition rate is solved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a scene recognition device;
   (2) a mobile terminal; and
   (3) a computer-readable storage medium storing instructions for performing scene recognition method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating scene recognition method. (Drawing includes non-English language text)
   Step for obtaining the image to be processed (S101)
   Step for performing the scene recognition on the image to be processed to obtain a scene label matrix including multiple scene labels (S102)
   Step for determining scene category of image to be processed according to scene label matrix (S103)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2A; T01-J10B3A; T01-S03; T04-D07B1
IP G06K-009/00
PD CN109086742-A   25 Dec 2018   G06K-009/00   201912   Pages: 18   Chinese
AD CN109086742-A    CN10981469    27 Aug 2018
PI CN10981469    27 Aug 2018
UT DIIDW:201902380Y
ER

PT P
PN CN109086737-A
TI Video recognition method for shipping cargo monitoring, involves classifying and identifying current shipping events according to neural network classifier, and determining classification of current shipping events.
AU ZHANG H
   LIU W
   DAI G
   ZHAO L
AE WUHAN HENGSHI TUAN TECHNOLOGY CO LTD (WUHA-Non-standard)
GA 2019023810
AB    NOVELTY - The method involves acquiring (101) shipping monitoring data. The shipping monitoring data is included with images captured by ship-borne camera at intervals of time, camera number, shooting time and types of shipping events. The image data base is established (102) according to shipping monitoring data. The training set and test set are generated (103) according to image database. The deep residual network model is established (104) according to training set and test set. The image features of each image in the image database are extracted (105) and the image feature database is established, according to depth residual network model. The image feature database is included with image features and image feature vectors. The neural network classifier is established (106) according to the image feature database. The current shipping events are classified (107) and identified, according to the neural network classifier, and the classification of the current shipping events is determined.
   USE - Video recognition method for shipping cargo monitoring based on convolutional neural network.
   ADVANTAGE - The identification accuracy of current shipping events is improved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a video recognition system for shipping cargo monitoring based on convolutional neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating the video recognition method for shipping cargo monitoring based on convolutional neural network. (Drawing includes non-English language text)
   Step for acquiring shipping monitoring data (101)
   Step for establishing image data base according to shipping monitoring data (102)
   Step for generating training set and test set according to image database (103)
   Step for establishing deep residual network model according to training set and test set (104)
   Step for extracting image features of each image in the image database and establishing the image feature database (105)
   Step for establishing neural network classifier according to the image feature database (106)
   Step for classifying and identifying current shipping events according to the neural network classifier, and determining classification of the current shipping events (107)
DC S01 (Electrical Instruments); T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC S01-H07A; T01-J05B2; T01-J05B4F; T01-J07D1; T01-J10B2A; T01-N01A2E; T01-N01B3A; T01-N02B2; T04-D04
IP G06K-009/00; G06K-009/62; G06N-003/04
PD CN109086737-A   25 Dec 2018   G06K-009/00   201912   Pages: 16   Chinese
AD CN109086737-A    CN10952859    21 Aug 2018
PI CN10952859    21 Aug 2018
UT DIIDW:2019023810
ER

PT P
PN CN109086569-A
TI Method for predicting relationship between protein interaction direction and regulation, involves training recognition model and predicting interaction direction and regulation relationship of protein according to training model.
AU ZHANG L
   CAO Y
   SHI Z
AE WUHAN DEEPBIO BIOTECHNOLOGY CO LTD (WUHA-Non-standard)
GA 201902200L
AB    NOVELTY - The method involves performing homology modeling of the interacting proteins to obtain the homologous spatial structure of each interacting protein. The spatial structure of the protein complex is collected, and the homologous spatial structure of each interacting protein is superimposed with each sub-unit of the protein complex to obtain a spatial structure of the simulated complex. A stable complex structure is selected from simulated complex. The direction and frequency of the amino acid residues in the protein interaction interface are calculated, and an orientation contact map of amino acid residues of the interaction protein is generated according to protein interaction model with spatial structure resolution. The target feature of the directed contact map is extracted by convolutional neural network, and recognition model is trained to obtain the training model. The interaction direction and regulation relationship of the protein are predicted according to the training model.
   USE - Method for predicting relationship between protein interaction direction and regulation.
   ADVANTAGE - The method for predicting relationship between protein interaction direction and regulation is accurate and efficient.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a directional and regulatory relationship.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the method for predicting protein interaction direction and regulation relationship. (Drawing includes non-English language text)
DC T01 (Digital Computers)
MC T01-N01B3
IP G06F-019/18; G06N-003/04
PD CN109086569-A   25 Dec 2018   G06F-019/18   201912   Pages: 9   Chinese
AD CN109086569-A    CN11090240    18 Sep 2018
PI CN11090240    18 Sep 2018
UT DIIDW:201902200L
ER

PT P
PN CN109086824-A
TI Convolutional neural network based submarine sonar image classifying method, involves performing validating process on convolutional neural network classifier structure, and classifying test set according to classification model.
AU ZHAO Y
   FU N
   LIU C
   ZHAO T
   WAN H
   DONG J
   ZHANG W
   ZHU K
AE UNIV HARBIN ENGINEERING (UHEG-C)
GA 2019014705
AB    NOVELTY - The method involves calculating and generating a gray-cells co-occurrence matrix according to edge shape information and gray value of an edge image. A convolution neural network input sample set is obtained from a gray image set formed by the grayscale image. The convolution neural network input sample set is divided into a training set, a validation set and a testing set. A convolution neural network classifier structure is constructed. Training and validating processes are performed on the convolution neural network classifier structure to obtain a classification model. The test set is classified and identified according to the classification model.
   USE - Convolutional neural network based submarine sonar image classifying method.
   ADVANTAGE - The method enables learning and training different types of submarine sediments through learning strategy of the convolutional neural network classifier structure, and obtaining classification function of the classification model to perform accurate classification process on the submarine medium sonar image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a convolutional neural network based submarine sonar image classifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J05B2; T01-J10B2; T01-N01B3A; T04-D03; T04-D04
IP G06K-009/46; G06K-009/62; G06N-003/04
PD CN109086824-A   25 Dec 2018   G06K-009/62   201912   Pages: 11   Chinese
AD CN109086824-A    CN10864966    01 Aug 2018
PI CN10864966    01 Aug 2018
UT DIIDW:2019014705
ER

PT P
PN CN109087380-A
TI Method for generating cartoon animation, involves generating comic animation including continuous change expression of comic face with second image as starting frame, according to second image and first difference vector.
AU ZHONG Y
   MA J
   ZHAO L
   MO D
   ZHANG J
AE MIGU CULTURE TECHNOLOGY CO LTD (MIGU-Non-standard)
   CHINA MOBILE COMMUNICATIONS GROUP CO (CNMO-C)
GA 2019023864
AB    NOVELTY - The method involves inputting a first image to the convolutional neural network model. A second image that corresponds to the first image output by the convolutional neural network model is received, and a first difference vector corresponding to the expression of the real human face. The first difference vector is used to represent an image difference between a multi-frame image corresponding to a continuously changing expression of the facial expression of the comic face in the second image as an expression starting point and a previous frame image. A comic animation including the continuous change expression of the comic face is generated with the second image as a starting frame, according to the second image and the first difference vector.
   USE - Method for generating cartoon animation.
   ADVANTAGE - The process is improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a cartoon animation generating device; and
   (2) a computer-readable storage medium storing program for generating cartoon animation.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the process for generating cartoon animation. (Drawing includes non-English language text)
   Step for inputting a first image to the convolutional neural network model (101)
   Step for receiving a second image corresponding to the first image output by the convolutional neural network model, and a first difference vector corresponding to the expression of the real human face (102)
   Step for generating comic animation including the continuous change expression of the comic face with the second image as a starting frame, according to the second image and the first difference vector (103)
DC T01 (Digital Computers)
MC T01-S03
IP G06T-013/80; G06N-003/04
PD CN109087380-A   25 Dec 2018   G06T-013/80   201912   Pages: 21   Chinese
AD CN109087380-A    CN10872328    02 Aug 2018
PI CN10872328    02 Aug 2018
UT DIIDW:2019023864
ER

PT P
PN CN109063678-A
TI Method for identifying face image by using computer device, involves establishing depth convolutional neural network model, and performing correcting operation in three-dimensional facial image based on normal component map characteristic.
AU CHEN R
AE BEIJING ZIJIETIAODONG NETWORK TECHNOLOGY (BEIJ-Non-standard)
GA 2019006561
AB    NOVELTY - The method involves obtaining a three-dimensional facial image of a to-be-identified object. A coordinate is projected to a two-dimensional plane of the three-dimensional facial image. Characteristic of a normal component map is extracted. A depth convolutional neural network model is established in the two-dimensional plane. Correcting operation is performed in the three-dimensional facial image according to characteristic of the normal component map. Gaussian and median filtering operation is performed in the three-dimensional facial image.
   USE - Method for identifying a face image by using a computer device (claimed).
   ADVANTAGE - The method enables performing identifying operation in a precise manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for identifying a face image by using a computer device
   (2) a computer readable storage medium for storing a set of instructions for identifying a face image by using a computer device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for identifying a face image by using a computer device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J10B3; T04-D07F1
IP G06K-009/00; G06N-003/04
PD CN109063678-A   21 Dec 2018   G06K-009/00   201912   Pages: 16   Chinese
AD CN109063678-A    CN10975866    24 Aug 2018
PI CN10975866    24 Aug 2018
UT DIIDW:2019006561
ER

PT P
PN CN109064460-A
TI Multi-time attribute element depth characteristics based wheat severe disease prediction method, involves inputting to-be-predicted images and environment data to wheat severe disease prediction model to obtain prediction result of disease.
AU CHEN T
   WANG R
   XIE C
   ZHANG J
   LI R
   CHEN H
   HU H
AE CHINESE ACAD SCI HEFEI INST PHYS SCI (CAWZ-C)
GA 2019006375
AB    NOVELTY - The method involves obtaining a multi-image data set and environment data captured by a drone. A wheat severe disease prediction model is constructed by using a deep convolutional neural network and a sequence information storage network. The sequence information storage network and the deep convolutional neural network are trained together. The multi-image data set is utilized as a training sample of the deep convolutional neural network. The environment data is utilized as a training sample of the sequence information storage network. To-be-predicted images and to-be-predicted environment data are obtained. The to-be-predicted images and the to-be-predicted environment data are input to the wheat severe disease prediction model to obtain a prediction result of a wheat severe disease.
   USE - Multi-time attribute element depth characteristics based wheat severe disease prediction method.
   ADVANTAGE - The method enables utilizing the sequence information storage network and the deep convolutional neural network to combine sequence attribute elements of the wheat sever disease so as to ensure automatic learning and learning degree of the wheat diseases at different periods in a data sequence, thus realizing wheat severe diseases prediction.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-time attribute element depth characteristics based wheat severe disease prediction method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-N01A2; T01-N01B3
IP G06T-007/00; G06Q-010/04; G06Q-050/02
PD CN109064460-A   21 Dec 2018   G06T-007/00   201912   Pages: 11   Chinese
AD CN109064460-A    CN10865344    01 Aug 2018
PI CN10865344    01 Aug 2018
UT DIIDW:2019006375
ER

PT P
PN CN109063764-A
TI Method for judging closing operation of isolating switch based on machine vision, involves judging state of switch according to depth learning classification result, and determining opening and closing states of switch in detection process.
AU CHEN T
   GAO W
   LIANG L
   HU G
   LIU R
   LIN J
   ZHANG N
   LIN X
   YANG F
AE FUJIAN HESHENG HI TECH IND CO LTD (FUJI-Non-standard)
GA 201900653P
AB    NOVELTY - The method involves collecting large number of samples in field for performing training process by using a deep learning algorithm (S1). A training model is established. A primary location and state judgment of a switch are determined (S2) based on the deep learning model. Image characteristics of the switch are extracted (S3). The switch is accurately positioned (S4). Position information of the switch is acquired (S5). A state of the switch is judged (S6) according to angle and depth learning classification result. Opening and closing states of the switch are determined (S7) in detection process.
   USE - Method for judging closing operation of an isolating switch based on machine vision.
   ADVANTAGE - The method enables realizing high processing speed and detection precision with high actual application value.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for judging closing operation of an isolating switch based on machine vision. '(Drawing includes non-English language text)'
   Step for collecting large number of samples in field for performing training process by using deep learning algorithm (S1)
   Step for determining primary location and state judgment of switch (S2)
   Step for extracting image characteristics of switch (S3)
   Step for accurately positioning switch (S4)
   Step for acquiring position information of switch (S5)
   Step for judging state of switch according to angle and depth learning classification result (S6)
   Step for determining opening and closing states of switch in detection process (S7)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J10B2; T01-J16C2; T01-J30A; T04-D04; T04-D07D3
IP G06K-009/62; G06T-007/73; G06T-007/13
PD CN109063764-A   21 Dec 2018   G06K-009/62   201912   Pages: 11   Chinese
AD CN109063764-A    CN10834617    26 Jul 2018
PI CN10834617    26 Jul 2018
UT DIIDW:201900653P
ER

PT P
PN CN109063841-A
TI Bayesian network and deep learning algorithm based fault mechanism intelligent analyzing method, involves obtaining electronic product failure mechanism inference logic rule, and performing fault mechanism probabilistic reasoning.
AU CHEN Y
   WANG Y
   KANG R
AE UNIV BEIHANG (UNBA-C)
GA 201900651V
AB    NOVELTY - The method involves obtaining an electronic product failure mechanism inference logic rule. Electronic product elements device information, electronic component failure mechanism and trigger mechanism of fault mechanism are sorted. A database corresponding to a fault mechanism of the electronic component is established. A logical relationship representation rule is established by using a predicate logic representation. Fault mechanism analysis instance data is obtained. A set of evidence is obtained. A fault mechanism probabilistic reasoning is performed by determining evidence set as input.
   USE - Bayesian network and deep learning algorithm based fault mechanism intelligent analyzing method.
   ADVANTAGE - The method enables ensuring component design information and cognitive analysis of environment condition information, guiding reliability design of electronic product and avoiding failure mechanism of high probability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a Bayesian network and deep learning algorithm based fault mechanism intelligent analyzing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B4P; T01-J15X; T01-J16A; T01-J16B; T01-J16C2; T01-N01B3; T01-N02B2B
IP G06N-005/04; G06N-007/00; G06F-017/50
PD CN109063841-A   21 Dec 2018   G06N-005/04   201912   Pages: 29   Chinese
AD CN109063841-A    CN10982243    27 Aug 2018
PI CN10982243    27 Aug 2018
UT DIIDW:201900651V
ER

PT P
PN CN109064477-A
TI U-Net cell nucleus edge detecting method, involves reading image of to-be-detected cell nucleus for pre-processing image, and outputting cell nucleus edge result image by using imshow function in Python third-party toolkit Matplotlib.
AU CHEN Y
   LI X
   MA M
   YAO H
AE UNIV SHAANXI NORMAL (UNSN-C)
GA 201900636P
AB    NOVELTY - The method involves storing updated weights and parameters of U-Net process in a real-time depth learning weight frame. An edge of a U-Net cell nucleus is detected. An image of a to-be-detected cell nucleus is read for pre-processing the image. An enhanced U-Net is loaded to store weights and parameters in a function learning frame. An edge of the cell nucleus is detected by using the image of the cell nuclear by using the deep learning framework to obtain an image of a cell nucleus edge result image. The cell nucleus edge result image is output by using imshow function in a Python (RTM: High-level programming language) third-party toolkit Matplotlib.
   USE - U-Net cell nucleus edge detecting method.
   ADVANTAGE - The method enables cell nucleus edge detection precision in color and gray cell image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a U-Net cell nucleus edge detecting method. '(Drawing includes non-English language text)'
DC S06 (Electrophotography and Photography); T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC S06-K01A; S06-K07A4; T01-J10B1; T01-J10B2; T01-J10B3B; T01-J30A; T04-D08
IP G06T-007/13
PD CN109064477-A   21 Dec 2018   G06T-007/13   201912   Pages: 15   Chinese
AD CN109064477-A    CN10734283    06 Jul 2018
PI CN10734283    06 Jul 2018
UT DIIDW:201900636P
ER

PT P
PN CN109064389-A
TI Method for realizing deep learning to generate realistic image by using hand-drawn line drawing function, involves inputting coarse-grained reality image into sub-layer GAN structure to obtain high-resolution realistic image.
AU CHEN Z
   CAI Y
   YE D
AE UNIV FUZHOU (UFZU-C)
GA 201900638Y
AB    NOVELTY - The method involves establishing a dual-layer GAN structure, where the dual-layer GAN structure comprises two sub-layer GAN structures. Real image-analog line drawing data set generating and collecting operations are performed for training a network. The sub-layer GAN structures are trained according to real image-analog line drawing data set. The real image-analog line drawing data set is input into the first sub-layer GAN structure to obtain a coarse-grained reality image. The coarse-grained reality image is input into the second sub-layer GAN structure to obtain a high-resolution realistic image.
   USE - Method for realizing deep learning to generate a realistic image by using hand-drawn line drawing function.
   ADVANTAGE - The method enables correcting rich image detail to generate the high-resolution realistic image with reality feeling in a convenient manner, thus ensuring human-computer interaction.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a dual-layer GAN structure. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B2B; T01-J10B2; T01-J10B3A; T01-N01B3
IP G06T-003/00; G06N-003/04; G06N-003/08
PD CN109064389-A   21 Dec 2018   G06T-003/00   201912   Pages: 14   Chinese
AD CN109064389-A    CN10859788    01 Aug 2018
PI CN10859788    01 Aug 2018
UT DIIDW:201900638Y
ER

PT P
PN CN109063666-A
TI Depth separable convolution based light face recognition method, involves performing network model separating process, and outputting pre-processed image to convolution neural network model to perform light face recognition process.
AU CHENG J
   LIU J
   LIU S
   SU Y
   LI Y
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 201900656B
AB    NOVELTY - The method involves constructing three-layer concatenated convolutional neural network (S1). First face image data sets of image pyramid are inputted. The three-layer concatenated convolutional neural network is trained. AM-Softmax loss function of a separable convolutional neural network is obtained based on depth (S2). Second face image data set is inputted. A convolution neural network model separating process is performed to obtain the depth. Weighting coefficient of the convolutional neural network model is compressed (S3). Depth of the convolution neural network model is calculated. An input face image of the image pyramid is pre-processed (S4). The pre-processed image is outputted to the convolution neural network model to perform a light face recognition process.
   USE - Depth separable convolution based light face recognition method.
   ADVANTAGE - The method enables utilizing depth separable convolution to reduce calculated amount and model size and separating weighting coefficient convolution layer of the convolutional neural network model to compress the depth.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a depth separable convolution based light face recognition system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a depth separable convolution based light face recognition method. '(Drawing includes non-English language text)'
   Step for constructing three-layer concatenated convolutional neural network (S1)
   Step for obtaining AM-Softmax loss function of separable convolutional neural network (S2)
   Step for compressing weighting coefficient of convolutional neural network model (S3)
   Step for pre-processing input face image of image pyramid (S4)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B2A; T04-D03; T04-D04; T04-D07F1
IP G06K-009/00; G06K-009/62; G06N-003/04; G06N-003/08
PD CN109063666-A   21 Dec 2018   G06K-009/00   201912   Pages: 12   Chinese
AD CN109063666-A    CN10921647    14 Aug 2018
PI CN10921647    14 Aug 2018
UT DIIDW:201900656B
ER

PT P
PN CN109048492-A
TI Method for detecting wear state of tool e.g. cutter, involves predicting wear state of target tool based on sensor data sequence using pre-trained cutter attrition state prediction model.
AU DAI W
   WANG Y
AE UNIV BEIHANG (UNBA-C)
GA 201901033Y
AB    NOVELTY - The pre-trained cutter attrition state prediction model constructed based on convolutional neural network is acquired (S101). The sensor data sequence collected in the cutting process of the target tool is acquired (S102), the sensor data sequence comprises the sequence acquisition of sensor data. The wear state of the target tool is predicted (S103) based on the sensor data sequence, using the pre-trained cutter attrition state prediction model.
   USE - Method for detecting wear state of tool e.g. cutter.
   ADVANTAGE - The detection method does not need to extract complex frequency domain characteristics, thus can effectively simplify the data processing procedure and improve detection efficiency. Moreover, the detection accuracy is effectively improved, since the sensor data sequence is collected in a time series of sensor data based on internal sequence features predicted tool wear state.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) convolutional neural network-based tool wear state detecting device; and
   (2) terminal device for executing convolutional neural network-based tool wear state detecting method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the tool wear state detection method. (Drawing includes non-English language text)
   Pre-trained cutter attrition state prediction model acquiring step (S101)
   Sensor data sequence acquiring step (S102)
   Target tool wear state prediction step (S103)
DC P56 (Machine tools (B23P,Q).); T01 (Digital Computers)
MC T01-J05B4P
IP B23Q-017/09; B23Q-017/00
PD CN109048492-A   21 Dec 2018   B23Q-017/09   201912   Pages: 18   Chinese
AD CN109048492-A    CN10852180    30 Jul 2018
PI CN10852180    30 Jul 2018
UT DIIDW:201901033Y
ER

PT P
PN CN109064294-A
TI Fusion time factor and correlation text feature based medicine recommending method, involves establishing personalized recommendation model, and determining user medicine predictive score to recommend maximum score medicine to user.
AU FENG Y
   WU T
   BIAN C
   WEI R
   SHANG J
AE UNIV CHONGQING (UYCQ-C)
   UNIV GUILIN ELECTRONIC TECHNOLOGY (UYGE-C)
GA 201900641C
AB    NOVELTY - The method involves obtaining interaction information of a user and medicine (S1). A time dynamic model is established (S2) based on the information of the user and medicine. First information of the user is collected and processed (S3) to obtain second information. Text features in the second information are extracted by using a convolutional neural network. A medicine correlation matrix is established (S4). A personalized recommendation model is established and trained (S5) to establish a medicine potential factor matrix. User medicine predictive score is determined to recommend (S6) the maximum score medicine to a user, where the interaction information comprises user scoring information and corresponding timestamp information.
   USE - Fusion time factor and correlation text feature based medicine recommending method.
   ADVANTAGE - The method enables recommending medicine to the required user in real time manner with high accuracy and better performance.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a fusion time factor and correlation text feature based medicine recommending method. '(Drawing includes non-English language text)'
   Step for obtaining interaction information of user and medicine (S1)
   Step for establishing time dynamic model based on information of user and medicine (S2)
   Step for collecting and processing first information of user to obtain second information (S3)
   Step for establishing medicine correlation matrix (S4)
   Step for establishing personalized recommendation model to establish medicine potential factor matrix (S5)
   Step for determining user medicine predictive score to recommend maximum score medicine to user (S6)
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-G02G4; T01-J04B2; T01-N01A; T01-N01E1
IP G06Q-030/06; G16H-020/10
PD CN109064294-A   21 Dec 2018   G06Q-030/06   201912   Pages: 11   Chinese
AD CN109064294-A    CN10953583    21 Aug 2018
PI CN10953583    21 Aug 2018
UT DIIDW:201900641C
ER

PT P
PN CN109063565-A
TI Low-resolution human face image recognizing method, involves inputting low resolution human-face image into deep neural network for obtaining low-resolution face depth characteristics, and identifying low-resolution human face.
AU GE S
AE CAS INFORMATION ENG INST (CAIE-C)
GA 201900658Y
AB    NOVELTY - The method involves training a deep neural network by using an identity mark of a high resolution human-face image for obtaining teacher flow deep learning network. A student flow deep learning network is obtained according to the identity mark of the high resolution human-face image and an identity mark of a low resolution human-face image. The low resolution human-face image is input into the trained deep neural network to obtain low-resolution face depth characteristics. A low-resolution human face is identified according to the low-resolution face depth characteristics.
   USE - Low-resolution human face image recognizing method.
   ADVANTAGE - The method enables improving dual-depth learning network structure knowledge training efficient by a selective low-resolution human face identification network, realizing low-resolution face recognition with close accuracy of a high-resolution face recognition model.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a low-resolution human face image recognition device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a low-resolution human face image. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2A; T01-J10D; T01-J16C2; T01-N01B3; T04-D04; T04-D07F1
IP G06K-009/00; G06K-009/66; G06N-003/04
PD CN109063565-A   21 Dec 2018   G06K-009/00   201912   Pages: 14   Chinese
AD CN109063565-A    CN10698280    29 Jun 2018
PI CN10698280    29 Jun 2018
UT DIIDW:201900658Y
ER

PT P
PN CN109059954-A
TI Real-time high-accuracy map lane line fusion update supporting method, involves converting view coordinate to original coordinate, combining original coordinate and driving data to rebuild lane line, and updating fusion result on lane line.
AU GU M
   LIU S
   LIU F
   TONG R
AE GUANGDONG STARCART TECHNOLOGY CO LTD (GUAN-Non-standard)
GA 201900748N
AB    NOVELTY - The method involves collecting video image data and track point vehicle driving data. Scene segmentation process is performed in the video image data by deep learning process and an object class segmentation model. A lane line region is located in the image according to a segmentation result. Lane line projection process is performed in a visual tracking processing lane. A view coordinate of the visual tracking processing lane is converted to an original coordinate. The lane line original coordinate and the driving data are combined to rebuild a lane line. A fusion result is updated on the lane line.
   USE - Real-time high-accuracy map lane line fusion update supporting method.
   ADVANTAGE - The method enables realizing lane artificial intelligent identification process in the video image according to the video frame image scene change characteristic, tracking lane line data, improving detection accuracy of the lane line, storing map data in a grid structure, and performing parallel map data processing and updating process so as to improve collocation efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a real-time high-accuracy map lane line fusion update supporting system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a real-time high-accuracy map lane line fusion update supporting method. '(Drawing includes non-English language text)'
DC S02 (Engineering Instrumentation)
MC S02-B08G
IP G01C-021/36; G01C-021/32
PD CN109059954-A   21 Dec 2018   G01C-021/36   201912   Pages: 19   Chinese
AD CN109059954-A    CN10721451    29 Jun 2018
PI CN10721451    29 Jun 2018
UT DIIDW:201900748N
ER

PT P
PN CN109063772-A
TI Depth learning based personalized image semantic analyzing method, involves outputting semantic word vector when semantic word vector within preset semantic factor range, and obtaining image semantics analysis result.
AU HE L
   LIN S
   WENG H
AE UNIV GUANGDONG TECHNOLOGY (UGTE-C)
GA 201900653G
AB    NOVELTY - The method involves converting an original image signal into a convolutional network to generate high-level abstract semantic vector. Similarity calculation process is performed on the high-level abstract semantic vector to obtain a first-level semantic vector. The first-level semantic vector is switched into a sequence. Converted first-level semantic vector is output. The converted first-level semantic vector and feature word vector are combined to obtain second-level semantic word vector. The first-level semantic vector is compared with the second-level semantic word vector. The second-level semantic word vector is output when the second-level semantic word vector within a preset semantic factor range. Image semantics analysis result is obtained.
   USE - Depth learning based personalized image semantic analyzing method.
   ADVANTAGE - The method enables combining a convolutional neural network with mixed recommendations so as to realize natural language image analyzing process in a better manner, and presenting different expressions of different objects so as to perform personalized semantic analysis process in an effective manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a depth learning based personalized image semantic analyzing device
   (2) a computer-readable storage medium for storing a set of instructions for analyzing personalized image semantic based on depth learning.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a depth learning based personalized image semantic analyzing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-E01C; T01-E03; T01-E04; T01-J04C; T01-J05B2A; T01-J10B2; T01-J16C3; T01-N01B3; T01-S03; T04-D04
IP G06K-009/62; G06K-009/72; G06N-003/04
PD CN109063772-A   21 Dec 2018   G06K-009/62   201912   Pages: 20   Chinese
AD CN109063772-A    CN10870503    02 Aug 2018
PI CN10870503    02 Aug 2018
UT DIIDW:201900653G
ER

PT P
PN CN109064434-A
TI Method for performing image enhancement, involves performing activation function of hidden layer in deep learning network, and obtaining enhanced image corresponding to low quality image according to output of deep learning network.
AU HE Y
AE GUANGZHOU SHIYUAN ELECTRONICS TECHNOLOGY (GUAZ-C)
GA 201900637Q
AB    NOVELTY - The method involves traversing a preset training sample base. First and second color histograms of a low quality image and a corresponding enhanced image in the training sample base are obtained. A mapping relationship is established, where the mapping relationship matches the first color histogram of the low quality image in the training sample base with the second color histogram of the corresponding enhanced image. Activation function of a last hidden layer in a deep learning network is performed according to the mapping relationship. The low quality image to be enhanced is input into the deep learning network. An enhanced image is obtained corresponding to the low quality image according to an output of the deep learning network.
   USE - Method for performing image enhancement by using a computing device (claimed).
   ADVANTAGE - The method enables reducing color difference between the low-quality image and the enhanced image, and improving image enhancement effect.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an image enhancing network construction method
   (2) an image enhancing network construction device
   (3) a device for performing image enhancement by using a computing device
   (4) a computer readable storage medium storing a set of instructions for performing image enhancement by using a computing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for performing image enhancement by using a computing device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J10B1; T01-J10B3A; T01-J10B3B; T01-N01B3; T04-D08; W04-W05A
IP G06T-005/40; G06T-005/50
PD CN109064434-A   21 Dec 2018   G06T-005/40   201912   Pages: 15   Chinese
AD CN109064434-A    CN10687742    28 Jun 2018
PI CN10687742    28 Jun 2018
UT DIIDW:201900637Q
ER

PT P
PN CN109064399-A
TI Image ultra-resolution reconstruction method, involves dividing to-be reconstructing low resolution image, inputting input data to deep learning network, and reconstructing ultra resolution image according to network parameters.
AU HE Y
AE GUANGZHOU SHIYUAN ELECTRONICS TECHNOLOGY (GUAZ-C)
GA 201900638P
AB    NOVELTY - The method involves dividing to-be reconstructing low resolution image according to the scene content. Label value of image data of each divided scene region is set. The image data of each divided scene area and corresponding label value are combined to obtain input data of a deep learning network. The input data is input to the deep learning network to learn parameters. Ultra resolution image is reconstructed according to learned network parameters. The label value of each type of the scene content is set. The label value is written into the image data of the divided scene area. RGB three-channel image data of each divided scene area is obtained.
   USE - Image ultra-resolution reconstruction method.
   ADVANTAGE - The method enables avoiding problem of difficult in accurately recovering corresponding details of scene content in the different images so as to accurately recover the corresponding details of the different scene content in the image, thus improving image quality of the reconstruction.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an image ultra-resolution reconstruction system
   (2) a computer device
   (3) a computer storage medium for storing a set of instruction for executing image ultra-resolution reconstruction method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an image ultra-resolution reconstruction method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J10B1; T01-J10B2; T01-J10B3A; T01-J10D; T01-N01B3; T01-N01D1B; T01-N01D2; T04-D08; W04-W05A
IP G06T-003/40; G06T-007/11; G06N-003/04
PD CN109064399-A   21 Dec 2018   G06T-003/40   201912   Pages: 12   Chinese
AD CN109064399-A    CN10803433    20 Jul 2018
PI CN10803433    20 Jul 2018
UT DIIDW:201900638P
ER

PT P
PN CN109064493-A
TI Meta-learning based target tracking method, involves processing characteristics of final regression to obtain response picture, and obtaining target frame included in current frame according to response picture.
AU HE Z
   DONG Y
   BAI H
   XIONG F
AE BEIJING FACEALL TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 2019006369
AB    NOVELTY - The method involves obtaining a modified filter parameter of filtering algorithm by a convolutional neural network based on a tracking model trained during improvement of current frame. Feature extraction of a video search sample and a frame of search sample is performed. Characteristics of the current frame corresponding to the frame are synthesized to obtain final characteristic based on a regression layer tracing model. The characteristics of final regression are processed to obtain a response picture. A target frame included in the current frame is obtained according to the response picture.
   USE - Method for tracking a target by using an electronic device (claimed) based on meta-learning.
   ADVANTAGE - The method enables updating target parameter during target tracking process and performing quick adjustment of convolutional neural network parameter through meta-learning so as to increase updating speed and improve target tracking accuracy.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a meta-learning based target tracking device
   (2) a non-transitory computer-readable storage medium for storing a set of instructions to perform a meta-learning based target tracking method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for tracking a target by using an electronic device based on meta-learning. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-N03A2; T01-S03
IP G06T-007/246; G06N-003/04
PD CN109064493-A   21 Dec 2018   G06T-007/246   201912   Pages: 12   Chinese
AD CN109064493-A    CN10862625    01 Aug 2018
PI CN10862625    01 Aug 2018
UT DIIDW:2019006369
ER

PT P
PN CN109063916-A
TI Wind power predicting method, involves performing normalization process on current wind power data and historical wind power data, and performing deep learning process to obtain wind power prediction result.
AU HUANG S
   YIN H
   MENG A
   ZENG Y
AE UNIV GUANGDONG TECHNOLOGY (UGTE-C)
GA 2019006501
AB    NOVELTY - The method involves obtaining current wind power data and historical wind power data. Normalization process is performed on the current wind power data and the historical wind power data. Deep learning process is performed to obtain a wind power prediction result. The current wind power data is compared with the historical wind power data. Predetermined prediction duration is divided into multiple windows. Minimum blur is determined. Maximum blur particles and average fuzzy particles are obtained. Normalization process is performed on the maximum blur particles and the average fuzzy particles.
   USE - Wind power predicting method.
   ADVANTAGE - The method enables reducing electric energy supply plan error.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a wind power predicting device
   (2) a computer-readable storage medium for storing set of instructions to perform wind power predicting process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a wind power predicting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems); X15 (Non-Fossil Fuel Power Generating Systems)
MC T01-D03; T01-E01C; T01-J05A2A; T01-J30A; T01-S03; W04-W05A; X15-B
IP G06Q-010/04; G06Q-050/06; G06N-003/04; G06N-003/08
PD CN109063916-A   21 Dec 2018   G06Q-010/04   201912   Pages: 13   Chinese
AD CN109063916-A    CN10915574    13 Aug 2018
PI CN10915574    13 Aug 2018
UT DIIDW:2019006501
ER

PT P
PN CN109063642-A
TI HMM algorithm based road crossing pedestrian predicting method, involves judging to check whether pedestrian is crossing road, sending control signal to speaker, and issuing alarm by speaker for warning driver according to control signal.
AU HUANG W
   CHEN J
   HUANG J
   YU W
   WANG D
   CHEN C
   WU Y
AE UNIV GUANGZHOU (UNGZ-C)
GA 2019006571
AB    NOVELTY - The method involves receiving current data collected by a collecting device. Pre-stored historical data is divided into training samples of fixed length. A depth learning model is established in a layer by layer manner based on HMM algorithm according to the current data and the pre-stored historical data. The depth learning model is adjusted. The current data is inputted into the depth learning model. Judgment is made to check whether a pedestrian is crossing road or not by utilizing the depth learning model. A control signal is sent to a speaker. Alarm is issued by the speaker for warning a driver according to the control signal.
   USE - HMM algorithm based road crossing pedestrian predicting method.
   ADVANTAGE - The method enables detecting the pedestrian in environment of a vehicle, adding road information when establishing the depth learning model for predicting a road crossing pedestrian in an accurate manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an HMM algorithm based road crossing pedestrian predicting system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a HMM algorithm based road crossing pedestrian predicting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J21; T01-J30A; T04-D04
IP G06K-009/00; G06K-009/62
PD CN109063642-A   21 Dec 2018   G06K-009/00   201912   Pages: 9   Chinese
AD CN109063642-A    CN10861624    01 Aug 2018
PI CN10861624    01 Aug 2018
UT DIIDW:2019006571
ER

PT P
PN CN109063456-A
TI Image identifying code security detecting method, involves recognizing word label and sub-image identifying code by utilizing tag identification model and sub-graph identification model, and determining security of image verification code.
AU JI S
   ZHAO B
   WENG H
   CHEN J
AE UNIV ZHEJIANG (UYZH-C)
GA 201900661L
AB    NOVELTY - The method involves collecting original image verification code data, text label and an artificial mark from target website. A first convolutional neural network is established by utilizing a tag training set for obtaining a tag identification model. An internet picture is collected for establishing a graph training set according to type of a text label. A second convolutional neural network is established by utilizing the graph training set to obtain a sub-graph identification model. A word label and a sub-image identifying code are recognized by utilizing the tag identification model and the sub-graph identification model. Security of a target website image verification code is determined according to identification accuracy.
   USE - Image identifying code security detecting method.
   ADVANTAGE - The method enables realizing image verification code safety estimation process in an effective manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an image identifying code security detecting system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an image identifying code security detecting system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2A; T01-J10D; T01-J12C; T01-N01B3; T01-N02B1B; T04-D04; T04-D07B1; T04-E; T04-K03B
IP G06F-021/36; G06K-009/62; G06N-003/08
PD CN109063456-A   21 Dec 2018   G06F-021/36   201912   Pages: 13   Chinese
AD CN109063456-A    CN10873961    02 Aug 2018
PI CN10873961    02 Aug 2018
UT DIIDW:201900661L
ER

PT P
PN CN109064405-A
TI Dual-path-network based multi-scale image ultra-resolution realizing method, involves converting low resolution image into high resolution image, and calculating index peak value signal-to-noise ratio and structure similarity of objective.
AU KUANG H
   LIN G
   WANG H
   LIU X
   MA X
   ZHANG J
   ZHOU W
   LIN J
AE WUHAN CHANGE MEDICAL ANTI AGING ROBOT CO (WUHA-Non-standard)
GA 201900638H
AB    NOVELTY - The method involves generating a multi-scale image training set according to different size (S1). A pre-processing module, a high characteristic extracting module and an upper sampling module are established (S2). A convolution neural network model is established (S3). A low-resolution image is input to the convolutional neural network model. A reconstructed high resolution image is obtained. The convolutional neural network model is trained (S4) corresponding to a loss function. A super-resolution model optimizing process is performed (S5) by using an Adam optimizer. The low resolution image is converted (S6) into a high resolution image. An index peak value signal-to-noise ratio (PSNR) and structure similarity (SSIM) of an objective are calculated.
   USE - Dual-path-network based multi-scale image ultra-resolution realizing method.
   ADVANTAGE - The method enables reducing difference in input images at different scales, avoiding problem in disappearing a gradient of a depth neural network and improving image reconstruction performance.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a dual-path-network based multi-scale image ultra-resolution realizing method. '(Drawing includes non-English language text)'
   Step for generating a multi-scale image training set according to different size (S1)
   Step for establishing a pre-processing module, a high characteristic extracting module and an upper sampling module (S2)
   Step for establishing a convolution neural network model (S3)
   Step for training the convolution neural network model (S4)
   Step for performing a super-resolution model optimizing process (S5)
   Step for converting the low resolution image into a high resolution image (S6)
DC T01 (Digital Computers)
MC T01-J04A; T01-J04B2; T01-J10B1; T01-J10B3A; T01-J10D; T01-N01B3
IP G06T-003/40
PD CN109064405-A   21 Dec 2018   G06T-003/40   201912   Pages: 10   Chinese
AD CN109064405-A    CN10964588    23 Aug 2018
PI CN10964588    23 Aug 2018
CP CN109064405-A
      CN106991646-A   FUJIAN IMPERIAL VISION INFORMATION TECHN (FUJI-Non-standard)   TONG T, GAO Q
CR CN109064405-A
      W. SHI: "Real-time single im-age and video super-resolution using an efficient sub-pixel convolutional neural network", CVPR,relevantClaims[1-10],relevantPassages[1874-1883]
      YULUN ZHANG: "Residual Dense Network for Image Super-Resolution", THESE CVPR 2018,relevantClaims[1-10],relevantPassages[2472-2481]
      : "", ,relevantClaims[1-10],relevantPassages[984-993]
UT DIIDW:201900638H
ER

PT P
PN CN109063826-A
TI Memristor based convolutional neural network realizing method, involves re-training to-be trained convolutional neural network when training precision not satisfies standard, and obtaining required convolutional neural network.
AU LEE J
   LIU J
   TANG Y
   HU W
   GONG W
   LI W
AE UNIV CHONGQING (UYCQ-C)
GA 2019006528
AB    NOVELTY - The method involves preparing a to-be trained convolutional neural network, where convolutional neural network comprises a convolution pooling layer and a fully-connected layer. Training set image information is input to obtain a full-connection layer output value. The to-be trained convolutional neural network is trained by performing back propagation function according to deviation between the full-connection layer output value of the to-be trained convolutional neural network and standard information. Determination is made to check whether training precision satisfies standard. The to-be trained convolutional neural network is re-trained when the training precision not satisfies standard. A required convolutional neural network is obtained.
   USE - Memristor based convolutional neural network realizing method.
   ADVANTAGE - The method enables reducing hardware noise, preparation difficulty and time consumption, and improving training precision.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a memristor based convolutional neural network realizing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-N01B3
IP G06N-003/04
PD CN109063826-A   21 Dec 2018   G06N-003/04   201912   Pages: 10   Chinese
AD CN109063826-A    CN11066871    13 Sep 2018
PI CN10223942    19 Mar 2018
   CN11066871    13 Sep 2018
CP CN109063826-A
      CN105160401-A   UNIV ELECTRONIC SCI & TECHNOLOGY (UEST)   DENG J, CAI J, HU Q, LI C, LIU X, YANG C, YU Y, ZHANG R, MEN Y
      CN105224986-A   UNIV TSINGHUA (UYQI)   DENG L, PEI J, SHI L, LI G
      CN106599941-A   UNIV XIDIAN (UYXN)   LI Y, ZHOU L, JIAO L, LIU F, SHANG R, MA W, MA J, GOU S
      CN107133668-A   UNIV PEKING (UYPK)   YANG Y, ZHANG T, YIN M, LU X, HUANG R
      CN107194462-A   UNIV TSINGHUA (UYQI)   LI H, SHI L, WANG Y, WU S, ZHANG Z
      CN107239736-A   BEIJING ATHENA EYES TECHNOLOGY CO LTD (BEIJ-Non-standard)   DING J
      CN107392224-A   UNIV TIANJIN SCI & TECHNOLOGY (UYTC)   ZHANG C, YANG J, CHEN J, HUANG S, LI J
CR CN109063826-A
      : "PIM", ,relevantClaims[1-7],relevantPassages[1367-1380]
UT DIIDW:2019006528
ER

PT P
PN CN109063710-A
TI Multi-scale characteristic pyramid-based three-dimensional CNN nasopharyngeal carcinoma image dividing method, involves marking artificial edge pathological region, and pre-processing and dividing MRI image data for collecting medical image.
AU LI X
   GUO F
   SHI C
   LV J
   WU X
   HE J
   LUO C
   ZHANG X
   LIU S
   LEE C
AE UNIV CHENGDU INFORMATION TECHNOLOGY (UYUH-C)
GA 2019006556
AB    NOVELTY - The method involves collecting a nasopharyngeal carcinoma image. A nasopharyngeal part of three-dimensional (3D) MRI image data is obtained. A standardization treatment operation is performed on the MRI image data. Artificial edge pathological region is marked as label data. A three-dimensional convolutional neural network is constructed according to pyramid multi-scale characteristics. The MRI image data is pre- processed and divided for collecting a medical image. A re-sampling image is collected in tumor position. The re-sampling image is cut to predetermined size.
   USE - Multi-scale characteristic pyramid-based 3D CNN nasopharyngeal carcinoma image dividing method.
   ADVANTAGE - The method enables training a segmentation model by a network to detect cancer cases by a radiological doctor, dividing a MRI image, satisfying requirement, improving experience and image dividing precision.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-scale characteristic pyramid-based three-dimensional CNN nasopharyngeal carcinoma image dividing method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC S05-D02B1; S05-F; T01-J10B1; T01-J10B2; T01-N01E; T04-D02
IP G06K-009/34; G06N-003/04
PD CN109063710-A   21 Dec 2018   G06K-009/34   201912   Pages: 12   Chinese
AD CN109063710-A    CN10907208    09 Aug 2018
PI CN10907208    09 Aug 2018
UT DIIDW:2019006556
ER

PT P
PN CN109063823-A
TI Intelligent 3D labyrinth explore body batch A3C reinforcement learning method, involves removing LSTM network training samples corresponding to lower number from shared queue, and updating LSTM network parameters.
AU LI Y
   NIE X
   LIU Z
   ZHANG T
AE UNIV BEIJING TECHNOLOGY (UYBT-C)
GA 201900652A
AB    NOVELTY - The method involves selecting a labyrinth map. A reinforcement learning system is established, where the reinforcement learning system includes environment and an intelligent body. A state of the environment is determined to establish a game screen pixel matrix. Vector of the reinforcement learning system is input into an LSTM network. Probability distribution of the reinforcement learning system is determined. LSTM network training samples are collected by using a shared queue. The LSTM network training samples corresponding to lower number is removed from the shared queue. The LSTM network parameters are updated.
   USE - Intelligent 3D labyrinth explore body batch A3C reinforcement learning method.
   ADVANTAGE - The method enables combining high-efficiency reinforcement learning algorithm with deep learning process to independently explore 3D labyrinth so as to reduce short 3D labyrinth explore body batch A3C reinforcement learning time and memory loss.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a reinforcement learning system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J16C2; T01-N01B1; T01-N01B3; T01-N01D
IP G06N-003/04; G06N-003/08
PD CN109063823-A   21 Dec 2018   G06N-003/04   201912   Pages: 8   Chinese
AD CN109063823-A    CN10820233    24 Jul 2018
PI CN10820233    24 Jul 2018
UT DIIDW:201900652A
ER

PT P
PN CN109063547-A
TI Deep learning based cell type identifying method, involves pre-establishing neural network model, collecting cell images, and combining binary image and cell density heatmap to obtain cell-image type identification result.
AU LIN H
   HUANG K
   WANG D
   WANG R
   KANG D
AE UNIV SUN YAT-SEN ZHONGSHAN OPHTHALMIC CE (UYSY-C)
   UNIV SUN YAT-SEN (UYSY-C)
GA 201900659E
AB    NOVELTY - The method involves pre-establishing a neural network model. Cell images are collected. A cell type heatmap is obtained. Cell images preliminary processing operation is performed. Each cell image is divided into multiple partial images by adopting a sliding window mode. A local image input neural network model is established. A cell type label in partial images is obtained. The cell type label is integrated into the cell type heatmap. A binary image and a cell density heatmap are obtained. The binary image and the cell density heatmap are combined to obtain a cell-image type identification result.
   USE - Deep learning based cell type identifying method.
   ADVANTAGE - The method enables directly inputting the cell images into the neural network model, improving cell type heatmaps obtaining precision, avoiding need of manual operation and completing cell invisibility detection in rapid and convenient manners.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based cell type identifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-N01B3; T04-D04
IP G06K-009/00; G06K-009/62; G06N-003/04; G06N-003/08
PD CN109063547-A   21 Dec 2018   G06K-009/00   201912   Pages: 12   Chinese
AD CN109063547-A    CN10610019    13 Jun 2018
PI CN10610019    13 Jun 2018
UT DIIDW:201900659E
ER

PT P
PN CN109063610-A
TI Visual recognition and depth neural network-based traffic system, has executing mechanism provided with stair device, and control center connected with step device, where executing mechanism is provided vehicle and small vehicle.
AU LIU D
   LI Y
   WANG Y
   WANG Z
   LIN L
   YAN T
AE UNIV HUBEI ARTS & SCI (UYRU-C)
GA 201900657S
AB    NOVELTY - The system has a visual recognition system provided with an image collecting unit (1). The image collecting unit collects an image signal. An analysis system is provided with a mainframe (3) and a control center (4). The mainframe is provided with a deep neural network structure. The deep neural network structure receives a classification signal. The control center receives weight information of the mainframe. An executing mechanism is provided with a stair device (6). An end of the control center is connected with the step device. The executing mechanism is provided a vehicle and a small vehicle by three types of conditions.
   USE - Visual recognition and depth neural network-based traffic system.
   ADVANTAGE - The system realizes picture collection and classification by a visual identification system, ensures shortest time of the passing mode of pedestrian and vehicle, adjusts working type by the actuator according to conclusion of the analysis system, and relieves traffic pressure.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a visual recognition and depth neural network-based traffic system controlling method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a visual recognition and depth neural network-based traffic system.
   Image collecting unit (1)
   Cloud server (2)
   Mainframe (3)
   Control center (4)
   Stair device (6)
DC T01 (Digital Computers)
MC T01-J05B2; T01-J07D3A; T01-J10B2A; T01-M06B
IP G06K-009/00; G06N-003/08
PD CN109063610-A   21 Dec 2018   G06K-009/00   201912   Pages: 12   Chinese
AD CN109063610-A    CN10796554    19 Jul 2018
PI CN10796554    19 Jul 2018
UT DIIDW:201900657S
ER

PT P
PN CN109063660-A
TI Multi-spectral satellite image based crop identification method, involves utilizing sequence spectrum information of pixel and crop types of crop samples as input, and performing on sampling area by trained machine learning model.
AU LIU Y
   QIAN J
   LI L
   XIA S
   YE X
   ZHOU G
   LV T
   WANG C
AE CHENGDU TIANDI QUANTUM TECHNOLOGY CO LTD (CHEN-Non-standard)
GA 201900656H
AB    NOVELTY - The method involves collecting a crop sample (S1). Multi-spectral satellite image data of the crop sample is obtained (S2). A pixel corresponding to the crop samples are determined (S3) on the multi-spectral satellite image data by collecting position of the crop sample. Sequence spectrum information of the pixel and crop types of the crop samples are utilized (S4) as an input to train a machine learning model. Crop classification is performed (S5) on a sampling area by the trained machine learning model, where the machine-learning model is utilized with a convolutional neural network algorithm for training.
   USE - Multi-spectral satellite image based crop identification method.
   ADVANTAGE - The method enables utilizing the time spectral information of the image as the input to train the machine learning model, which greatly expands number of crop spectral information so as to avoid problem of insufficient spectral information of crops at single time, thus identifying the crop from spectral information of cycle of crop growth, and hence improving identification efficiency of the crop.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-spectral satellite image based crop identification method. '(Drawing includes non-English language text)'
   Step for collecting a crop sample (S1)
   Step for obtaining multi-spectral satellite image data of the crop sample (S2)
   Step for determining pixel corresponding to the crop samples (S3)
   Step for utilizing sequence spectrum information of the pixel and crop types of the crop samples as an input (S4)
   Step for performing crop classification (S5)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J10B2; T01-N01B3; T04-D04
IP G06K-009/00; G06K-009/62
PD CN109063660-A   21 Dec 2018   G06K-009/00   201912   Pages: 8   Chinese
AD CN109063660-A    CN10901457    09 Aug 2018
PI CN10901457    09 Aug 2018
UT DIIDW:201900656H
ER

PT P
PN CN109063732-A
TI Feature interaction and multi-task learning based image sorting method, involves aggregating image visual features, inputting aggregated image visual features into trained classifier, and sorting images according to classification result.
AU NIE L
   CHEN Z
   DU C
   SONG X
   CHENG Z
   WANG Y
AE UNIV SHANDONG (USHA-C)
GA 201900654K
AB    NOVELTY - The method involves extracting visual features of an original image by using a convolutional neural network. Region-based image visual feature interaction process is performed. Image visual features after interaction are aggregated by using a multi-task learning neural network. The aggregated image visual features are input into a trained classifier. The images are sorted according to the classification result. Dimensionality reduction process is performed on the aggregated image visual features, where the classifier classifies the aggregated image visual features by using a softmax function.
   USE - Feature interaction and multi-task learning based image sorting method.
   ADVANTAGE - The method enables increasing image sorting accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a feature interaction and multi-task learning based image sorting system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a feature interaction and multi-task learning based image sorting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-F02; T01-J05B2; T01-J10B2; T01-J10B3A; T01-N01B3
IP G06K-009/62; G06N-003/04; G06N-003/08
PD CN109063732-A   21 Dec 2018   G06K-009/62   201912   Pages: 11   Chinese
AD CN109063732-A    CN10668293    26 Jun 2018
PI CN10668293    26 Jun 2018
CP CN109063732-A
      CN103399951-A   UNIV SHANDONG (USHA)   XU X, WANG X, WANG Y
      CN103745217-A   UNIV BEIJING TECHNOLOGY (UYBT)   ZHANG P, ZHUO L, LI X, CAI Y, ZHANG J, ZHANG X
      CN104346370-A   ALIBABA GROUP HOLDING LTD (ABAB)   LIU R, ZHANG H, RU X
      CN106126581-A   UNIV FUDAN (UYFU)   ZHANG Y, HUANG F, JIN C, ZHANG T
      CN106202256-A   UNIV XIDIAN (UYXN)   MENG F, SONG M, CHAN D, SHI R
      CN106529601-A   UNIV NORTHEASTERN (UNEN)   CAO P, LIU X, CHAN X, LIU S, LI W, QIN W, FENG C, YANG J
      CN106855883-A   SHANGHAI ADVANCED RES INST CHINESE ACAD (CAAR)   CHEN X
      CN107391594-A   ANHUI RJI INTELLIGENT TECHNOLOGY CO LTD (ANHU-Non-standard)   HU J, MA F, SHAO S, TANG C, TIAN X, WANG D, XIA T
UT DIIDW:201900654K
ER

PT P
PN CN109064394-A
TI Convolutional neural network based ultra-resolution image reconstruction method, involves inputting first resolution image into feature enhancing layer, and inputting second resolution image into network to obtain third resolution image.
AU SHAO X
   LIU F
   WANG J
   ZHOU J
   XU Z
   CHEN H
   ZHAO X
AE UNIV XIDIAN (UYXN-C)
GA 201900638U
AB    NOVELTY - The method involves obtaining a first resolution image. Up-sampling process is performed on the first resolution image to obtain a second resolution image. The second resolution image is input into a feature enhancing layer to obtain a third resolution image, where the enhancing layer comprises a convolution kernel. The third resolution image is input into a convolutional neural network to obtain a fourth resolution image. Transposition treatment is performed on the first resolution image. Down-sampling process is performed on the first resolution image by adopting bicubic interpolation algorithm to obtain a down-sampled image.
   USE - Convolutional neural network based ultra-resolution image reconstruction method.
   ADVANTAGE - The method enables performing up-sampling operation on an image by adopting nearest interpolation algorithm, and inputting the image to the convolutional neural network to form a single- input single-output enhanced layer for processing the image so as to avoid loss of image information. The method enables obtaining convolutional neural network processing characteristic input to obtain better reconstruction results.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based ultra-resolution image reconstruction method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J10B1; T01-J10B3A; T01-J10D
IP G06T-003/40; G06N-003/04
PD CN109064394-A   21 Dec 2018   G06T-003/40   201912   Pages: 16   Chinese
AD CN109064394-A    CN10596676    11 Jun 2018
PI CN10596676    11 Jun 2018
UT DIIDW:201900638U
ER

PT P
PN CN109064748-A
TI Time clustering analysis and variable convolution neural network based city traffic road average speed predicting method, involves updating average reward value of variable convolutional neural network when current sub-time period is ended.
AU SHEN G
   CHEN C
   YANG X
   LIU Z
   ZHU L
   LIU D
   RUAN Z
   SHEN S
   ZHU D
AE UNIV ZHEJIANG TECHNOLOGY (UYZT-C)
GA 201900630A
AB    NOVELTY - The method involves obtaining an optimum segmentation point by adopting differential evolution algorithm based on continuous differential evolution. An average correlation coefficient between average traffic speed sequences of divided sub-time periods is calculated. A final predictor is selected from a series of a basic variable convolutional neural network according to average return. A prediction value obtained by a highest variable convolutional neural network in a current time sub-segment is reported as a prediction value of preset time. An average reward value of a basic variable convolutional neural network is updated when current sub-time period is ended.
   USE - Time clustering analysis and variable convolution neural network based city traffic road average speed predicting method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a time clustering analysis and variable convolution neural network based city traffic road average speed predicting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J05B3; T01-J07B; T01-J16C4; T01-J21C; T01-N03A2
IP G08G-001/01; G08G-001/052; G06N-003/02
PD CN109064748-A   21 Dec 2018   G08G-001/01   201912   Pages: 9   Chinese
AD CN109064748-A    CN11089218    18 Sep 2018
PI CN11089218    18 Sep 2018
UT DIIDW:201900630A
ER

PT P
PN CN109063687-A
TI Deep convolutional neural network based micro-seismic P-wave identification method, involves converting one-hot code by using Softmax function corresponding to tag type to identify signal, and dividing signal into P-wave signal.
AU SHENG G
   XIE K
   TANG X
   ZHENG Z
   WEN F
   TANG J
   YI F
AE UNIV YANGTZE (UYYA-Non-standard)
GA 201900655T
AB    NOVELTY - The method involves obtaining a data set of a convolutional neural network by using different frequency and different signal-to-noise ratio of a micro seismic forward signal and an actual micro-seismic record (S1), where the convolutional neural network includes an input layer. The data set is divided into an effective P-wave and a noise. The data set is trained (S2) by using the convolution neural network. A signal response is enhanced (S3) by using Relu activation function. An one-hot code is converted (S4) by using Softmax function corresponding to a tag type to identify a signal. The signal is divided into a P-wave signal and a noise signal.
   USE - Deep convolutional neural network based micro-seismic P-wave identification method.
   ADVANTAGE - The method enables improving micro-seismic P-wave identification efficiency and precision.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep convolutional neural network based micro-seismic P-wave identification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep convolutional neural network based micro-seismic p-wave identification method. '(Drawing includes non-English language text)'
   Step for obtaining data set of convolutional neural network (S1)
   Step for training data set by using convolution neural network (S2)
   Step for enhancing signal response (S3)
   Step for converting one-hot code by using Softmax function (S4)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B2; T04-D03A; T04-D04; T04-K03B
IP G06K-009/00; G06K-009/62; G06N-003/04; G06N-003/08
PD CN109063687-A   21 Dec 2018   G06K-009/00   201912   Pages: 9   Chinese
AD CN109063687-A    CN10997081    29 Aug 2018
PI CN10997081    29 Aug 2018
UT DIIDW:201900655T
ER

PT P
PN CN109065110-A
TI Method for automatically generating medical image diagnosis report based on deep learning mode, involves matching text corresponding to subject according to theme vector of each picture, and obtaining diagnostic report of image.
AU SU T
   YU L
   HUO D
AE HARBIN INST TECHNOLOGY (HAIT-C)
   HAINAN CANCER HOSPITAL (HAIN-Non-standard)
GA 2019022463
AB    NOVELTY - The method involves using theme vector as label of each medical image. The single-channel computed tomography image and positron-emission tomography image with the same size are combined to obtain a dual-channel image. The square image is taken from the dual channel image. The largest side length of all squares is taken as the standard size. The image of the unified size is utilized as the training data, and the theme vector is utilized as the label. The VGGNet-19 is utilized as the network model to train, and the theme vector generation model is obtained. The text generation model is constructed, and each text is utilized as a corpus of the text generation model to separately train the text generation model. The text representing each topic is respectively generated according to the training model. The text corresponding to the subject is matched according to the theme vector of each picture, and a diagnostic report of the image is obtained.
   USE - Method for automatically generating medical image diagnosis report based on deep learning mode.
   ADVANTAGE - The doctors does not need too much manual summary of training data labels, and only the location and size of the lesions can be labeled in order to improve the accuracy while effectively reducing the work of doctors.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating the sample of data for automatically generating medical image report. (Drawing includes non-English language text)
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D08A; S05-G02G; T01-J11A1; T01-N01B3; T01-N01E1
IP G16H-015/00; G16H-030/40; G06F-017/27
PD CN109065110-A   21 Dec 2018   G16H-015/00   201912   Pages: 21   Chinese
AD CN109065110-A    CN10758999    11 Jul 2018
PI CN10758999    11 Jul 2018
UT DIIDW:2019022463
ER

PT P
PN CN109063854-A
TI Method for controlling intelligent maintenance cloud platform system, involves performing state information testing process, determining whether state information is fault information, and judging whether fault occurs in front end equipment.
AU SUN X
AE HENAN ZHONGYU GUANGHENG TECHNOLOGY DEV (HENA-Non-standard)
GA 201900651H
AB    NOVELTY - The method involves acquiring operation state information of a front end equipment. A state information set of the front end equipment is established corresponding to identification information of the front end equipment. The state information set is transferred corresponding to testing environment of the front end equipment. A state information set testing process is performed. Determination is made to check whether the operation state information is fault information. Judgment is made to check whether fault occurs in the front end equipment. A convolutional neural network is established corresponding to a failure information set of the front end equipment.
   USE - Method for controlling an intelligent maintenance cloud platform system (claimed).
   ADVANTAGE - The method enables processing the operation state information of the front end equipment so as to improve front end device maintenance efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an intelligent maintenance cloud platform system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an intelligent maintenance cloud platform system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-N01A2; T01-N01D3A
IP G06Q-010/00; G06N-003/04
PD CN109063854-A   21 Dec 2018   G06Q-010/00   201912   Pages: 13   Chinese
AD CN109063854-A    CN10969256    23 Aug 2018
PI CN10969256    23 Aug 2018
UT DIIDW:201900651H
ER

PT P
PN CN109063708-A
TI Outline extraction based industrial image feature identification method, involves receiving character identification information of detection areas, and receiving industrial image feature detecting result.
AU WANG J
   XU X
AE GOERTEK INC (QDGR-C)
GA 2019006558
AB    NOVELTY - The method involves extracting feature outline information of an industrial raw image. A detection area is divided into detection areas. The industrial raw image is sent to the detection areas according to the characteristic profile information. A position of each detecting area is determined. Character identification information of each detection area is received. Industrial image feature detecting result is received. A deep learning model is established. Character outline information and industrial original image information of the deep learning model are obtained. The character outline information is sent to a detection area. The detection area is processed with cutting and zooming process.
   USE - Outline extraction based industrial image feature identification method.
   ADVANTAGE - The method enables extracting the industrial image feature in an automatic manner so as to reduce labor intensity.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an outline extraction based industrial image feature identification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an outline extraction based industrial image feature identification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J10B3A; T01-J30A; T04-D02; T04-D03; T04-D04; T04-D07D5
IP G06K-009/34; G06K-009/46; G06K-009/62
PD CN109063708-A   21 Dec 2018   G06K-009/34   201912   Pages: 8   Chinese
AD CN109063708-A    CN10645620    21 Jun 2018
PI CN10645620    21 Jun 2018
UT DIIDW:2019006558
ER

PT P
PN CN109063748-A
TI Data enhancement based target detection method, involves transforming label frame into rectangular frame box, performing perspective transformation black filling process by target original image, and performing target detection process.
AU WANG K
   FANG B
   QIAN J
   YANG S
   ZHOU X
AE UNIV CHONGQING (UYCQ-C)
GA 2019006546
AB    NOVELTY - The method involves obtaining a target original image. A mark frame is added on the target original image. Original image perspective conversion process is performed to generate a perspective converted image. Mark alignment process is performed. A label frame is transformed into a rectangular frame box. Perspective transformation black filling process is performed by the target original image Target detection process is performed. Four random sampling points in the original image are extracted. Pixel points in the original image are extracted by a transformation matrix to obtain a target image.
   USE - Data enhancement based target detection method.
   ADVANTAGE - The method enables reducing effective sample scarce difficulty and automatically generating a marked image with high quality so as to improve effect of a deep neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a screenshot of a target original image.
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B1; T01-J10B2; T01-J10B3A; T01-J10D; T04-D04; T04-D07B1
IP G06K-009/62; G06N-003/08
PD CN109063748-A   21 Dec 2018   G06K-009/62   201912   Pages: 11   Chinese
AD CN109063748-A    CN10777961    16 Jul 2018
PI CN10777961    16 Jul 2018
UT DIIDW:2019006546
ER

PT P
PN CN109063903-A
TI Deep learning reinforcement based building energy consumption prediction method, involves inputting multiple predictive samples into trained deep reinforcement learning network model to predict building energy consumption.
AU WANG M
   ZHANG R
   ZHANG Y
   DONG H
   WANG Y
AE UNIV SHANDONG JIANZHU (SDSJ-C)
GA 201900650B
AB    NOVELTY - The method involves dividing collected data into a training sample set and a prediction sample set. The sample set data is preprocessed. Training sample data is input into a deep reinforcement learning model for training. An input vector of a training sample is determined by the deep reinforcement learning model. A state action value is output to obtain iterative loss function and gradient when the input vector is computed by the convolutional neural network and a fully connected neural network. A weight value is updated by using gradient descent process. Multiple predictive samples are input into a trained deep reinforcement learning network model to predict building energy consumption.
   USE - Deep learning reinforcement based building energy consumption prediction method.
   ADVANTAGE - The method enables reducing data amount and data storage requirement, thus improving data using efficiency and increasing data processing efficiency.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a deep learning reinforcement based building energy consumption prediction system
   (2) a computer readable storage medium for storing a set of instructions for predicting building energy consumption based on deep learning reinforcement.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning reinforcement based building energy consumption prediction method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04C; T01-N01A2; T01-N01B3
IP G06Q-010/04; G06Q-050/08; G06N-003/04
PD CN109063903-A   21 Dec 2018   G06Q-010/04   201912   Pages: 11   Chinese
AD CN109063903-A    CN10796307    19 Jul 2018
PI CN10796307    19 Jul 2018
UT DIIDW:201900650B
ER

PT P
PN CN109063824-A
TI Method for establishing deep three-dimensional convolutional neural network by processor, involves training first target deep convolutional network model according to supervisory signal, and obtaining second deep convolutional network model.
AU WANG Z
   ZHOU W
AE SHENZHEN EMWIT TECHNOLOGY CO LTD (SHEN-Non-standard)
GA 2019006529
AB    NOVELTY - The method involves obtaining a first target deep three-dimensional (3D) convolutional network model, where the 3D convolution network model comprises a bottleneck layer and a 3D convolution layer. Judgment is made to check whether a preset video sequence reaches a convergence state of a preset shallow 3D convolution neural network model for obtaining a supervisory signal. The first target deep 3D convolutional network model is trained according to the supervisory signal. A second target deep 3D convolutional network model is obtained, where a convergence state of the second target deep 3D convolutional network model reaches a convergence state of the first target deep 3D convolutional network model.
   USE - Method for establishing a deep 3D convolutional neural network by a processor (claimed).
   ADVANTAGE - The method enables enhancing network performance of a 3D convolution neural network.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for establishing a deep 3D convolutional neural network by a processor
   (2) a storage medium for set of instructions for establishing a deep 3D convolutional neural network by a processor.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for establishing a deep 3D convolutional neural network by a processor. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-N01B3; T01-S03
IP G06N-003/04
PD CN109063824-A   21 Dec 2018   G06N-003/04   201912   Pages: 16   Chinese
AD CN109063824-A    CN10824695    25 Jul 2018
PI CN10824695    25 Jul 2018
UT DIIDW:2019006529
ER

PT P
PN CN109064446-A
TI Method for detecting display screen quality of electronic device, involves receiving quality detecting request, and determining quality of display screen corresponding to display screen image according to defect classification result.
AU WEN Y
   LENG J
   LIU M
   XU Y
   GUO J
   LI X
AE BEIJING BAIDU NETCOM SCI & TECHNOLOGY CO (BIDU-C)
GA 201900637G
AB    NOVELTY - The method involves receiving a quality detecting request sent by a control platform arranged on a display screen production line, where the quality detecting request comprises a display screen image. The display screen image is collected by an image collection device. The display screen image is input into a defect classification model to obtain a defect classification result. A historical defect display image is obtained by training the defect classification model and using a deep convolutional neural network structure and image classification algorithm. Quality of a display screen is determined corresponding to the display screen image according to the defect classification result.
   USE - Method for detecting display screen quality of an electronic device (claimed).
   ADVANTAGE - The method enables improving system performance and ensuring high service expansion ability and high defect detection accuracy.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for detecting display screen quality of an electronic device
   (2) a storage medium for storing a set of instructions to perform display screen quality detecting process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for detecting display screen quality of electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B2; T01-J10B1; T01-J10B2; T01-N01B3
IP G06K-009/62; G06T-007/00
PD CN109064446-A   21 Dec 2018   G06T-007/00   201912   Pages: 17   Chinese
AD CN109064446-A    CN10709847    02 Jul 2018
PI CN10709847    02 Jul 2018
UT DIIDW:201900637G
ER

PT P
PN CN109063653-A
TI Method for processing image of electronic device, involves inputting key point detection model to determine finger tip and key point information of image, where key point information includes finger tip position information.
AU WU X
AE BEIJING ZIJIETIAODONG NETWORK TECHNOLOGY (BEIJ-Non-standard)
GA 201900656P
AB    NOVELTY - The method involves acquiring an image of a gesture frame, where the gesture frame includes gesture. The image is indicated from an image position. A local image is extracted from the gesture by a partial image of the gesture frame. A key point detection model i.e. convolutional neural network (CNN), is inputted to determine finger tip and key point information of the image, where the key point information includes finger tip position information. A gesture detection process is performed to the image to determine the image position. Training sample set is obtained.
   USE - Method for processing an image of an electronic device (claimed).
   ADVANTAGE - The method enables detecting the finger tip of a key point so as to avoid low gesture application frequency in the key point detection model and improve detection efficiency in an effective manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an image processing device
   (2) a computer readable storage medium is stored with a computer program by a processor to implement a method for processing an image of an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for processing an image of an electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2A; T01-J10B2; T01-N01B3; T01-S03; T04-D03; T04-D04; T04-D07D5; T04-D07F2; T04-J01
IP G06K-009/00; G06K-009/46; G06K-009/62
PD CN109063653-A   21 Dec 2018   G06K-009/00   201912   Pages: 12   Chinese
AD CN109063653-A    CN10888877    07 Aug 2018
PI CN10888877    07 Aug 2018
UT DIIDW:201900656P
ER

PT P
PN CN109063746-A
TI Non-visual similarity learning based depth similarity learning method, involves generating compact cluster and batch, training convolutional neural network, obtaining local time and performing multiple instance learning.
AU XIA C
AE SHENZHEN VISION TECHNOLOGY CO LTD (SHEN-Non-standard)
GA 2019006548
AB    NOVELTY - The method involves generating a compact cluster and a batch. A convolutional neural network (CNN) is trained. A local time is obtained. Multiple instance learning is performed. An associated initial sample set is obtained from a sample. The compact cluster is obtained. A cluster with similar similarities is selected to form a random gradient descent (SGD) batch. A clustering packet is terminated. The compact cluster is assigned a unique proxy label. An optimization problem is designed to optimize the compact cluster and the batch. A minimum similarity in each batch of the compact cluster is searched.
   USE - Non-visual similarity learning based depth similarity learning method.
   ADVANTAGE - The method enables reducing amount of manual labeling data and computational cost, providing a finer similar structure and realizing better performance in attitude analysis tasks and classification problems.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a non-visual similarity learning based depth similarity learning method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J05B3; T01-J10B2; T01-N01A; T01-N01B3; T01-N03A2; T04-D04
IP G06K-009/62
PD CN109063746-A   21 Dec 2018   G06K-009/62   201912   Pages: 9   Chinese
AD CN109063746-A    CN10773103    14 Jul 2018
PI CN10773103    14 Jul 2018
UT DIIDW:2019006548
ER

PT P
PN CN109063719-A
TI Combined structure similarity and class information based image classifying method, involves obtaining category probability of image, and obtaining and outputting image classification result according to category probability of image.
AU XIONG W
   LIU H
   WANG J
   ZENG C
   ZHANG F
   FENG C
   WANG X
AE UNIV HUBEI TECHNOLOGY (UYHI-C)
GA 201900654W
AB    NOVELTY - The method involves obtaining and pre-processing original image. The processed image is input into a first convolutional layer in a convolutional neural network. An output result of the first convolutional layer is input into a second convolution layer and a pooling layer. An output result of the second convolution layer is input into a third convolutional layer. A soft-max loss training network is measured to obtain total loss function. Category probability of the image is obtained. Image classification result is obtained and output according to category probability of the image.
   USE - Combined structure similarity and class information based image classifying method.
   ADVANTAGE - The method enables introducing joint metric loss and image category information to establish loss function, calculating metric loss by calculating similarity between high-level semantic vectors of the image, minimizing intra-class distance, accelerating convergence speed of training of a network, and improving classification accuracy of test set images and image recognition performance and training efficiency of network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a combined structure similarity and class information based image classifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J04C; T01-J05B2; T01-J10B2A; T01-J16C3; T01-N01B3A; T04-D04
IP G06K-009/62; G06N-003/04
PD CN109063719-A   21 Dec 2018   G06K-009/62   201912   Pages: 10   Chinese
AD CN109063719-A    CN10365992    23 Apr 2018
PI CN10365992    23 Apr 2018
UT DIIDW:201900654W
ER

PT P
PN CN109063557-A
TI Heart coronary capillary tube identification data set rapid generating method, involves utilizing pseudo-marking picture to identify neural network data set of cardiac capillary tube by establishing neural network based on original image.
AU XU B
   ZHAI M
   WANG X
   CHEN D
   YE D
AE BEIJING HONGYUN ZHISHENG TECHNOLOGY CO (BEIJ-Non-standard)
GA 2019006595
AB    NOVELTY - The method involves performing binarization processing on a single channel image, where the processed single channel image is stored as a binary image. The binary image and corresponding original image are utilized as training data. An initial convolutional neural network is established for separating a vessel and a guide wire from a background network. The background network is selected as a first network. Entire original images are input to the first network to obtain a binary result image. A pseudo-marking image is generated based on a binarization result image and a corresponding coarse image. The pseudo-marking picture is utilized to identify a convolutional neural network data set of a cardiac capillary tube by establishing the corresponding neural network based on the original image.
   USE - Heart coronary capillary tube identification data set rapid generating method.
   ADVANTAGE - The method enables reducing manual marking cost, thus ensuring quality of data collection and improving neural network establishing speed.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a heart coronary capillary tube identification data set rapid generating method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-J10E; T01-N01B3; T04-D03; T04-D04
IP G06K-009/00; G06K-009/46; G06K-009/62; G06N-003/04
PD CN109063557-A   21 Dec 2018   G06K-009/00   201912   Pages: 16   Chinese
AD CN109063557-A    CN10675750    27 Jun 2018
PI CN10675750    27 Jun 2018
UT DIIDW:2019006595
ER

PT P
PN CN109063561-A
TI Method for identifying and calculating formula by processor, involves obtaining target information, identifying to-be-calculated formula, and obtaining and outputting calculating result, where result comprises calculating process.
AU XU H
AE GUANGZHOU SHIYUAN ELECTRONICS TECHNOLOGY (GUAZ-C)
GA 2019006591
AB    NOVELTY - The method involves obtaining target information, where target information includes picture information, handwriting command information and keyboard instruction information. To-be-calculated formula is identified. A calculating result is obtained and output, where the calculating result comprises calculating process. Determination is made to check whether target information identifying is successfully completed. Prediction formula is determined. Determination is made to check whether click-to-click command of character is received. Predicted characters are displayed.
   USE - Method for identifying and calculating formula by a processor (claimed).
   ADVANTAGE - The method enables calculating an output result to reduce user depth learning problem.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for identifying and calculating formula by a processor
   (2) a storage medium for storing program instructions to identify and calculate formula by a processor.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for identifying and calculating formula by a processor. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-C02A; T01-J04A; T01-J10B2; T01-S03
IP G06K-009/00; G06F-003/0484
PD CN109063561-A   21 Dec 2018   G06K-009/00   201912   Pages: 12   Chinese
AD CN109063561-A    CN10691934    28 Jun 2018
PI CN10691934    28 Jun 2018
UT DIIDW:2019006591
ER

PT P
PN CN109063559-A
TI Enhanced area regression based pedestrian detection method, involves obtaining fusing result of candidate network and deep convolutional neural network, and predicting deep convolutional neural network to background of confidence score.
AU YAO L
   ZHOU W
   WU H
AE UNIV SOUTHEAST (UYSE-C)
GA 2019006593
AB    NOVELTY - The method involves marking a rectangular frame in a bounding box. An area generation network is constructed, where the area generating network comprises convolutional feature extraction, semantic segmentation, classification layer and area regression layer. A training sample is input into the area generating network. A deep convolutional neural network is constructed. A prediction confidence degree of pedestrian is selected. An image is obtained for detecting pedestrians. A fusing result of region candidate network and deep convolutional neural network is obtained. A deep convolutional neural network is predicted to a background of confidence score.
   USE - Enhanced area regression based pedestrian detection method.
   ADVANTAGE - The method enables providing end-to-end pedestrian detection scheme by combining multiple network outputs.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an enhanced area regression based pedestrian detection system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J10B2; T01-J16C3; T01-N01B3; T04-D03
IP G06K-009/00; G06N-003/04
PD CN109063559-A   21 Dec 2018   G06K-009/00   201912   Pages: 13   Chinese
AD CN109063559-A    CN10685848    28 Jun 2018
PI CN10685848    28 Jun 2018
UT DIIDW:2019006593
ER

PT P
PN CN109063828-A
TI Method for interconnecting materials and intelligent air chip based on quantum theory, involves obtaining optical quantum information, where light quantum chip includes information input and output pins.
AU YI X
AE YOUFENER WISDOM TECHNOLOGY CO LTD (YOUF-Non-standard)
GA 2019006526
AB    NOVELTY - The method involves connecting an operation layer with a cloud coefficient library through an unsupervised training network and a convolutional neural network. Light quantity of a sub-carrier is calculated by an optical frequency comb generator. Operational data is stored. The operational data is sent the optical frequency comb generator. Quantum modulation process is performed on orthogonal sub-carriers by an interferometer. Modulation process is performed on multiple optical signals by an optical coupler. Optical quantum information is obtained, where a light quantum chip includes information input and output pins.
   USE - Method for interconnecting materials and intelligent air chip based on quantum theory.
DC T01 (Digital Computers)
MC T01-J16C1; T01-M06Q; T01-N01B3
IP G06N-003/06; G06N-003/063; G06N-003/08
PD CN109063828-A   21 Dec 2018   G06N-003/06   201912   Pages: 5   Chinese
AD CN109063828-A    CN11244782    24 Oct 2018
PI CN11244782    24 Oct 2018
UT DIIDW:2019006526
ER

PT P
PN CN109063713-A
TI Structural characteristic image depth learning-based wood identification method, involves optimizing model parameters for generating image recognition deep learning algorithm model, and outputting recognition result and confidence level.
AU YIN Y
   HE T
   JIAO L
   ZHANG M
   HAN L
   LU Y
   ZHANG Y
   LI R
AE RES INST WOOD IND CHINESE ACAD FORESTRY (CLYK-C)
GA 2019006553
AB    NOVELTY - The method involves constructing a wood image identifying multi-layer convolutional neural network. Deep learning process is performed to the wood image identifying multi-layer convolution neural network by utilizing a training set. A depth learning model is tested by utilizing a testing set. Model parameters are optimized for generating a wooden structure image recognition deep learning algorithm model according to a testing result, where the wooden structure image recognition deep learning algorithm model is utilized for identifying wood structure image data. A recognition result and confidence level are output.
   USE - Structural characteristic image depth learning-based wood identification method.
   ADVANTAGE - The method enables accurately and quickly identifying wood tree species.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a structural characteristic image depth learning-based wood identification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a structural characteristic image depth learning-based wood identification method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J05B2B; T01-J10B2A; T01-J10C5; T01-J16C2; T01-N01B3A; T04-D03; T04-D04
IP G06K-009/46; G06N-003/04; G06N-003/08; G06K-009/62
PD CN109063713-A   21 Dec 2018   G06K-009/46   201912   Pages: 10   Chinese
AD CN109063713-A    CN10800080    20 Jul 2018
PI CN10800080    20 Jul 2018
CP CN109063713-A
      CN106462549-A   ENTRUPY INC (ENTR-Non-standard)   SHARMA A, SRINIVASAN V, SUBRAMANIAN L
      CN107392896-A   UNIV FOSHAN NANHAI GUANGDONG TECHNOLOGY (UYFO-Non-standard);  FOSHAN GUANGGONGDA CNC EQUIP TECHNOLOGY (FOSH-Non-standard)   HUANG K
      WO2005078652-A1   GRUNDITZ C H (GRUN-Individual);  SPAANENBURG L (SPAA-Individual)   GRUNDITZ C H, SPAANENBURG L, WALDNER M
UT DIIDW:2019006553
ER

PT P
PN CN109074489-A
TI Method for identifying fingerprint by electronic device, involves processing fingerprint image to obtain image gradient information, and judging whether object is selected as real finger according to gradient information of image.
AU YU X
   TAN B
AE SHENZHEN GOODIX TECHNOLOGY CO LTD (SZHD-C)
GA 201902122K
AB    NOVELTY - The method involves obtaining a fingerprint image of a target pressing fingerprint sensor. The fingerprint image is processed to obtain fingerprint image gradient information. Judgment is made to check whether an object is selected as a real finger according to the gradient information of the fingerprint image. The fingerprint image is processed by a convolutional neural network according to the gradient information of the fingerprint image, where the convolutional neural network comprises two convolutional layers and two cell layers. Multiple true finger fingerprint images are input to the convolutional neural network.
   USE - Method for identifying a fingerprint by an electronic device (claimed).
   ADVANTAGE - The method enables identifying the fingerprint collected by the fingerprint sensor from a real finger according to gradient characteristics of the fingerprint image to save identifying cost and improve security of fingerprint identification.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for identifying a fingerprint by an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for identifying a fingerprint by an electronic device. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC S05-D; T01-J10B2; T01-N02B1B; T04-D04; T04-D07F2
IP G06K-009/00; G06K-009/62
PD CN109074489-A   21 Dec 2018   G06K-009/00   201912   Pages: 15   Chinese
AD CN109074489-A    CN80001044    20 Jul 2018
FD  CN109074489-A PCT application Application WOCN096437
PI CN80001044    20 Jul 2018
   CN80001044    21 Aug 2018
UT DIIDW:201902122K
ER

PT P
PN CN109063609-A
TI User abnormal behavior detecting method, involves judging check whether abnormal position is not determined as isolated points, and judging whether abnormal behavior exists target frame to obtain final detecting result.
AU ZHANG P
   XIA Q
   LIU J
   YANG X
   FEI C
   ZHANG L
   GUO Z
   SHEN F
   LI J
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 201900657T
AB    NOVELTY - The method involves obtaining optical flow characteristics of original data. The optical flow characteristics are normalized and decremented. Judgment is made to check whether input of the original data includes an original image and optical flow characteristic of the original image by using a convolutional neural network. The optical flow characteristics of the original image are added into a convolution layer. Judgment is made to check whether an abnormal position is not determined as isolated points to remove noise points. Judgment is made to check whether abnormal behavior exists in a target frame to obtain the final detecting result.
   USE - Optical flow and full-convolution semantic dividing characteristic combination based abnormal behavior detecting method.
   ADVANTAGE - The method enables detecting user abnormal behavior under complex scene by combining optical flow and semantic characteristics so as to effectively improve accuracy and robustness of user abnormal behavior detection process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a FCN network. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J10B2; T01-J16C3
IP G06K-009/00; G06K-009/62; G06N-003/04
PD CN109063609-A   21 Dec 2018   G06K-009/00   201912   Pages: 9   Chinese
AD CN109063609-A    CN10791861    18 Jul 2018
PI CN10791861    18 Jul 2018
UT DIIDW:201900657T
ER

PT P
PN CN109064502-A
TI Deep learning and artificial design feature based multi-source image registering method, involves transforming image into reference image coordinate system by using transformation matrix to complete image registration process.
AU ZHANG X
   ZHANG Y
   QI Y
   TIAN M
   JIN J
   CHEN Y
   LI F
AE UNIV NORTHWESTERN POLYTECHNICAL (UNWP-C)
GA 2019006361
AB    NOVELTY - The method involves establishing multi-source image block similarity measure depth convolution network model, where the multi-source image block similarity measure depth convolution network model includes convolution layer, a pool layer and a full-connection layer. Training set data of a multi-source image matching data set is input to the multi-source image block similarity measure depth convolution network. An output loss value of test data is determined. Parameters of the full-connection layer are optimized. An image to be registered is transformed into a reference image coordinate system by using a transformation matrix to complete image registration process.
   USE - Deep learning and artificial design feature based multi-source image registering method.
   ADVANTAGE - The method enables realizing multi-source image registering process based on the artificial design feature and deep learning so as to improve image registering precision.
DC T01 (Digital Computers)
MC T01-J04B2; T01-J10B1; T01-J10B2; T01-N01B3A
IP G06T-007/33; G06N-003/04
PD CN109064502-A   21 Dec 2018   G06T-007/33   201912   Pages: 13   Chinese
AD CN109064502-A    CN10754359    11 Jul 2018
PI CN10754359    11 Jul 2018
UT DIIDW:2019006361
ER

PT P
PN CN109048918-A
TI Wheelchair robot mechanical arm visual guiding method, involves performing unified coordinate conversion of position and attitude information for mechanical arm motion planning of wheelchair robot for grasping and delivering.
AU ZHANG Z
   MAO S
AE UNIV SOUTH CHINA TECHNOLOGY (UYSC-C)
   FOSHAN SHUNDE ZHIKE INTELLIGENT TECHNOLOGY CO LTD (FOSH-Non-standard)
GA 201902310B
AB    NOVELTY - The method involves dividing a background plane through region growing process on a cloud image. A target object is identified by a user by using a convolutional neural network. Removing-noise process is performed on obtained target object point cloud data by using density-based clustering process. A posture of the target object is determined by using a main component analysis process. Positive and negative orientations of the target object are judged by using a statistical process. A nozzle part is positioned in a center part of the target object. Unified coordinate conversion of position and attitude information is performed for mechanical arm motion planning of a wheelchair robot for grasping and delivering.
   USE - Wheelchair robot mechanical arm visual guiding method.
   ADVANTAGE - The method enables improving water capture task for proving complete and effective solving process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a wheelchair robot mechanical arm visual guiding method. '(Drawing includes non-English language text)'
DC P62 (Hand tools, cutting (B25, B26).); T01 (Digital Computers)
MC T01-J03; T01-J05B3; T01-N03A2
IP B25J-009/16
PD CN109048918-A   21 Dec 2018   B25J-009/16   201912   Pages: 13   Chinese
AD CN109048918-A    CN11112165    25 Sep 2018
PI CN11112165    25 Sep 2018
UT DIIDW:201902310B
ER

PT P
PN CN109064382-A
TI Method for processing image information in server, involves receiving user request by server through graphics processors, and constructing deep learning network model in server by processing user request.
AU ZHAO H
   ZHANG M
AE BEIJING MOSHANGHUA TECHNOLOGY CO LTD (BEIJ-Non-standard)
GA 2019006394
AB    NOVELTY - The method involves deploying graphics processors in a server. Load-balancing operation is performed by the server when the graphics processors used in a depth study reasoning frame through a pre-set application program programming interface. User request is received by the server through the graphics processors. A deep learning network model in the server is constructed by processing the user request, where a preset application programming interface comprises PyCUDA and the depth inference learning frame comprises TensorRT. Judgment is made to check whether the graphics processors is available or not.
   USE - Method for processing image information in a server (claimed).
   ADVANTAGE - The method enables improving operation efficiency and resource utilization rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a server.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for processing image information in a server. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-C04D; T01-F02C2; T01-F03; T01-J10B2; T01-J20B1; T01-L02; T01-N01B3; T01-N02A3C; W04-W05A
IP G06T-001/20; G06F-009/50
PD CN109064382-A   21 Dec 2018   G06T-001/20   201912   Pages: 11   Chinese
AD CN109064382-A    CN10645137    21 Jun 2018
PI CN10645137    21 Jun 2018
UT DIIDW:2019006394
ER

PT P
PN CN109063670-A
TI Printed object-based letter grouping full word recognizing method, involves determining whether first and second recognition networks are deep convolutional neural network, and finishing full word identifying.
AU ZHENG R
   LI M
   HE J
   XU S
   WU B
AE UNIV DALIAN MINZU (UYND-C)
GA 2019006568
AB    NOVELTY - The method involves obtaining a color image and full text word according to initial letter of language word to perform full text word grouping. Language letter word is assigned to a first recognition network. Meanings of individual letter are independently expressed. Different letter is allocated to a second recognition network. Determination is made to check whether first and second recognition networks are a deep convolutional neural network. Full word identifying is finished. A convolutional neural network is formed with a convolutional layer, a down-sampling layer and a connecting layer.
   USE - Printed object-based letter grouping full word recognizing method.
   ADVANTAGE - The method enables reducing identification defects and improving correct identification rate.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a printed object-based letter grouping full word recognizing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-E01A; T01-J10B2A; T01-J10B3B; T04-D08
IP G06K-009/00; G06N-003/04
PD CN109063670-A   21 Dec 2018   G06K-009/00   201912   Pages: 16   Chinese
AD CN109063670-A    CN10934923    16 Aug 2018
PI CN10934923    16 Aug 2018
UT DIIDW:2019006568
ER

PT P
PN CN109063164-A
TI Deep learning based intelligent question answering method, involves performing model prediction process by using original conversation data, and inputting question data to automatically generate comment data.
AU ZHONG L
   XIA Y
   FANG P
AE BAIZHUO NETWORK TECHNOLOGY CO LTD (BAIZ-Non-standard)
GA 201900669E
AB    NOVELTY - The method involves collecting (S1) original conversation data. Original conversation data pre-processing operation is performed (S2), where the original conversation data pre-processing operation comprises text vectorized representation. Original dialog data is converted into model-understandable number. The original dialog data is added to an identifier, where the identifier comprises a start identifier and an end identifier. A Seq2seq model is constructed (S3). A coding layer and a decoding layer are established. Model prediction process is performed (S4) by using the original conversation data. Question data is input to automatically generate comment data.
   USE - Deep learning based intelligent question answering method.
   ADVANTAGE - The method enables reducing error propagation problem and question answering complexity.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based intelligent question answering method. '(Drawing includes non-English language text)'
   Step for collecting original conversation data (S1)
   Step for performing original conversation data pre-processing operation, where original conversation data pre-processing operation comprises text vectorized representation (S2)
   Step for constructing Seq2seq model (S3)
   Step for performing model prediction process by using original conversation data (S4)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-D02; T01-J05B4P; T01-J30A; W04-P01F3; W04-W05A
IP G06F-017/30; G06N-003/04; G06N-003/08
PD CN109063164-A   21 Dec 2018   G06F-017/30   201912   Pages: 13   Chinese
AD CN109063164-A    CN10927717    15 Aug 2018
PI CN10927717    15 Aug 2018
UT DIIDW:201900669E
ER

PT P
PN CN109063639-A
TI Real-time brain activity predicting method, involves outputting time sequence characteristic, and classifying characteristic using determination unit for outputting probability of brain action to realize prediction of brain activity.
AU ZHOU F
   WANG H
   CHEN Y
AE UNIV ZHEJIANG (UYZH-C)
GA 2019006574
AB    NOVELTY - The method involves pre-processing a brain signal. The brain signal is obtained as an input vector by using a space characteristic extracting unit to perform input vector space feature fetching process for obtaining input vector space characteristic. Time sequence characteristic is output using a timing memory unit for updating space feature memory. The time sequence characteristic is classified by using a behavior determination unit for outputting probability of brain action to realize prediction of brain activity. Input vector space feature fetching process is performed by using trained perceptron, a fully-connected neural network, a convolutional neural network, a depth confidence neural networks and a Boltzmann machine or a limiting Boltzmann machine.
   USE - Real-time brain activity predicting method.
   ADVANTAGE - The method enables extracting a time sequence signal contained in trend information by timing memory cycle processing so as to improve prediction accuracy of brain activity.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a real-time brain activity predicting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2
IP G06K-009/00
PD CN109063639-A   21 Dec 2018   G06K-009/00   201912   Pages: 7   Chinese
AD CN109063639-A    CN10853377    30 Jul 2018
PI CN10853377    30 Jul 2018
UT DIIDW:2019006574
ER

PT P
PN CN109063845-A
TI Method for generating sample in robot system based on deep learning, involves generating multiple samples from sample supervised learning step, and performing supervised learning function on depth learning model by using samples.
AU ZHU D
AE DAGUO INNOVATIVE INTELLIGENT TECHNOLOGY (DAGU-Non-standard)
GA 201900651R
AB    NOVELTY - The method involves initializing a depth learning model. A sample unsupervised learning step is generated by training the depth learning model. A sample is selected from multiple first samples. Determination is made to check whether the first samples satisfy a preset condition. Multiple second samples are generated from the sample supervised learning step. Supervised learning function is performed on the depth learning model by using the second samples. Judgment is made to check whether an output of the first sample is consistent with an expected output of the second sample. The depth learning model is tested by using a real sample.
   USE - Method for generating a sample in a robot system (claimed) based on deep learning.
   ADVANTAGE - The method enables combining a current sample with real samples so as to improve an application range of the depth learning model, so that universality of the depth learning model can be improved without using the sample during training condition in an effective manner, thus increasing size of the sample and improving accuracy of the depth learning model in an automatic manner by using the sample.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning based sample generating system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for generating a sample in a robot system based on deep learning. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J30A
IP G06N-099/00
PD CN109063845-A   21 Dec 2018   G06N-099/00   201912   Pages: 20   Chinese
AD CN109063845-A    CN10774082    15 Jul 2018
PI CN10774082    15 Jul 2018
UT DIIDW:201900651R
ER

PT P
PN CN109009102-A
TI Electroencephalogram depth learning based auxiliary diagnostic method, involves distinguishing electroencephalogram sample data when disease onset probability of electroencephalogram sample data exceeds predetermined probability.
AU CHEN Z
   XIAO Y
   LIU J
AE UNIV CENT SOUTH (UYCS-C)
GA 2018A5473H
AB    NOVELTY - The method involves acquiring collected electroencephalogram (EEG) sample data (S10). The EEG sample data is integrated into a preset standardization model to obtain normalized EEG data. The normalized EEG data is converted (S20) into a word embedding vector according to a predetermined lexical embedded model. Feature extraction process is performed (S30) to the word embedding vector. Disease onset probability is output (S40) according to a time mark and identification diagnosis. The EEG sample data is distinguished when the disease onset probability of EEG sample data exceeds predetermined probability.
   USE - EEG depth learning based auxiliary diagnostic method.
   ADVANTAGE - The method enables automatically diagnosing EEG of a patient by a training model, automatically identifying and marking a time zone of seizures, outputting disease onset probability, reducing working efficiency of a clinical doctor and improving diagnosis efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an EEG depth learning based auxiliary diagnostic system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an EEG depth learning based auxiliary diagnostic method. '(Drawing includes non-English language text)'
   Step for acquiring collected EEG sample data (S10)
   Step for converting normalized EEG data into word embedding vector (S20)
   Step for performing feature extraction process to word embedding vector (S30)
   Step for outputting disease onset probability (S40)
DC B04 (Natural products and polymers. Including testing of body fluids (other than blood typing or cell counting), pharmaceuticals or veterinary compounds of unknown structure, testing of microorganisms for pathogenicity, testing of chemicals for mutagenicity or human toxicity and fermentative production of DNA or RNA. General compositions.); P31 (Diagnosis, surgery (A61B).); S05 (Electrical Medical Equipment)
MC B11-C08B; B11-C11; B12-K04G2D; S05-D01A2
IP A61B-005/0476; A61B-005/00
PD CN109009102-A   18 Dec 2018   A61B-005/0476   201912   Pages: 15   Chinese
AD CN109009102-A    CN10909558    10 Aug 2018
PI CN10909558    10 Aug 2018
UT DIIDW:2018A5473H
ER

PT P
PN CN109008952-A
TI Deep learning based intelligent user physiological state monitoring method, involves determining physiological state of user according to output result, and determining monitoring strategy according to physiological state of user.
AU DING X
   LI T
   JIN D
   YIN Y
AE SHENZHEN ZHIHUILIN NETWORK TECHNOLOGY CO (SHEN-Non-standard)
GA 2018A54775
AB    NOVELTY - The method involves acquiring a face image, voice information and physiological data. The face image is processed to obtain first target data. The voice information is processed to obtain second target data. The physiological data is processed to obtain third target data. Three target data are input into a preset network model for performing forward operation to obtain an output result. Physiological state of a user is determined according to the output result. A monitoring strategy is determined according to physiological state of the user.
   USE - Deep learning based intelligent user physiological state monitoring method.
   ADVANTAGE - The method enables performing state monitoring operation in an effective manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a deep learning based intelligent user physiological state monitoring device
   (2) a computer-readable storage medium for storing a set of instructions for performing intelligent user physiological state monitoring operation.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based intelligent user physiological state monitoring method. '(Drawing includes non-English language text)'
DC P31 (Diagnosis, surgery (A61B).); S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D01; T01-J10B2; T01-N01B3; T01-N01E; T01-N02B2; T01-S03
IP A61B-005/00
PD CN109008952-A   18 Dec 2018   A61B-005/00   201912   Pages: 18   Chinese
AD CN109008952-A    CN10432383    08 May 2018
PI CN10432383    08 May 2018
UT DIIDW:2018A54775
ER

PT P
PN CN109034016-A
TI S-convolutional neural network model based hand back vein image identifying method, involves judging whether hand vein image is matched with vein object in data set when probability value of output layer is less than preset threshold value.
AU JIA X
   SUN F
   CAO Y
AE UNIV LIAONING TECHNOLOGY (UYAO-C)
GA 2018A4853S
AB    NOVELTY - The method involves establishing an initial data set to collect hand vein images of multiple collected objects. A training data set is established. Different scale scaling transformation process is performed on each hand vein image of each collected object. An S-convolutional neural network (CNN) model is established, where the S-CNN model includes an input layer and a convolution layer. Judgment is made to check whether a vector of an output layer of the S-CNN model is consistent with a vector of a vein image label. Each hand vein image is input into the S-CNN model. Judgment is made to check whether the hand vein image is matched with a vein object in the data set when a maximum probability value of the output layer is less than a preset threshold value.
   USE - S-CNN model based hand back vein image identifying method.
   ADVANTAGE - The method enables simulating multiple translations, rotations, and scale changing processes of the hand vein images by expanding acquired images in the data set to identify the hand back vein image in an easy manner using the CNN model so as to enhance robustness by adopting a back vein image identifying algorithm.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an S-CNN model. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J04B2; T01-J10B2; T01-J10B3; T01-N01B3
IP G06K-009/00; G06K-009/62; G06N-003/04
PD CN109034016-A   18 Dec 2018   G06K-009/00   201912   Pages: 15   Chinese
AD CN109034016-A    CN10760335    12 Jul 2018
PI CN10760335    12 Jul 2018
UT DIIDW:2018A4853S
ER

PT P
PN CN109033994-A
TI Convolutional neural network-based human face expression identifying method, involves inputting test set of images into trained neural network model, and calculating recognition rate of facial expression picture.
AU JIANG Y
   GE S
   GUO Y
   WANG G
   YANG F
AE UNIV LIAONING TECHNOLOGY (UYAO-C)
GA 2018A4854C
AB    NOVELTY - The method involves collecting a facial expression picture by a digital camera, a mobile phone or monitoring device. Large magnitude information of the facial expression picture is obtained. The facial expression picture is divided into a training set and a test set. A convolution neural network model is established. The convolutional neural network model is trained and stored. The test set of images are inputted into the trained neural network model. A recognition rate of the facial expression picture is calculated.
   USE - Convolutional neural network-based human face expression identifying method.
   ADVANTAGE - The method enables improving convergence speed of the convolutional neural network model so as to improve identification efficiency, thus increasing accuracy of the convolutional neural network model, and hence improving face expression recognition efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network-based human face expression identifying method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J04B2; T01-J10B2A; T01-N01B3A; T01-N02B2; T04-D04; T04-D07F1; W04-J07
IP G06K-009/00; G06K-009/62; G06N-003/04; G06N-003/08
PD CN109033994-A   18 Dec 2018   G06K-009/00   201912   Pages: 12   Chinese
AD CN109033994-A    CN10717584    03 Jul 2018
PI CN10717584    03 Jul 2018
UT DIIDW:2018A4854C
ER

PT P
PN CN109033993-A
TI Image recognition switch door detecting method, involves determining whether judgment result exceeds certain ratio threshold value of current state of switch door, and outputting state transition signal of switch door.
AU LIN J
   LI J
   WANG Q
   CAO H
   ZHOU J
   ZHOU Y
AE NANJING XINGZHEYI INTELLIGENT TRAFFIC (NANJ-Non-standard)
GA 2018A4854D
AB    NOVELTY - The method involves acquiring an image or video through a camera device in a bus. An image sample set is obtained. A closing state of the sample image set is manually classified. A deep neural network door detection model is obtained by using an image training neural network. The acquired image is analyzed to output a state instruction by adopting a trained neural network model. A current state of a switch door and the state instruction are compared and analyzed by adopting a state transformation algorithm. Determination is made to check whether a judgment result exceeds a certain ratio threshold value of the current state of the switch door. A state transition signal of the switch door is output.
   USE - Image recognition switch door detecting method.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an image recognition switch door detecting device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an image recognition switch door detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2A; T01-N01B3
IP G06K-009/00; G06N-003/08; G06N-003/04
PD CN109033993-A   18 Dec 2018   G06K-009/00   201912   Pages: 11   Chinese
AD CN109033993-A    CN10714050    29 Jun 2018
PI CN10714050    29 Jun 2018
UT DIIDW:2018A4854D
ER

PT P
PN CN109031415-A
TI Vibrating source data ringing suppression method based on deep convolutional neural network, involves inputting actual seismic data into trained network and receiving result of ringing suppression at output.
AU LU W
   JIA Z
AE UNIV TSINGHUA (UYQI-C)
GA 2018A5546B
AB    NOVELTY - The method involves extracting the seismic wavelet from the actual seismic data, and synthesizing the pseudo-reflection coefficient, convoluting the seismic wavelet with the pseudo-reflection coefficient to obtain synthetic seismic data. The pseudo-reflection coefficient is used as the output to be fitted, the seismic data is synthesized as an input, and the deep convolutional neural network is trained. The actual seismic data is inputted into the trained network and the result of ringing suppression is received at the output.
   USE - Vibrating source data ringing suppression method based on a deep convolutional neural network.
   ADVANTAGE - The method provides uses a deep convolutional neural network to de-ring a two-dimensional seismic gather, and the processing result after the ringing suppression is output through the forward propagation of the neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the vibrating source data ringing suppression method. (Drawing includes non-English language text)
DC S03 (Scientific Instrumentation)
MC S03-C01X
IP G01V-001/30; G01V-001/34
PD CN109031415-A   18 Dec 2018   G01V-001/30   201912   Pages: 12   Chinese
AD CN109031415-A    CN10634656    20 Jun 2018
PI CN10634656    20 Jun 2018
UT DIIDW:2018A5546B
ER

PT P
PN CN109039472-A
TI Deep learning based data center optical communication dispersion estimation and management method, involves training artificial neural network equalizer, and realizing estimation and compensation of optical channel dispersion.
AU QU G
AE NANTONG ZHIDA INFORMATION TECHNOLOGY CO (NANT-Non-standard)
GA 2018A5499K
AB    NOVELTY - The method involves training an artificial neural network (ANN) equalizer of an optical channel by using impulse response data. Model parameters of an ANN are optimized. Nonlinear response model of the ANN equalizer is established. Transmission data of the optical channel is processed by using the trained ANN equalizer. Estimation and compensation of optical channel dispersion are realized. A transmitting module is provided with a direct modulation laser or an externally modulated laser. The ANN-based nonlinear feed-forward equalizer structure is provided with an input layer, a hidden layer and an output layer. The hidden layer and the output layer are provided with nodes. Response function of the ANN equalizer is determined.
   USE - Deep learning based data center optical communication dispersion estimation and management method.
   ADVANTAGE - The method enables improving optical signal-to-noise ratio, and increasing transmission distance of optical communication.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a deep learning based data center optical communication dispersion estimation and management method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W02 (Broadcasting, Radio and Line Transmission Systems)
MC T01-J05B4P; T01-N01B3; T01-N01D; W02-C04A3
IP H04B-010/69; G06N-003/04
PD CN109039472-A   18 Dec 2018   H04B-010/69   201912   Pages: 15   Chinese
AD CN109039472-A    CN10754897    11 Jul 2018
PI CN10754897    11 Jul 2018
UT DIIDW:2018A5499K
ER

PT P
PN CN109034372-A
TI Probability based neural network pruning method, involves ending pruning probability when value of neural network layer reaches target pruning rate, and obtaining pruned neural network model when neural network model accuracy is increased.
AU WANG H
   HU H
   WANG Y
AE UNIV ZHEJIANG (UYZH-C)
GA 2018A4844E
AB    NOVELTY - The method involves establishing a to-be-pruned neural network model. A training data set and a network structure configuration file are obtained for training the to-be-pruned neural network model. Target pruning rate of a neural network layer is defined. A group parameter of the neural network layer is updated. Same participation probability is shared by parameters in same group. The participation probabilities are initialized to 1. Pruning probability is ended when value of the neural network layer reaches target pruning rate. Pruned neural network model is obtained when trained neural network model accuracy is increased.
   USE - Probability based neural network pruning method.
   ADVANTAGE - The method enables reducing calculation amount of a deep learning model represented by a convolutional neural network, and realizing deep learning models deployed on resource-constrained devices such as mobile phones, wearable devices to achieve application of intelligent algorithms on a mobile end.
   DESCRIPTION OF DRAWING(S) - The drawing shows a graphical view illustrating relationship between target pruning rate and participation probability.
DC T01 (Digital Computers)
MC T01-J05B2B; T01-N01B3; T01-N01D2
IP G06N-003/04; G06N-003/08
PD CN109034372-A   18 Dec 2018   G06N-003/04   201912   Pages: 7   Chinese
AD CN109034372-A    CN10691867    28 Jun 2018
PI CN10691867    28 Jun 2018
UT DIIDW:2018A4844E
ER

PT P
PN CN109033945-A
TI Deep learning based human body outline extraction method, involves testing structure of convolutional neural network character model, and recording time-consuming of contour image of human body to evaluate contour image of human body.
AU WANG L
   DONG N
AE UNIV XIAN TECHNOLOGY (UYXT-C)
GA 2018A4855N
AB    NOVELTY - The method involves extracting a Gabor texture feature map of an original image. A Canny edge feature map of the original image is extracted. A convolutional neural network (CNN) architecture is constructed for performing human contour extraction. The Gabor texture feature map and the extracted Canny edge feature map of the original image are jointly transmitted to the CNN architecture to generate a CNN character model. A structure of the trained CNN character model is tested to obtain a contour image of a human body. Overlap rate and time-consuming of the contour image of the human body are record to evaluate the contour image of the human body.
   USE - Deep learning based human body outline extraction method.
   ADVANTAGE - The method enables improving extraction accuracy, thus improving detection rate and reducing testing time.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based human body outline extraction method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J10B2; T01-N01B3A; T01-N01D1B; T04-D03; T04-D04
IP G06K-009/00; G06K-009/46; G06K-009/62
PD CN109033945-A   18 Dec 2018   G06K-009/00   201912   Pages: 17   Chinese
AD CN109033945-A    CN10582283    07 Jun 2018
PI CN10582283    07 Jun 2018
UT DIIDW:2018A4855N
ER

PT P
PN CN109040091-A
TI Deep neural network model encryption method, involves matching response signal with key information, verifying user equipment when matching is successful, and decrypting encrypted model by using user equipment.
AU ZHANG X
AE JEEJIO BEIJING TECHNOLOGY CO LTD (JEEJ-Non-standard)
GA 2018A47229
AB    NOVELTY - The method involves generating key information by a server according to device information of a user equipment. A key generation mode is set. Key information is selected. A deep neural network (DNN) model to-be-encrypted is encrypted by using an encryption algorithm for generating an encrypted DNN model and a key. The encrypted DNN model and the key are transmitted to the user equipment. Response signal is matched with key information. The user equipment is verified when the matching is successful. The encrypted DNN model is decrypted by using the user equipment.
   USE - Deep neural network model encryption method.
   ADVANTAGE - The method enables realizing encrypted protection for the DNN model.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep neural network model encryption device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep neural network model encryption method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W01 (Telephone and Data Transmission Systems)
MC T01-D01A; T01-J12C; T01-N01D3; T01-N02B1B; W01-A05A; W01-A06E1
IP H04L-029/06; G06F-021/60
PD CN109040091-A   18 Dec 2018   H04L-029/06   201912   Pages: 11   Chinese
AD CN109040091-A    CN10942143    17 Aug 2018
PI CN10942143    17 Aug 2018
UT DIIDW:2018A47229
ER

PT P
PN CN109009097-A
TI Self-adaptive multi-sampling frequencies EEG classification method, involves establishing convolutional neural network-E classification model, and performing length sample data training and testing process.
AU ZHANG Z
   WEN T
AE UNIV XIAMEN (UYXI-C)
GA 2018A5473N
AB    NOVELTY - The method involves establishing a convolutional neural network (CNN)-E classification model. Length sample data training and testing process is performed. The convolutional neural network is fixed with a convolutional layer, a tank layer, a connecting layer and a softmax layer. Input signal data checking process is performed to obtain a characteristic pattern by the convolutional layer. A convolution operation feature map is obtained by performing down-sampling process. Single-dimensional single channel EEG signal data is obtained by the CNN-E classification model for classifying the input sample data.
   USE - Self-adaptive multi-sampling frequencies EEG classification method.
   ADVANTAGE - The method enables realizing EEG signal data classification process at different sample lengths so as to ensure better classification effect and universality.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a CNN-E classification model. '(Drawing includes non-English language text)'
DC P31 (Diagnosis, surgery (A61B).); T01 (Digital Computers)
MC T01-J04B2; T01-J05B2; T01-N01B3A; T01-N01E
IP A61B-005/0476
PD CN109009097-A   18 Dec 2018   A61B-005/0476   201912   Pages: 11   Chinese
AD CN109009097-A    CN10788700    18 Jul 2018
PI CN10788700    18 Jul 2018
UT DIIDW:2018A5473N
ER

PT P
PN CN109086913-A
TI Power system transient stability evaluation method based on deep learning, involves using support vector machine integrated regression model corresponding to each fixed expectation accident to predict transient stability margin index.
AU LI C
   YIN X
   LIU Y
   ZHANG Q
   SU D
AE UNIV SHANDONG (USHA-C)
   STATE GRID JIANGSU ELECTRIC POWER CO LTD (SGCC-C)
GA 2019023807
AB    NOVELTY - The method involves using the layer features extracted by the stacking noise reduction automatic encoder as the input of the support vector machine sub-learner. The support vector machine integrated regression model is established to predict the transient stability margin index of various operating modes under fixed expected accidents. The plan data and the predicted data are used to generate multiple future operation modes and an online expected accident set based on the online operation mode. The support vector machine integrated regression model corresponding to each fixed expectation accident is used to predict the transient stability margin index. The degree of transient stability of the system under different operating modes is graded based on the utility theory.
   USE - Power system transient stability evaluation method based on deep learning.
   ADVANTAGE - The method combines with the existing distributed parallel computing technology, and finally realizes the optimal network structure capable of adaptively determining a deep learning model with a large number of network nodes and multiple hidden layers, and can quickly and accurately evaluate the temporary power system. The degree of stability of the state meets the requirements of online applications.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a power system transient stability evaluation system based on deep learning.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning-based transient stability evaluation method. (Drawing includes non-English language text)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-E03; T01-E04; T01-J10B2; T01-M02C2; T01-N01A2; W04-W05A
IP G06Q-010/04; G06Q-010/06; G06Q-050/06; G06K-009/62
PD CN109086913-A   25 Dec 2018   G06Q-010/04   201911   Pages: 15   Chinese
AD CN109086913-A    CN10759783    11 Jul 2018
PI CN10759783    11 Jul 2018
UT DIIDW:2019023807
ER

PT P
PN CN109063643-A
TI Method for recognizing facial expression under face information portion hidden condition, involves training automatic expression recognition learning model, and retraining loss function by automatic expression recognition learning model.
AU CHEN T
   WANG R
   SONG Q
   XIE C
   ZHANG J
   LI R
   CHEN H
   HU H
AE CHINESE ACAD SCI HEFEI INST PHYS SCI (CAWZ-C)
GA 2019006570
AB    NOVELTY - The method involves obtaining facial expression data under condition of a hidden face information database. An automatic expression recognition learning model is established based on continuous expression frame image fusion deep learning technology. A to-be-detected expression frame image is obtained. A facial expression recognition process is performed. A face information image frame is input to the automatic expression recognition learning model. A pain score is generated by the automatic expression recognition learning model. The automatic expression recognition learning model is trained. A loss function is retrained by the automatic expression recognition learning model.
   USE - Method for recognizing facial expression under face information portion hidden condition.
   ADVANTAGE - The method enables avoiding rely in a face image expression identification process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for recognizing facial expression under face information portion hidden condition. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-J05B4P; T01-J10B2A; T01-J30A; T04-D02; T04-D04; T04-D07F1; W04-W05A
IP G06K-009/00; G06K-009/32; G06K-009/62
PD CN109063643-A   21 Dec 2018   G06K-009/00   201911   Pages: 10   Chinese
AD CN109063643-A    CN10862983    01 Aug 2018
PI CN10862983    01 Aug 2018
UT DIIDW:2019006570
ER

PT P
PN CN109064454-A
TI Method for detecting defect in product, involves performing image category prediction according to extracted features to implement defect detection of product to be detected.
AU DUAN Y
AE SHANGHAI DIEYU INTELLIGENT TECHNOLOGY CO LTD (SHAN-Non-standard)
GA 2019022533
AB    NOVELTY - The method involves collecting (S1) an image of a product to be detected. The image of the collected product to be detected is pre-processed (S2). The image data obtained by pre-processing is inputted (S3) into a convolutional neural network model trained by using data in a training sample library to implement feature extraction. The training sample library is included with multiple product images of labeled defect classification. The image category prediction is performed (S4) according to the extracted features to implement defect detection of the product to be detected.
   USE - Method for detecting defect in product.
   ADVANTAGE - The influence of external factors on the detection result is reduced. The accurate detection of defects of the production line product is realized. The versatility and accuracy of the detection in a complex environment and a constantly changing environment are improved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for detecting defect in product.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the method for detecting defect in product. (Drawing includes non-English language text)
   Step for collecting the image of the product to be detected (S1)
   Step for pre-processing the image of the collected product to be detected (S2)
   Step for inputting the image data obtained by pre-processing into the convolutional neural network model trained by using data in the training sample library to implement feature extraction (S3)
   Step for performing image category prediction according to the extracted features to implement defect detection of the product to be detected (S4)
   Step for performing alarm operation according to the result of the defect detection (S5)
DC T01 (Digital Computers)
MC T01-J05B2; T01-J10B2; T01-N01B3
IP G06T-007/00; G06T-007/10; G06T-007/90
PD CN109064454-A   21 Dec 2018   G06T-007/00   201911   Pages: 9   Chinese
AD CN109064454-A    CN10765439    12 Jul 2018
PI CN10765439    12 Jul 2018
UT DIIDW:2019022533
ER

PT P
PN CN109057776-A
TI Fish swarm algorithm based pumping well fault diagnosis method, involves utilizing swarm algorithm for referring visual field and parameters, which are replaced by exponential function, and determining maximal classification function value.
AU GAO X
   WANG J
   WEI J
   LI X
   ZHENG B
   WANG M
AE UNIV CHINESE NORTHEASTERN (UYDB-C)
GA 201900803C
AB    NOVELTY - The method involves obtaining known fault type of sucker-rod pump oil pumping well indicator diagram to obtain displacement and load of pixel pair. Deep belief network is formed for extracting pre-processed diagram characteristic. Classification function is established for performing calculation of classification function value. Supporting width parameter is optimized. Penalty factor of a vector machine is determined. Swarm algorithm is utilized for referring visual field and parameters, which are replaced by negative exponential function. Maximal classification function value is determined.
   USE - Fish swarm algorithm based pumping well fault diagnosis method.
   ADVANTAGE - The method enables avoiding uncertainty of manual weight and preset bias for improving deep belief network extracting features classifying accuracy, and increasing speed of convergence.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a fish swarm algorithm based pumping well fault diagnosis method. '(Drawing includes non-English language text)'
DC Q49 (Mining (E21)); T01 (Digital Computers)
MC T01-J05B2
IP E21B-047/009; G06N-003/00; G06N-003/04; G06N-003/08
PD CN109057776-A   21 Dec 2018   E21B-047/009   201911   Pages: 23   Chinese
AD CN109057776-A    CN10717992    03 Jul 2018
PI CN10717992    03 Jul 2018
UT DIIDW:201900803C
ER

PT P
PN CN109063834-A
TI Convolution response map based network pruning method, involves performing network pruning operation, extracting target response characteristics, updating initial network structure, and removing samples to obtain final pruned network sample.
AU GE Y
   GAO F
   LU S
   ZHANG Y
   LU J
AE UNIV ZHEJIANG TECHNOLOGY (UYZT-C)
GA 2019006522
AB    NOVELTY - The method involves reading network weight file and network configuration file to obtain an initial network structure. Training sample set sample is read. Width of a training sample image is captured. Image label information is obtained. Number of training sample is detected. Left upper angle coordinates are obtained. An image file is input into a network. A network pruning operation is performed according to a layer feature response diagram. Target response characteristics are extracted. The initial network structure is updated. Samples are removed to obtain final pruned network sample.
   USE - Convolution response map based network pruning method.
   ADVANTAGE - The method enables performing a network redundancy convolution operation according to convolution characteristic picture ratio, effectively realizing automatic feature extraction operation, reducing calculation amount and parameter number of neural network, and solving deep learning problem in real scene.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a convolution response map. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J04B2; T01-J10B2; T01-J10B3A; T01-N01B3; T01-N01D2; T04-D04
IP G06N-003/08; G06K-009/62
PD CN109063834-A   21 Dec 2018   G06N-003/08   201911   Pages: 8   Chinese
AD CN109063834-A    CN10765996    12 Jul 2018
PI CN10765996    12 Jul 2018
UT DIIDW:2019006522
ER

PT P
PN CN109063589-A
TI Neural network-based instrument device online monitoring method, involves sending video data to deep neural network to calculate similarity by characteristic vector, and determining whether instrument is online if similarity meets condition.
AU GUO C
   LIN H
   SUN Y
AE UNIV HANGZHOU DIANZI (UYHH-C)
GA 201900658A
AB    NOVELTY - The method involves using a data set for training a deep neural network, and using a deep neural network model. A large amount of data is trained to make a neural network, and similarity measure is learned from the data. A video data is acquired by a video capture device and sent to the deep neural network in the server to calculate similarity through a characteristic vector of the neural network. A determination is made whether an instrument is online when the similarity meets the preset condition.
   USE - Neural network-based instrument device online monitoring method.
   ADVANTAGE - The method enables online monitoring of the instrument device in an accurate and efficient manner with high training efficiency, strong convergence, high modeling precision and better detection effect.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a neural network-based instrument device online monitoring system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a neural network-based instrument device online monitoring system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-E03; T01-E04; T01-J04C; T01-J10B2; T01-N01B3; T01-N01D1B; T01-N01D3; T01-N02B2
IP G06K-009/00; G06K-009/62; G06N-003/08
PD CN109063589-A   21 Dec 2018   G06K-009/00   201911   Pages: 9   Chinese
AD CN109063589-A    CN10762360    12 Jul 2018
PI CN10762360    12 Jul 2018
UT DIIDW:201900658A
ER

PT P
PN CN109063825-A
TI Convolutional neural network acceleration device has block floating-point converter which converts block floating-point output result of convolutional layer to obtain floating-point output result of convolutional layer.
AU JI X
   LIAN X
AE UNIV TSINGHUA (UYQI-C)
GA 2019022494
AB    NOVELTY - The device has a floating point-block floating point converter which converts the first input feature map group and the first convolution kernel group of the convolution layer to generate a second input feature map group and a second convolution kernel group. A shifter converts a first offset set of the convolution layer into a second offset set according to a block index of the data in the second input feature map group and the second convolution kernel group. A convolutional layer accelerator performs a convolution multiply and add operation according to the second input feature map group, the second convolution kernel group, and the second bias set to obtain a block floating point output result of the convolution layer. A block floating-point converter converts a block floating-point output result of the convolutional layer to obtain a floating-point output result of the convolutional layer as an output characteristic map of the convolutional layer.
   USE - Convolutional neural network acceleration device.
   ADVANTAGE - The accuracy of the convolutional neural network model is ensured and the deviation of the convolutional neural network model parameters in the forward propagation process is effectively avoided, so that the need to retrain the convolutional neural network model in the forward inference process of the convolutional neural network is eliminated.
   DESCRIPTION OF DRAWING(S) - The drawing shows a data flow diagram of single output channel of convolutional neural network acceleration device. (Drawing includes non-English language text)
DC T01 (Digital Computers)
MC T01-E02A; T01-E02B; T01-J04B2
IP G06N-003/04
PD CN109063825-A   21 Dec 2018   G06N-003/04   201911   Pages: 20   Chinese
AD CN109063825-A    CN10865157    01 Aug 2018
PI CN10865157    01 Aug 2018
UT DIIDW:2019022494
ER

PT P
PN CN109063113-A
TI Asymmetric hash discrete depth based rapid image retrieval model construction method, involves collecting lot of training pictures, establishing loss layer asymmetric hash learning model, and performing hash network re-training process.
AU LI H
   MA L
AE CHENGDU KUAIYAN TECHNOLOGY CO LTD (CHEN-Non-standard)
GA 201900670P
AB    NOVELTY - The method involves collecting lot of training pictures. Type of the training pictures is marked. Size of the training pictures is adjusted to fixed size. A database is randomly divided into three sets. A loss layer asymmetric hash learning model is established. An asymmetric hash learning network is constructed for initializing parameter. Number of a query image and a database image is determined. Approximate error is reduced by using additional regular items. Information in the image database is determined. A discrete variable updating process is performed. The database image is outputted for calculating average precision of query. A learning rate adjusting process is performed according to principle of cross-validation. A hash network re-training process is performed.
   USE - Asymmetric hash discrete depth based rapid image retrieval model construction method.
   ADVANTAGE - The method enables constructing a measure learning model by a deep learning and discrete optimization to simultaneously study query image similar to binary code, thus judging whether hash code minimum hamming distance is greater than maximum hamming distance.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an asymmetric depth discrete hash based rapid image retrieval method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic illustration of an asymmetric hash discrete depth based rapid image retrieval model construction method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-E04; T01-G01A; T01-J05B2B; T01-J05B4F; T01-J05B4M; T01-J10B3A; T01-N01B3; T01-N02B1A; T01-N03A2; W04-P01F3; W04-W05A
IP G06F-017/30
PD CN109063113-A   21 Dec 2018   G06F-017/30   201911   Pages: 17   Chinese
AD CN109063113-A    CN10851999    30 Jul 2018
PI CN10851999    30 Jul 2018
UT DIIDW:201900670P
ER

PT P
PN CN109064731-A
TI Picture recognition system for use in data collection system, has learning training library which performs deep learning on data transmitted by digital transmission module and feeds result back to picture processing module.
AU LI Y
   HU P
   LUO G
   LIU Q
AE SHENZHEN MIHUAN ZHICHAO TECHNOLOGY CO (SHEN-Non-standard)
GA 201902234Y
AB    NOVELTY - The system (10) has a file server (1), a picture processing module (2), a digital transmission module (3), and a learning training library (4). The file server is configured to store a picture received by an input end and data transmitted by the digital transmission module. The picture processing module is configured to periodically process the picture in the file server. The digital transmission module is configured to determine an output of the picture processing module. The output of the picture processing module is transmitted to the file server or the learning training library according to the judgment result. The learning training library is configured to perform deep learning on the data transmitted by the digital transmission module, and feed the result back to the picture processing module. The picture processing module includes a picture capture module and a picture recognition module. The picture capture module periodically captures the picture from the file server.
   USE - Picture recognition system for use in data collection system (claimed).
   ADVANTAGE - The data collection system with the image recognition system with strong adaptability and smart management is realized, and a remote centralized meter reading process with high accuracy and convenience is realized.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a data collection system; and
   (2) a remote centralized meter reading method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating the picture recognition system for use in data collection system. (Drawing includes non-English language text)
   File server (1)
   Picture processing module (2)
   Digital transmission module (3)
   Learning training library (4)
   Picture recognition system (10)
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems); W05 (Alarms, Signalling, Telemetry and Telecontrol)
MC T01-J10B2A; T01-J10E; T01-N01B3; T01-N01D1B; T01-N01D2; T01-N01D3; W04-W05A; W05-D08E
IP G08C-019/00; G08C-017/02; G06K-009/00
PD CN109064731-A   21 Dec 2018   G08C-019/00   201911   Pages: 11   Chinese
AD CN109064731-A    CN11168567    08 Oct 2018
PI CN11168567    08 Oct 2018
UT DIIDW:201902234Y
ER

PT P
PN CN109062094-A
TI Method for controlling model exhibited in museum, involves performing deep learning based on control message and knowledge base, and generating response instruction when knowledge base does not have response instruction.
AU LU K
AE LU K (LUKK-Individual)
GA 2019021236
AB    NOVELTY - The method involves parsing (S10) a control message when receiving a control message, and detecting whether a knowledge base has a response instruction corresponding to the control message. The deep learning is performed (S20) according to the control message and the knowledge base, and the response instruction is generated when detecting that the knowledge base does not have the response instruction corresponding to the control message. The response instruction is executed (S30) by the control model.
   USE - Method for controlling model exhibited in museum, science museum, park, convention center, and shopping mall.
   ADVANTAGE - The problem that the model realizes intelligent interaction with the user is solved. The user experience is improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a control device for a model; and
   (2) a computer readable storage medium storing program for controlling a model.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the method for controlling model. (Drawing includes non-English language text)
   Step for parsing the control message (S10)
   Step for performing deep learning (S20)
   Step for executing the response instruction (S30)
DC T01 (Digital Computers); T06 (Process and Machine Control); W04 (Audio/Video Recording and Systems)
MC T01-J11A1; T01-J16A; T01-J30A; T06-A04B1; W04-X03G7
IP G05B-019/042
PD CN109062094-A   21 Dec 2018   G05B-019/042   201911   Pages: 14   Chinese
AD CN109062094-A    CN10823105    24 Jul 2018
PI CN10823105    24 Jul 2018
UT DIIDW:2019021236
ER

PT P
PN CN109067289-A
TI Method for controlling depth-learning optimized position sensorless BLDC sliding mode observer, involves detecting line back electromotive force in real time, and performing adaptive online adjustment on optimized parameters.
AU LU Y
   ZHAO P
   CHEN Y
   XIE X
   ZHANG H
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 201900569X
AB    NOVELTY - The method involves establishing (S1) a differential equation of the position sensorless BLDC sliding mode observer to determine the range of parameters to be optimized based on the current and line back electromotive (EMF) observation errors. A deep learning network is constructed (S2), take the differential of the BLDC line back electromotive force observation error and the BLDC line back electromotive force observation error as the input of the network, and the deep learning training network is adopted to output the optimized parameters. The optimized parameters is inputted (S3) into the sliding mode observer to update the parameters to be optimized, detect the line back electromotive force is detected n real time, and adaptive online adjustment is performed on the optimized parameters.
   USE - Method for controlling depth-learning optimized position sensorless BLDC sliding mode observer.
   ADVANTAGE - The method achieves good adaptive effect, which provides an effective way for the stable operation of the position sensorless BLDC. The system adjustment is more flexible and the chattering phenomenon is reduced.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a method for controlling depth-learning optimized position sensorless BLDC sliding mode observer. (Drawing includes non-English language text)
   Step for establishing controlling depth-learning optimized position sensorless BLDC sliding mode observer (S1)
   Step for constructing a deep learning network take the differential of the BLDC line back electromotive force observation error and the BLDC line back electromotive force observation error as the input of the network (S2)
   Step for inputting the optimized parameters into the sliding mode observer to update the parameters to be optimized (S3)
   Step for initializing deep belief network parameter (S22)
   Step for using a contrast divergence algorithm and adjusting weights by a back propagation algorithm (S23)
DC V06 (Electromechanical Transducers and Small Machines); X13 (Switchgear, Protection, Electric Drives)
MC V06-N03; X13-H01E3
IP H02P-023/00; H02P-023/12
PD CN109067289-A   21 Dec 2018   H02P-023/00   201911   Pages: 12   Chinese
AD CN109067289-A    CN11060234    12 Sep 2018
PI CN11060234    12 Sep 2018
UT DIIDW:201900569X
ER

PT P
PN CN109063569-A
TI Remote sensing image based detection method of semantic level change, involves cutting input image into small image pairs and performing splicing after pairwise change detection, if input image exceeds limitation of computer display.
AU SHI Z
   SHI T
   WU X
   CHEN H
   WANG B
   WANG H
   YU W
AE UNIV BEIHANG (UNBA-C)
   CHINA RESOURCE SATELLITE DATE & APP CENT (CAER-C)
GA 201902242K
AB    NOVELTY - The method involves training a convolutional neural network to build a full convolutional neural network using a produced data set, and using a Caffe deep learning framework to train and record the corresponding neural network parameters. The visible bands of an input remote sensing image are extracted and matched according to a format of training set data, and a three-tuning neural network is used to annotate the test set data or the data other than the data set, so as to detect the change of remote sensing image. The input image is cut into small image pairs and a splicing is performed after pairwise change detection to obtain a final result, if the input image exceeds the limitation of computer display.
   USE - Remote sensing image based detection method of semantic level change.
   ADVANTAGE - The problem of extracting semantic level change information from the multi-temporal remote sensing images is avoided, while ensuring a high automation degree and a detection accuracy. Hence the efficiency of change detection is greatly improved, and the broad application prospect and value are ensured. The labor cost is greatly reduced.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the remote sensing image based detection method of the semantic level change. (Drawing includes non-English language text)
DC T01 (Digital Computers)
MC T01-J10B2; T01-J16C3; T01-N01B3A
IP G06K-009/00; G06K-009/62
PD CN109063569-A   21 Dec 2018   G06K-009/00   201911   Pages: 12   Chinese
AD CN109063569-A    CN10721849    04 Jul 2018
PI CN10721849    04 Jul 2018
UT DIIDW:201902242K
ER

PT P
PN CN109068174-A
TI Video frame rate up-conversion method based on circular convolutional neural network, involves inserting intermediate frame image into image block to obtain target video whose video frame rate is up-converted.
AU SONG L
   ZHANG Z
   XIE R
   CHEN L
AE UNIV SHANGHAI JIAOTONG (USJT-C)
GA 201900548S
AB    NOVELTY - The method involves receiving an initial video transmitted by a transmitting end. The initial video is divided into multiple sets of image blocks comprising two consecutive frames of images. Two consecutive frames of the image block are taken as input to a target circular convolutional neural network. The intermediate frame images are combined corresponding to the consecutive two frames of images. The target circular convolutional neural network is trained through a preset training data set. The intermediate frame image is inserted into the image block to obtain a target video whose video frame rate is up-converted.
   USE - Video frame rate up-conversion method based on circular convolutional neural network.
   ADVANTAGE - The mapping from two frames to the intermediate frame is completed. The frame rate of the original video is improved and the up-conversion of the video frame rate is better completed.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a video frame rate up-conversion system based on a circular convolutional neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the video frame rate up-conversion method based on circular convolutional neural network. (Drawing includes non-English language text)
DC T01 (Digital Computers); W03 (TV and Broadcast Radio Receivers)
MC T01-N01B3; T01-N01D1B; W03-A16C5A
IP H04N-021/4402; G06N-003/04
PD CN109068174-A   21 Dec 2018   H04N-021/4402   201911   Pages: 12   Chinese
AD CN109068174-A    CN11059369    12 Sep 2018
PI CN11059369    12 Sep 2018
UT DIIDW:201900548S
ER

PT P
PN CN109062956-A
TI Space-time feature mining method for regional integrated energy system, involves constructing fully connected neural network and extracting feature tensor of external factors based on information of factors related to energy system.
AU TAN M
   YUAN S
   LI H
   CHEN Y
   SU Y
   LI S
AE UNIV XIANGTAN (UNXT-C)
GA 201900674M
AB    NOVELTY - The method involves constructing (S102) a spatiotemporal loop neural network based on the energy flow network data adjacent to the continuous sequence, and extracting time series related feature tensors. A fully connected neural network is constructed (S103) and feature tensor of external factors is extracted based on the information of external influence factors related to the energy system. The tensor and time-dependent feature tensor of the fusion spatial correlation feature is the spatio-temporal correlation feature tensor, and then the spatio-temporal correlation feature tensor is combined (S104) with the external factor feature tensor to obtain the energy flow multi-source feature fusion tensor.
   USE - Space-time feature mining method for regional integrated energy system.
   ADVANTAGE - The method exploits the spatiotemporal correlation characteristics and the external factor characteristics of the multi-energy flow network, so as to provide a decision basis for implementing the comprehensive demand response and multi-energy coordinated operation optimization of the regional integrated energy system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a space-time feature mining method for regional integrated energy system. (Drawing includes non-English language text)
   Step for constructing a deep residual convolutional neural network (S101)
   Step for constructing the spatiotemporal loop neural network (S102)
   Step for constructing fully connected neural network (S103)
   Step for combining multi-time scale spatial correlation feature tensor (S104)
DC T01 (Digital Computers)
MC T01-J04B2; T01-J05B4P
IP G06F-017/30; G06N-003/04
PD CN109062956-A   21 Dec 2018   G06F-017/30   201911   Pages: 14   Chinese
AD CN109062956-A    CN10666018    26 Jun 2018
PI CN10666018    26 Jun 2018
UT DIIDW:201900674M
ER

PT P
PN CN109068105-A
TI Prison video surveillance method based on deep learning, involves displaying potentially dangerous target monitoring in monitoring room of monitoring area and prison command center and automatically saving frame with criminal violations.
AU WANG H
AE WANG H (WANG-Individual)
GA 2019005507
AB    NOVELTY - The method involves using the deep learning-based target detection model to detect the current frame in the prison surveillance video. According to the test results and the corresponding rules, determination is made whether there are potential dangers and criminal violations. The potentially dangerous target monitoring in the monitoring room of the monitoring area and the prison command center is displayed. The frames are automatically saved with criminal violations as the basis for video surveillance notifications.
   USE - Prison video surveillance method based on deep learning.
   ADVANTAGE - The economical and efficient scheduling and monitoring method is provided for the prison video surveillance system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a prison video surveillance method based on deep learning. (Drawing includes non-English language text)
DC T01 (Digital Computers); W02 (Broadcasting, Radio and Line Transmission Systems); W04 (Audio/Video Recording and Systems)
MC T01-J10B2; T01-J30A; W02-F01A5; W04-W05A
IP H04N-007/18; G06K-009/00
PD CN109068105-A   21 Dec 2018   H04N-007/18   201911   Pages: 8   Chinese
AD CN109068105-A    CN11091834    20 Sep 2018
PI CN11091834    20 Sep 2018
UT DIIDW:2019005507
ER

PT P
PN CN109067232-A
TI Recurrent neural network based grid-connected inverter current controlling method, involves determining pulse width modulation wave as drive pulse, and performing grid-connected inverter system current controlling process.
AU WANG Q
   WANG M
   WANG X
   CAO B
AE UNIV CHINA GEOSCIENCES WUHAN (UYCI-C)
GA 201900571B
AB    NOVELTY - The method involves collecting a three-phase power grid voltage signal and a current signal (S101). A coordinate system voltage signal is obtained by a Clarke transform according to the collected three- phase power grid voltage signal. A two-phase voltage signal is obtained. A three-phase power grid voltage phase is obtained. A current difference signal is obtained (S102). The current difference signal and an integrated signal are input (S103) to a recurrent neural network controller. An output voltage value of a grid-connected inverter system is obtained (S104). A pulse width modulation (PWM) wave is generated (S105) by performing a space vector PWM (SVPWM) process. The PWM wave is determined as a drive pulse of the grid-connected inverter system. A grid-connected inverter system current controlling process is performed.
   USE - Recurrent neural network based grid-connected inverter current controlling method.
   ADVANTAGE - The method enables accelerating convergence speed of a grid-connected inverter neural network controller, increasing dynamic response capability and anti-jamming ability and avoiding grid-connected inverter nonlinearity and parameter coupling problem.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a recurrent neural network based grid-connected inverter current controlling method. '(Drawing includes non-English language text)'
   Step for collecting a three-phase power grid voltage signal and a current signal (S101)
   Step for obtaining a current difference signal (S102)
   Step for input the current difference signal and an integrated signal to a recurrent neural network controller (S103)
   Step for obtaining an output voltage value of a grid-connected inverter system (S104)
   Step for generating a PWM wave (S105)
DC S01 (Electrical Instruments); T01 (Digital Computers); U22 (Pulse Generation and Manipulation); X12 (Power Distribution/Components/Converters)
MC S01-H07A; T01-J08A; U22-E01A; X12-H01B; X12-J01A9; X12-J05
IP H02M-007/5395; H02J-003/44
PD CN109067232-A   21 Dec 2018   H02M-007/5395   201911   Pages: 15   Chinese
AD CN109067232-A    CN10878593    03 Aug 2018
PI CN10878593    03 Aug 2018
UT DIIDW:201900571B
ER

PT P
PN CN109062561-A
TI Python-based dangerous driving warning system, has fatigue driving detector which performs facial marker detection on video captured by camera through face detector built and detects closed eye motion in video.
AU WANG Q
   ZHENG Z
AE HUAIHAI TECHNOLOGY INST (HUTE-C)
GA 201900684P
AB    NOVELTY - The system has seat belt detector which processes, detects and recognizes a picture acquired by a camera. The seat belt detection is a safety belt based on a Hough transform. The straight lines similar to parallel, spacing, and slope are calculated. The approximate strip shape detection process calculates the slope between the distance between the edge pixels and the multi-point. The fatigue driving detector performs facial marker detection on a video captured by the camera through a face detector built and detects a closed eye motion in the video. The handheld call behavior detector acquires images by the camera through the Keras construct convolutional neural network training classifier classification pictures.
   USE - Python-based dangerous driving warning system.
   ADVANTAGE - The system is compact and fast and can detect seat belt detection, fatigue driving detection, hand-held call behavior detection, and wider functions. The combination of multiple detection methods makes safety detection more accurate and diversified.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a Python-based dangerous driving warning system. (Drawing includes non-English language text)
DC T01 (Digital Computers)
MC T01-J05B2; T01-J10B2A; T01-N01B3
IP G06F-008/30
PD CN109062561-A   21 Dec 2018   G06F-008/30   201911   Pages: 14   Chinese
AD CN109062561-A    CN10839167    27 Jul 2018
PI CN10839167    27 Jul 2018
UT DIIDW:201900684P
ER

PT P
PN CN109063096-A
TI Method for quickly training Thai sentiment analysis model, involves modeling with convolutional neural network, taking word vector as input and training output convolution result.
AU WU Y
AE CHENGDU REMARK TECHNOLOGY CO LTD (CHEN-Non-standard)
   HANGZHOU SHUFENG TECHNOLOGY CO LTD (HANG-Non-standard)
GA 2019006717
AB    NOVELTY - The method involves using the reptile tool to capture the comments of the Thai e-commerce website, and classifying the comments into two categories. The favorable data is one type, and the middle evaluation and the bad evaluation data are one type. The Thai word segmentation tool is used to segment the Thai language and Thai word segmentation tool uses the open source project and then uses the N-gram to calculate the word vector, and uses the N-gram rule to convert the corpus after the word segmentation into a word vector. A convolutional neural network is modeled, the word vector is taken as an input and the output convolution result is trained. The pooling results are classified, and the positive and negative values are obtained, and the model training is completed after multiple convolution-pooling cycles as finally using a logistic regression function.
   USE - Method for quickly training Thai sentiment analysis model.
   ADVANTAGE - The method can quickly classify the comments of the Thai e-commerce website, so as to efficiently analyze the products targeted by each comment directly from a large amount of comment data, and the emotional tendency of the product has good practicability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating of the method for quickly training Thai sentiment analysis model. (Drawing includes non-English language text)
DC T01 (Digital Computers)
MC T01-E03; T01-J04B2; T01-J04C; T01-J05B4P; T01-J11A1; T01-N01A2E; T01-N01B3
IP G06F-017/30; G06F-017/27
PD CN109063096-A   21 Dec 2018   G06F-017/30   201911   Pages: 7   Chinese
AD CN109063096-A    CN10841591    27 Jul 2018
PI CN10841591    27 Jul 2018
UT DIIDW:2019006717
ER

PT P
PN CN109064459-A
TI Deep learning-based cloth defect detecting method, involves performing online automatic fabric defect detection process performed on defect sample and non-defect sample, and determining flaw area in rapid cyclic manner.
AU YAO K
   WANG X
   HAO D
AE UNIV JIANGSU TECHNOLOGY (UYJI-Non-standard)
GA 2019006376
AB    NOVELTY - The method involves collecting a training sample. Pretreatment process is performed on the training sample. The training sample is classified into two types as a defect sample and a non-defect sample. An online fabric defect detector is created by utilizing the defect sample and the non-defect sample through convolutional neural network offline learning training process. A trained convolutional neural network fabric defect detector is loaded on a host computer. The trained convolutional neural network fabric defect detector is placed on an automatic production line. Online automatic fabric defect detection process is performed on the defect sample and the non-defect sample. A flaw area is determined in a rapid cyclic manner by utilizing convolutional neural network method.
   USE - Deep learning-based cloth defect detecting method.
   ADVANTAGE - The method enables combining feature and the online fabric defect detector into a framework for automatically learning features from the sample, reducing large workload of manual design features during use so as to improve convenience and robustness of feature selection process and improving automation, intelligent level, accuracy and detection speed in deep learning-based cloth defect detecting and positioning process in an effective manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning-based cloth defect detecting method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-N01B3
IP G06T-007/00
PD CN109064459-A   21 Dec 2018   G06T-007/00   201911   Pages: 12   Chinese
AD CN109064459-A    CN10838891    27 Jul 2018
PI CN10838891    27 Jul 2018
UT DIIDW:2019006376
ER

PT P
PN CN109063714-A
TI Depth neural network based Parkinson slow motion video detection model constructing method, involves performing characteristics extraction function on frame change curve, and evaluating network model to construct video detection model.
AU YIN J
   LIN B
   ZHANG J
   LUO Z
   DENG S
   LI Y
AE UNIV ZHEJIANG (UYZH-C)
GA 2019006552
AB    NOVELTY - The method involves shooting a subject hand during continuous periodical gripping action. Video images are input to Hand-Seg-Net and pose-Net model to obtain position coordinates of key points in a frame image. An image frame index position is determined for calculating a peak value and a valley value of a frame change curve during gripping process. Characteristics extraction function is performed on the frame change curve by using self-similarity-metric process. Different subject video image collecting, pre-processing and characteristic extraction function is performed to obtain large quantities of samples. A network model is evaluated to construct a Parkinson slow motion video detection model by using a test set.
   USE - Depth neural network based Parkinson slow motion video detection model constructing method.
   ADVANTAGE - The method enables adopting expandability and transportability characteristics, replacing a convolutional neural network based on key point extraction process in a precise manner so as to describe motion behaviors, realize similar periodic motion evaluation in an efficient manner and click finger action by using MDS-UPDRS gage.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a depth neural network based Parkinson slow motion video detection model constructing method. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-D01; S05-D06; T01-J10B2; T01-L02; T01-N01E1
IP G06K-009/46; G06K-009/62; G06N-003/08; G16H-050/20
PD CN109063714-A   21 Dec 2018   G06K-009/46   201911   Pages: 13   Chinese
AD CN109063714-A    CN10884790    06 Aug 2018
PI CN10884790    06 Aug 2018
UT DIIDW:2019006552
ER

PT P
PN CN109063747-A
TI Deep learning based intestinal pathology slice image identifying and analyzing system, has server terminal determining pathological result corresponding to intestinal pathological slice image and feeds analysis result to client terminal.
AU YU H
   WU L
   GONG D
AE UNIV WUHAN RENMIN HOSPITAL (UYWU-C)
GA 2019006547
AB    NOVELTY - The system has a client terminal collects and transmits intestinal pathological slice image to a server terminal. The client terminal receives and displays an analysis result fed back by the server terminal. The server terminal determines a pathological result corresponding to the intestinal pathological slice image and feeds the analysis result to the client terminal. The server terminal is provided with a sample database, a convolution neural network model and a web service module. The sample database stores the intestinal pathology slice image of a sample. The convolution neural network model is provided with an intestinal pathology slice picture library.
   USE - Deep learning based intestinal pathology slice image identifying and analyzing system.
   ADVANTAGE - The system is simple to use, improves accuracy and validity of a pathology report, and has high economical value.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning based intestinal pathology slice image identifying and analyzing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a deep learning based intestinal pathology slice image identifying and analyzing system. '(Drawing includes non-English language text)'
DC S05 (Electrical Medical Equipment); T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC S05-D08A; S05-G02G3; T01-J04B2; T01-J05B4P; T01-J10B2; T01-N01B3; T01-N01D1B; T01-N01D3; T01-N01E1; T01-N02A2C; T04-D04
IP G06K-009/62; G06T-007/00; G16H-030/00
PD CN109063747-A   21 Dec 2018   G06K-009/62   201911   Pages: 7   Chinese
AD CN109063747-A    CN10777590    16 Jul 2018
PI CN10777590    16 Jul 2018
UT DIIDW:2019006547
ER

PT P
PN CN109063776-A
TI Method for training image recognition network by utilizing electronic device, involves generating training image, adding training image to training image set, and training image recognition network by using training image set.
AU ZHANG C
   ZHANG S
   JIN H
AE MEGVII TECHNOLOGY LTD (MEGV-Non-standard)
GA 201900653D
AB    NOVELTY - The method involves obtaining a reference characteristic set, where the reference characteristic set comprises a characteristic vector corresponding to a same object distributed in a characteristic space. A resistance network is generated according to the reference characteristic set. A training image is generated. The training image is added to a training image set. An image recognition network is trained by using the training image set. A reference image of the training image set is obtained, where the reference image comprises a same object. The training image set is extracted by a convolutional neural network.
   USE - Method for training an image recognition network by utilizing an electronic device (claimed).
   ADVANTAGE - The method enables generating the training image for identifying network training, improving image recognition precision of the network in the training image set, and increasing number of images in the training image set.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for training an image recognition network by utilizing an electronic device
   (2) an image re-identifying method
   (3) an image re-identifying device
   (4) a computer readable storage medium for storing a set of instructions for training an image recognition network by utilizing an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for training an image recognition network by utilizing an electronic device. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2A; T01-J10B1; T01-J10B2A; T01-J10B3; T01-N01B3; T01-N01D1B; T04-D04
IP G06K-009/62
PD CN109063776-A   21 Dec 2018   G06K-009/62   201911   Pages: 16   Chinese
AD CN109063776-A    CN10893815    07 Aug 2018
PI CN10893815    07 Aug 2018
UT DIIDW:201900653D
ER

PT P
PN CN109064428-A
TI Method for processing image de-noising, involves training constructed convolutional neural network model, and denoising processed image by trained convolutional neural network model.
AU ZHANG G
AE OPPO GUANGDONG MOBILE COMMUNICATION CO (GDOP-C)
GA 2019022539
AB    NOVELTY - The image de-noising processing method involves acquiring (S101) a training image, the training image comprising a source image and a target image corresponding to the source image, the target image being an image having a resolution greater than a preset value, the source image being a noise added to the target image obtained after the information. The constructed convolutional neural network model is trained (S102) through the corresponding source image and target image to obtain a trained convolutional neural network model. The processed image is de-noised (S103) by the trained convolutional neural network model.
   USE - Method for processing image de-noising.
   ADVANTAGE - The application can avoid image distortion when removing noise from the image to be processed.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) A terminal device; and
   (2) A computer readable storage medium storing program for performing the method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the method for processing image de-noising. (Drawing includes non-English language text)
   Step for acquiring a training image, the training image comprising a source image and a target image corresponding to the source image (S101)
   Step for training constructed convolutional neural network model through the corresponding source image and target image to obtain a trained convolutional neural network model (S102)
   Step for de-noising processed image by the trained convolutional neural network model (S103)
DC T01 (Digital Computers)
MC T01-J10B1; T01-J10D; T01-L02; T01-N01B3
IP G06T-005/00
PD CN109064428-A   21 Dec 2018   G06T-005/00   201911   Pages: 16   Chinese
AD CN109064428-A    CN10864115    01 Aug 2018
PI CN10864115    01 Aug 2018
UT DIIDW:2019022539
ER

PT P
PN CN109064462-A
TI Deep learning based rail surface defect detection method, involves matching similarities between color value or threshold value with pixel value of label, and obtaining rail surface defect result.
AU ZHANG H
   SONG Y
   LIU L
   ZHONG H
   LIANG Z
AE UNIV CHANGSHA SCI & TECHNOLOGY (UYSG-C)
GA 2019006373
AB    NOVELTY - The method involves collecting production data and rail surface image. Standard rail defects are determined by utilizing an image editing software. A convolution neural network training process is performed corresponding to the label data set. RGB value is determined. Crack defect distribution characteristics of a vertical or horizontal rail surface are determined. Characteristic small width is determined. Regular circular distribution characteristic random defect is determined. Similarities between color value or threshold value are matched with a pixel value of a label. Rail surface defect result is obtained.
   USE - Deep learning based rail surface defect detection method.
   ADVANTAGE - The method enables increasing rail surface image detection accuracy and precision and reducing labor intensity.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based rail surface defect detection method. '(Drawing includes non-English language text)'
DC S03 (Scientific Instrumentation); T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC S03-E04F; T01-J04B2; T01-J10B2; T01-J10B3A; T01-J10B3B; T01-J30F; T01-N01B3; T01-N01D1B; T01-N03; T04-D07D3; T04-D08
IP G06T-007/00; G06T-007/12; G06T-007/181; G01N-021/88
PD CN109064462-A   21 Dec 2018   G06T-007/00   201911   Pages: 19   Chinese
AD CN109064462-A    CN10885515    06 Aug 2018
PI CN10885515    06 Aug 2018
UT DIIDW:2019006373
ER

PT P
PN CN109062983-A
TI Method for identifying named entity for medical health knowledge map, involves for obtaining recognition result of statement to be tested by trained named entity recognition model.
AU ZHAO J
   BU J
   KONG F
   CHANG D
   LIU B
   LIU C
   JIANG P
AE BEIJING MIAOYIJIA INFORMATION TECHNOLOGY (BEIJ-Non-standard)
GA 2019006741
AB    NOVELTY - The method involves labeling (S10) a training corpus and generating a word vector according to the labeled training corpus. A named entity recognition model is trained (S20) using the word vector, the named entity recognition model is based on an cavity convolutional neural network (ID-CNN) and a conditional random field (CRF) model. The recognition result of the statement to be tested is obtained (S30) by the trained named entity recognition model. The training corpus is labeled by using an IOB mode.
   USE - Method for identifying named entity for medical health knowledge map.
   ADVANTAGE - The method can solve the problem of named entity recognition in constructing a medical health knowledge map in the health field by using the named entity recognition model of ID-CNN and CRF and the text corpus for the health management field can be automatically identify the specific knowledge entities for use in the medical health knowledge map.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a named entity identification system for a medical health knowledge map.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating of a method for identifying a named entity for a medical health knowledge map. (Drawing includes non-English language text)
   Step for labeling a training corpus (S10)
   Step for training named entity recognition model (S20)
   Step for obtaining recognition result of the statement to be tested (S30)
DC S05 (Electrical Medical Equipment); T01 (Digital Computers)
MC S05-G02G9; T01-J05B4P; T01-N01B3; T01-N01E1
IP G06F-017/30; G16H-050/70
PD CN109062983-A   21 Dec 2018   G06F-017/30   201911   Pages: 10   Chinese
AD CN109062983-A    CN10708672    02 Jul 2018
PI CN10708672    02 Jul 2018
UT DIIDW:2019006741
ER

PT P
PN CN109062700-A
TI Resource management method for distributed system, involves broadcasting model file of deep learning neural network model to be pre-exported to node servers with resources.
AU ZHAO R
AE ZHENGZHOU YUNHAI INFORMATION TECHNOLOGY (INEI-C)
GA 2019006816
AB    NOVELTY - The method involves evaluating (11) a resource according to the pre-processed training data and the preset depth learning neural network model, when the Spark task is started. The resource is applied to a server. The model file of the deep learning neural network model to be pre-exported is broadcasted (12) to each of the node servers with resources, after receiving the information of the node server with resources returned by the server.
   USE - Resource management method for distributed system.
   ADVANTAGE - The resource management method can automatically slice the input data to efficiently complete the data parallel model training.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a Resource management system for distributed system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the resource management method for distributed system. (Drawing includes non-English language text)
   Step for evaluating resource (11)
   Step for broadcasting model file of deep learning neural network model (12)
DC T01 (Digital Computers)
MC T01-F02C2; T01-F02C4; T01-N01B3; T01-N01D2; T01-N01D3; T01-N02B1
IP G06F-009/50; G06F-009/54
PD CN109062700-A   21 Dec 2018   G06F-009/50   201911   Pages: 14   Chinese
AD CN109062700-A    CN10953290    21 Aug 2018
PI CN10953290    21 Aug 2018
UT DIIDW:2019006816
ER

PT P
PN CN109074334-A
TI Data processing method for data processing, involves storing read operation result in output feature map according to second direct memory access (DMA) write configuration information.
AU ZHAO Y
   LI S
   GU Q
AE DAJIANG INNOVATIONS TECHNOLOGY CO LTD (DAJI-Non-standard)
GA 201902128V
AB    NOVELTY - The method involves determining second direct memory access (DMA) read configuration information and second DMA write configuration information. The data is read (202) from the input feature map according to the first DMA read configuration information. The read data is output to the operation logic according to the first DMA write configuration information. The operation result of the operation logic is read (203) according to the second DMA read configuration information. The read operation result is stored in the output feature map according to the second DMA write configuration information.
   USE - Data processing method for data processing.
   ADVANTAGE - The data movement in the convolutional neural network (CNN) can be realized by the DMA controller. The CPU not needs to implement data movement in the CNN, thus reducing the CPU load, moving the data more efficiently, and accelerating the CNN operation. The method is provided with flexibility.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a DMA controller;
   (2) a data processing device; and
   (3) a computer readable storage medium storing program for data processing.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic diagram of a pooling operation on an input feature map. (Drawing includes non-English language text)
   Step for generating first DMA read configuration information and first DMA write configuration information (201)
   Step for reading data (202)
   Step for reading the operation result (203)
DC T01 (Digital Computers)
MC T01-H01C2; T01-H05B1; T01-H05B2; T01-N02A3
IP G06F-013/28; G06N-003/04
PD CN109074334-A   21 Dec 2018   G06F-013/28   201911   Pages: 40   Chinese
AD CN109074334-A    CN80022803    29 Dec 2017
FD  CN109074334-A PCT application Application WOCN120273
PI CN80022803    29 Dec 2017
   CN80022803    10 Oct 2018
UT DIIDW:201902128V
ER

PT P
PN CN109074335-A
TI Data processing method for data processing, involves storing input data to target input feature map according to third direct memory access (DMA) configuration information.
AU ZHAO Y
   YANG K
   LI S
AE DAJIANG INNOVATIONS TECHNOLOGY CO LTD (DAJI-Non-standard)
GA 201902128U
AB    NOVELTY - The method involves acquiring (201) feature information and parameter information of an original input feature map. The second DMA configuration information is generated (202) according to the feature information. The first DMA configuration information and third DMA configuration information are generated according to the feature information and the parameter information. A target input feature map is constructed (203) according to the first DMA configuration information. The input data is read (204) from the original input feature map according to the second DMA configuration information. The input data is stored (205) to the target input feature map according to the third DMA configuration information.
   USE - Data processing method for data processing.
   ADVANTAGE - The data movement in the convolutional neural network (CNN) can be realized by the DMA controller. The CPU not needs to implement data movement in the CNN, thus reducing the CPU load, moving the data more efficiently, and accelerating the CNN operation. The method is provided with also no flexibility.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a DMA controller; and
   (2) a computer readable storage medium for data processing.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a data processing method. (Drawing includes non-English language text)
   Step for acquiring feature information and parameter information (201)
   Step for generating second DMA configuration information (202)
   Step for constructing a target input feature map (203)
   Step for reading input data (204)
   Step for storing input data (205)
DC T01 (Digital Computers)
MC T01-C01; T01-H01C2; T01-H05B1; T01-H05B2; T01-L02; T01-N02A3
IP G06F-013/28; G06F-003/06
PD CN109074335-A   21 Dec 2018   G06F-013/28   201911   Pages: 33   Chinese
AD CN109074335-A    CN80024875    29 Dec 2017
FD  CN109074335-A PCT application Application WOCN120235
PI CN80024875    29 Dec 2017
   CN80024875    19 Oct 2018
UT DIIDW:201902128U
ER

PT P
PN CN109068145-A
TI System for analyzing distributed smart video, has smart video analysis platform that is configured to receive preliminary identification information and process preliminary identification information based on deep learning algorithm.
AU ZHONG W
   GUO Z
   CHEN X
AE SHANGHAI YUANAN INTELLIGENT TECHNOLOGY (SHAN-Non-standard)
GA 2019023998
AB    NOVELTY - The system has a local video processing device with a front-end machine and an internet protocol (IP) video source. The IP video source is configured to collect or store an IP network video stream that needs to perform smart video analysis. The front-end machine is configured to acquire the IP network video stream output by the IP video source, perform preliminary identification on the IP network video stream, and send preliminary identification information to the smart video analysis platform. Smart video analysis platform is configured to receive the preliminary identification information, process the preliminary identification information based on a deep learning algorithm, obtain a recognition result, and transmit the identification result to the front-end machine. The smart video analysis platform is configured to transmit the updated pre-processed neural network model to the front-end machine.
   USE - Distributed smart video analysis system.
   ADVANTAGE - The human face in the video information output by the multiple IP video sources can be accurately recognized. The video recognition effect can be achieved, and the cost is greatly saved. The performance resources of the smart video analysis platform are fully utilized, and the performance of the intelligent video analysis is fully played.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a method for analyzing distributed smart video;
   (2) a device for analyzing distributed smart video;
   (3) a computer device; and
   (4) a computer-readable storage medium storing program for analyzing distributed smart video.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of the distributed smart video analysis system. (Drawing includes non-English language text)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W01 (Telephone and Data Transmission Systems); W02 (Broadcasting, Radio and Line Transmission Systems)
MC T01-J07B; T01-J10B2A; T01-J16C2; T01-N01D1B; T01-S03; T04-D04; W01-A06F2A; W02-F10K
IP H04N-021/218; H04N-021/231; H04N-021/234; G06K-009/00; G06K-009/62; G06N-003/02
PD CN109068145-A   21 Dec 2018   H04N-021/218   201911   Pages: 12   Chinese
AD CN109068145-A    CN10930648    15 Aug 2018
PI CN10930648    15 Aug 2018
UT DIIDW:2019023998
ER

PT P
PN CN109062958-A
TI TextRank and convolutional neural network based automatic primary school composition classification method, involves assigning local optimal feature map text to fully-connected layer, and assigning final classification result to classifier.
AU ZHU X
   LIU S
   SUN J
   SHI Y
AE UNIV CENT CHINA NORMAL (UYNZ-C)
GA 201900674L
AB    NOVELTY - The method involves analyzing school teachers, authorship, common read pictorial, shaped article and read characteristics to divide a data set. Key sentences are extracted based on a key sentence extraction model to remove redundant semantic information in the data set. Words in the data set are mapped to form a two-dimensional feature matrix. Text feature matrix convolution operation is executed by linear transformation. Multi-local convolutional feature map sampling process is performed by adopting max-pooling algorithm. Local optimal feature map text is assigned to a fully-connected layer. A final classification result is assigned to a classifier for predicting type of the text.
   USE - TextRank and convolutional neural network based automatic primary school composition classification method.
   ADVANTAGE - The method enables eliminating redundant information in the data set using TextRank algorithm so as to reduce number of interference information. The method enables realizing automatic characteristic selection process and improving machine learning efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a key sentence extraction model. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-E03; T01-J04B2; T01-J05B2; T01-J05B4P; T01-J16C3; T01-N01B3; W04-W05A
IP G06N-003/04
PD CN109062958-A   21 Dec 2018      201911   Pages: 8   Chinese
AD CN109062958-A    CN10671815    26 Jun 2018
PI CN10671815    26 Jun 2018
UT DIIDW:201900674L
ER

PT P
PN CN109033951-A
TI Graphics processing-based system for detecting shielding object of vehicle, has processor for determining whether shadow is moved along driving direction of vehicle, and actuator system for reducing speed of vehicle.
AU BATUR A U
AE FARADAY & FUTURE INC (FARA-Non-standard)
GA 2018A4855H
AB    NOVELTY - The system has a processor coupled to multiple cameras and brake systems. Each camera captures images of a shadow. The processor determines whether the shadow is moved along driving direction of a vehicle. An actuator system reduces speed of the vehicle according to a determination result of shadow moving direction. The processor is connected to a positioning system through a map interface. The positioning system identifies a position of the vehicle. A shadow detection mode identifies the shadow of the vehicle to determine whether the shadow is moved.
   USE - Graphics processing-based system for detecting a shielding object of a vehicle.
   ADVANTAGE - The system determines the shadow moving direction so as to reduce the speed of the vehicle, thus avoiding collision when a shielding object enters a road, and hence detecting the shadow of the vehicle by using image segmentation or convolutional neural network.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for detecting a driving mode of a vehicle.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a graphics processing-based system for detecting a shielding object of a vehicle. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-C04D; T01-J07D3A; T01-J10B2
IP G06K-009/00; B60W-040/04
PD CN109033951-A   18 Dec 2018   G06K-009/00   201911   Pages: 14   Chinese
AD CN109033951-A    CN10602376    12 Jun 2018
PI US518524P    12 Jun 2017
UT DIIDW:2018A4855H
ER

PT P
PN CN109035260-A
TI Method for dividing sky region of convolutional neural network, involves obtaining upper sampling layer by up-sampling target feature image, and determining pixel area of upper sampling feature image by sky region determining layer.
AU CHEN G
AE BOE TECHNOLOGY GROUP CO LTD (BOEG-C)
GA 2018A4833F
AB    NOVELTY - The method involves obtaining an original image of an image input layer. Multiple sky feature images of a first convolutional neural network are extracted from the original image. Multiple sky feature images are processed by multiple second convolutional neural networks to output a target feature image. Upper sampling layer is obtained by up-sampling a target feature image to obtain an original image scale feature image. A pixel area of an upper sampling feature image is determined by a sky region determining layer, where an image gray value is greater than or equal to a preset gray scale value.
   USE - Method for dividing sky region of a convolutional neural network (claimed).
   ADVANTAGE - The method enables extracting sky semantic characteristics from multiple levels of images in different scales by cascading multiple second convolutional neural networks so as to output the target feature in the image with sky features of each scale, thus accurately determining sky boundary under dark fog night scene, and hence accurately dividing the sky region.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a convolutional neural network
   (2) a device for dividing sky region of a convolutional neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for dividing sky region of a convolutional neural network. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B2; T01-J16C3
IP G06T-007/11; G06N-003/04
PD CN109035260-A   18 Dec 2018   G06T-007/11   201911   Pages: 35   Chinese
AD CN109035260-A    CN10844647    27 Jul 2018
PI CN10844647    27 Jul 2018
UT DIIDW:2018A4833F
ER

PT P
PN CN109033940-A
TI Method for identifying image, involves calculating weight matrix without sharing ratio of number of convolution kernels to total number of convolution kernels and negatively correlated with depth of convolution layer.
AU CHEN H
AE SHANGHAI YITU NETWORK TECHNOLOGY CO LTD (SHAN-Non-standard)
   SHANGHAI TUZHIAN NETWORK TECHNOLOGY CO (SHAN-Non-standard)
   SHENZHEN YITU INFORMATION TECHNOLOGY CO (SHEN-Non-standard)
GA 2018A4855S
AB    NOVELTY - The method involves obtaining a to-be-detected image. The to-be-detected image is input into a pre-trained convolutional neural network model. A characteristic image is determined by using the convolution neural network model. An identification result of the image is determined according to the characteristic image, where the convolutional neural network model comprises a first convolution unit, a second convolution unit and a third convolution unit. A weight matrix of the convolution kernel is determined. The weight matrix is calculated without sharing ratio of number of convolution kernels to total number of convolution kernels and negatively correlated with depth of the convolution layer.
   USE - Method for identifying an image.
   ADVANTAGE - The method enables realizing the identification of the image with high precision.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for identifying an image
   (2) a computer-readable storage medium for storing a set of instructions for identifying an image
   (3) a computing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for identifying an image. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-E03; T01-E04; T01-J04A; T01-J04B2; T01-J04C; T01-J10B2; T01-S03
IP G06K-009/00
PD CN109033940-A   18 Dec 2018   G06K-009/00   201911   Pages: 21   Chinese
AD CN109033940-A    CN10565420    04 Jun 2018
PI CN10565420    04 Jun 2018
CP CN109033940-A
      CN106682736-A   BEIJING XIAOMI MOBILE SOFTWARE CO LTD (XIAO)   WAN S
      CN107610091-A   ALIBABA GROUP HOLDING LTD (ABAB)   HOU J, ZHANG H, GUO X, XU J, WANG J, CHENG Y, CHENG D
      CN107944415-A   DONG W (DONG-Individual)   DONG W, HE B
      CN108009481-A   ZHEJIANG DAHUA TECHNOLOGY CO LTD (ZJDH)   CHENG F, HAO J
      US20170083796-A1      
      US20170372174-A1      
UT DIIDW:2018A4855S
ER

PT P
PN CN109035149-A
TI Deep learning based number plate image de-motion blurring method, involves determining gradient error of gradient image, and removing fuzzy number plate character to obtain fuzzy number plate image according to original sequence.
AU CHEN H
   MAO Y
   YE X
AE UNIV HANGZHOU DIANZI (UYHH-C)
GA 2018A48369
AB    NOVELTY - The method involves pre-processing a data set, where the data set includes a fuzzy image. Clear number plate calibration process is performed. A clear image of number plate region is obtained. Number plate character width is divided to obtain a single character data set. Channel number is determined. A pixel value of a clear number plate character image and the fuzzy number plate character image is calculated. A training data set is selected as a verification data set. A confidence result is output by a discriminator. A gradient error of a gradient image is determined. A fuzzy number plate character is removed to obtain a fuzzy number plate image according to an original sequence.
   USE - Deep learning based number Plate image de-motion blurring method.
   ADVANTAGE - The method enables constraining edge of a number plate image so as to improve number plate image motion quality, and thus reducing recovery time consumption.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a deep learning based number plate image de-motion blurring system. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-E01B; T01-J10B1; T01-J10B2; T01-J10B3A; T01-J30A; T04-D02; T04-D04; W04-W05A
IP G06T-005/00; G06K-009/20; G06K-009/34; G06K-009/62
PD CN109035149-A   18 Dec 2018   G06T-005/00   201911   Pages: 12   Chinese
AD CN109035149-A    CN10205661    13 Mar 2018
PI CN10205661    13 Mar 2018
UT DIIDW:2018A48369
ER

PT P
PN CN109034375-A
TI Neural network based asynchronous multi-source time sequence data processing method, involves performing weighting network and timing data offset network data processing operation to obtain sequence data analysis prediction result.
AU CHEN L
   JI Y
   YANG Y
AE ORCHES NANJING NETWORK TECHNOLOGY CO LTD (ORCH-Non-standard)
GA 2018A4844C
AB    NOVELTY - The method involves inputting asynchronous multi-source timing data to a weighting network and an offset network. Asynchronous multi-source sequence data source weight is analyzed by the weighting network. Asynchronous data multi-source sequence offset is calculated in the offset network. Weighting network and timing data offset network data processing operation is performed to obtain an asynchronous multi-source sequence data analysis prediction result. The weighting network and the offset network are a multi-layer convolution neural network. An asynchronous multi-source asynchronous characteristic value is obtained.
   USE - Neural network based asynchronous multi-source time sequence data processing method.
   ADVANTAGE - The method enables establishing an integrated self-regression and convolutional neural network for processing asynchronous time sequence data and improving asynchronous time sequence data analysis prediction result accuracy, detecting different weight distribution process and avoiding asynchronous lost data.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a neural network based asynchronous multi-source time sequence data processing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J03; T01-J04B2; T01-N01A1
IP G06N-003/04; G06N-003/08; G06F-017/18; G06Q-040/00
PD CN109034375-A   18 Dec 2018   G06N-003/04   201911   Pages: 5   Chinese
AD CN109034375-A    CN10727873    02 Jul 2018
PI CN10727873    02 Jul 2018
UT DIIDW:2018A4844C
ER

PT P
PN CN109034373-A
TI Parallel processor for convolutional neural network (CNN) e.g. feedforward neural network, has parallel output data storage control unit to store corresponding output feature plane data in parallel manner.
AU CHEN T
   ZHOU D
   ZHANG Y
AE DINGSHI SMART BEIJING TECHNOLOGY CO LTD (DING-Non-standard)
GA 2018A5537U
AB    NOVELTY - The processor has a parallel convolution and pooling processing unit (300), an input data windowing control unit (100), a convolution kernel storage control unit (200), and a parallel output data storage control unit (400). The input data windowing control unit performs two-dimensional recombination on the input feature plane data fin by using a variable depth shift register chain, outputs first window data, and inputs the first window data to the Parallel convolution and pooling processing unit. The parallel convolution and pooling processing unit performs a convolution operation and a pooling process on the first window data, the convolution kernel parameter, and the offset amount in a parallel manner to obtain parallel output feature plane data, and input the output feature plane data to the parallel output data storage control unit. The parallel output data storage control unit stores the corresponding output feature plane data in a parallel manner.
   USE - Parallel processor for convolutional neural network (CNN) e.g. feedforward neural network.
   ADVANTAGE - The processor optimizes the CNN calculation process and speeds up the calculation.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a parallel processing method for CNN.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic structural view of the parallel processor of the CNN. (Drawing includes non-English language text)
   Input data windowing control unit (100)
   Convolution kernel storage control unit (200)
   Parallel convolution and pooling processing unit (300)
   Parallel output data storage control unit (400)
   Direct memory access controller (500)
DC T01 (Digital Computers)
MC T01-H01C2; T01-J04B2; T01-J10B; T01-M02C
IP G06N-003/04; G06T-001/20
PD CN109034373-A   18 Dec 2018   G06N-003/04   201911   Pages: 15   Chinese
AD CN109034373-A    CN10710911    02 Jul 2018
PI CN10710911    02 Jul 2018
UT DIIDW:2018A5537U
ER

PT P
PN CN109035226-A
TI LSTM model based panel Mura defect detecting method, involves extracting qualified sample characteristic and defect sample characteristic, obtaining classification result of model, and checking whether defect position is located in panel.
AU CHEN W
   ZHANG S
   ZHENG Z
AE WUHAN JINGCE ELECTRONIC GROUP CO LTD (WUHA-Non-standard)
GA 2018A4834A
AB    NOVELTY - The method involves dividing a panel into multiple regions. A qualified sample and a defect sample are collected. A sample sequence is obtained based on the qualified sample and the defect sample. A LSTM model is established. The LSTM model is trained according to the sample sequence. Qualified sample characteristic and defect sample characteristic are extracted by deep learning algorithm and the LSTM model. A classification result of the LSTM model is obtained. Judgment is made to check whether Mura defect position is located in a panel to-be tested.
   USE - LSTM model based panel Mura defect detecting method.
   ADVANTAGE - The method enables establishing a defect detection model by a deep neural network, effectively avoiding background segmentation and parameter extraction limitation, reducing labor intensity and quickly outputting Mura defect so as to accurately detect defect and improving Mura defect inspection accuracy and efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a front view of a panel. '(Drawing includes non-English language text)'
DC S03 (Scientific Instrumentation); T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC S03-E04F; T01-J05B2; T01-J10B2; T01-J16C2; T01-N01B3; T04-D04; T04-D07A; T04-D07D5
IP G06T-007/00; G06K-009/62; G01N-021/88
PD CN109035226-A   18 Dec 2018   G06T-007/00   201911   Pages: 8   Chinese
AD CN109035226-A    CN10763443    12 Jul 2018
PI CN10763443    12 Jul 2018
UT DIIDW:2018A4834A
ER

PT P
PN CN109035163-A
TI Deep learning-based adaptive image de-noising method, involves inputting image to be de-noised into adaptive depth convolutional neural network model, and subtracting residual image from image to be de-noised to obtain denoised image.
AU CHEN X
   XU C
AE UNIV NANJING INFORMATION SCI & TECHNOLOG (UNAI-C)
GA 2018A4835Y
AB    NOVELTY - The method involves setting a noisy image, a corresponding ideal noise-free image and noise. An adaptive deep convolutional neural network is constructed. Learning rate and momentum parameter of the deep convolutional neural network are set. The adaptive deep convolutional neural network is trained through a deep learning framework to reach maximum iteration times. A trained adaptive depth convolutional neural network model is generated. An image to be de-noised is inputted into the trained adaptive depth convolutional neural network model to obtain a corresponding residual image. The residual image is subtracted from the image to be de-noised to obtain a denoised image.
   USE - Deep learning-based adaptive image de-noising method.
   ADVANTAGE - The method enables improving and stabilizing training performance of a convolutional neural network so as to ensure de-noising performance, saving time without manual adjustment of reference and realizing blind denoising of an unknown noise image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram illustrating a deep learning-based adaptive image de-noising method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B1; T01-N01B3
IP G06T-005/00
PD CN109035163-A   18 Dec 2018   G06T-005/00   201911   Pages: 9   Chinese
AD CN109035163-A    CN10750492    09 Jul 2018
PI CN10750492    09 Jul 2018
UT DIIDW:2018A4835Y
ER

PT P
PN CN109035263-A
TI Convolutional neural network based automatic brain tumor image segmentation method, involves selecting ideal set as training sample of module, obtaining ideal value, and evaluating and testing image segmentation result.
AU CHENG J
   GUO H
   SU Y
   GAO Y
   XU K
AE UNIV ELECTRONIC SCI & TECHNOLOGY (UEST-C)
GA 2018A4833D
AB    NOVELTY - The method involves collecting multimodal MRI images of a brain tumor. An image pre-processing is performed to obtain an original image set. A frame for brain tumor segmentation is constructed based on the multimodal MRI image, where the frame includes a module and the module includes a 3D convolutional neural network and a residual unit. A brain tumor segmentation image contour map is outputted. The original image set is inputted into the module of the frame. An ideal set is selected as a training sample of the module. An ideal value is obtained through iterations of training. An image segmentation result is evaluated and tested.
   USE - Convolutional neural network based automatic brain tumor image segmentation method.
   ADVANTAGE - The method enables reducing the problem of low segmentation accuracy of the brain tumor segmentation image, improving the recognizability of the tumor to ensure convenient image pre-processing operations and utilizing the loss function based on coefficient in the segmentation module to reduce the problem of class imbalance.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based automatic brain tumor image segmentation method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J10B1; T01-J10B2A; T01-J10B3A; T01-N01B3A
IP G06T-007/11; G06T-007/12; G06N-003/04
PD CN109035263-A   18 Dec 2018   G06T-007/11   201911   Pages: 11   Chinese
AD CN109035263-A    CN10921637    14 Aug 2018
PI CN10921637    14 Aug 2018
UT DIIDW:2018A4833D
ER

PT P
PN CN109034119-A
TI Face detection method based on optimized full convolutional neural network, involves performing face detection on image to be measured using full convolutional neural network model after training completion.
AU CHU C
   LIU J
AE SUZHOU GUANGMU INFORMATION TECHNOLOGY CO LTD (SUZH-Non-standard)
GA 2018A55547
AB    NOVELTY - The method involves establishing a full convolutional neural network model. The training data is prepared. The full convolutional neural network model is trained. The face detection is performed on the image to be measured using the full convolutional neural network model after training completion. The P-Net network layer obtains a regression window of the candidate window and the bounding box of the face region through the operation of the three-layer convolution layer. The R-Net/O-Net improved network layer refers to replacing the fully connected layer in the R-Net/O-Net network layer with a convolution layer. The convolution operation of the P-Net network layer, the R-Net improved network layer, and the convolutional layers in the O-Net improved network layer uses a concatenated decomposition convolution operation. The concatenated decomposition convolution operation includes a per-channel volume product operations and point-by-point convolution operations.
   USE - Face detection method based on optimized full convolutional neural network.
   ADVANTAGE - The iterative greedy compression operation is used to effectively compress the parameters of the model. The size of the parameters is reduced under the premise of ensuring accuracy. The storage space occupied by the model parameters is reduced. The versatility and scope of use of the model are increased.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of the full convolutional neural network. (Drawing includes non-English language text)
DC T01 (Digital Computers)
MC T01-J04B2; T01-J10B2; T01-N01B3
IP G06K-009/00; G06N-003/04
PD CN109034119-A   18 Dec 2018   G06K-009/00   201911   Pages: 13   Chinese
AD CN109034119-A    CN10980287    27 Aug 2018
PI CN10980287    27 Aug 2018
UT DIIDW:2018A55547
ER

PT P
PN CN109034230-A
TI Deep learning based single image camera tracing method, involves establishing hierarchy structure multi-task classification module, and identifying camera brand, camera model and individual camera device based on extracted features.
AU DING X
   CHEN Y
   TANG Z
   HUANG Y
AE UNIV XIAMEN (UYXI-C)
GA 2018A48488
AB    NOVELTY - The method involves sending an original image to a multi-scale Laplace filter for extracting high-frequency images. Feature extraction process is performed in the high-frequency image. Extracted features are merged. A feature extractor is established based on noise enhancement process. Merged features are transmitted into the feature extractor based on extracted features. A hierarchy structure multi-task classification module is established according to hierarchical relationship between camera brand and camera model. A camera brand, a camera model and an individual camera device are identified based on the extracted features.
   USE - Deep learning based single image camera tracing method.
   ADVANTAGE - The method enables extracting self-adaptively weak fingerprint characteristic in camera imaging process, inhibiting strong background noise, and improving camera tracing accuracy and realizing strong camera tampering image and mobile photographing image robustness.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based single image camera tracing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W04 (Audio/Video Recording and Systems)
MC T01-F02; T01-J05B2; T01-J10B1; T01-J10B2; T01-J30A; T04-D03A; T04-D04; W04-W05A
IP G06K-009/62; G06K-009/46; G06K-009/40; G06N-003/04
PD CN109034230-A   18 Dec 2018   G06K-009/62   201911   Pages: 16   Chinese
AD CN109034230-A    CN10785332    17 Jul 2018
PI CN10785332    17 Jul 2018
UT DIIDW:2018A48488
ER

PT P
PN CN109033277-A
TI Computer system based on machine learning for providing artificial intelligent service to user, has knowledge discovery module for converting data into knowledge data, and knowledge discovery module sends knowledge data to storage module.
AU DONG W
AE GUANGZHOU GIANTAN TECHNOLOGY CO LTD (GUAN-Non-standard)
GA 2018A4872P
AB    NOVELTY - The system has an intelligent question answering module for semantically understanding multi-mode data to generate a problem request, where the multi-mode data comprises text, voice, images and structured data. An engine module receives the problem request sent by the human-computer interaction interface. A knowledge discovery module identifies potentially effective data from service data through deep learning techniques. The knowledge discovery module converts the effective data into knowledge data of an understanding mode. The knowledge discovery module sends the knowledge data to a knowledge storage module, where the intelligent question answering module is a virtual proxy, and the knowledge data comprises FAQ, scene knowledge, a document, WIKI, a semantic knowledge map, a knowledge model and user data.
   USE - Computer system based on machine learning for providing artificial intelligent service to a user.
   ADVANTAGE - The system provides artificial intelligent service to a user in an efficient and accurate manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a computer device based on machine learning for providing artificial intelligent service to a user
   (2) a method for providing artificial intelligent service to a user based on machine learning
   (3) a computer-readable storage medium for storing a set of instructions for providing artificial intelligent service to a user based on machine learning.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a computer system based on machine learning. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); W04 (Audio/Video Recording and Systems)
MC T01-J05B2B; T01-J05B4F; T01-J07B; T01-J16C3; T01-N01D2; T01-S03; W04-W05A
IP G06F-017/30; G06N-003/02; G06N-099/00
PD CN109033277-A   18 Dec 2018   G06F-017/30   201911   Pages: 12   Chinese
AD CN109033277-A    CN10754780    10 Jul 2018
PI CN10754780    10 Jul 2018
UT DIIDW:2018A4872P
ER

PT P
PN CN109033309-A
TI Method for processing sample data, involves performing standardization process on characteristic data of two dimensions for reducing difference between characteristic data of two dimensions and obtaining input data of depth learning model.
AU FAN J
   CHEN T
AE TENCENT TECHNOLOGY SHENZHEN CO LTD (TNCT-C)
GA 2018A4871W
AB    NOVELTY - The method involves obtaining sample data of a depth learning model, where the sample data includes characteristic data of two dimensions. Standardization process is performed on the characteristic data of two dimensions to obtain processed characteristic data of the dimensions for reducing difference between characteristic data of two dimensions and obtaining input data of a depth learning model. Binarization process is performed on the characteristic data. A target matrix is generated by utilizing the characteristic data, where the target matrix is a two-dimensional matrix.
   USE - Method for processing sample data.
   ADVANTAGE - The method enables realizing subsequent processing of the characteristic data by using the depth learning model so as to increase convergence speed of the depth learning model and improve training efficiency and accuracy of the depth learning model.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a sample data processing device
   (2) a computer-readable storage medium for storing a set of instructions for processing sample data
   (3) a computer device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for processing sample data. '(Drawing includes non-English language text)'
DC T01 (Digital Computers)
MC T01-J05B4P; T01-J30A; T01-S03
IP G06F-017/30; G06N-099/00
PD CN109033309-A   18 Dec 2018   G06F-017/30   201911   Pages: 26   Chinese
AD CN109033309-A    CN10783681    17 Jul 2018
PI CN10783681    17 Jul 2018
UT DIIDW:2018A4871W
ER

PT P
PN CN109033978-A
TI Convolutional neural network (CNN) - support vector machine (SVM) hybrid model gesture recognition method based on error correction strategy, involves correcting classification result error correction strategy.
AU FENG Z
   LI J
AE UNIV JINAN (UJIN-C)
GA 2018A5554T
AB    NOVELTY - The CNN-SVM hybrid model gesture recognition method involves preprocessing the collected gesture data. The features are then automatically extracted, and the predictive classification is performed to obtain classification results, and finally utilized. The error correction strategy is corrected by the classification result. The static gestures are acquired, and depth images of the hand and color images are respectively acquired. The depth image is processed to obtain a mask image. A rough operation of the color image and the mask image is performed to obtain a rough gesture area image.
   USE - CNN-SVM hybrid model gesture recognition method based on error correction strategy.
   ADVANTAGE - The method reduces the misrecognition rate between confusing gestures and improves the recognition rate of static gestures.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram showing the steps of the method for recognizing CNN-SVM hybrid model gesture. (Drawing includes non-English language text)
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment)
MC T01-J05B2; T01-J10B2A; T01-J10B3B; T04-D04; T04-D07A; T04-D08
IP G06K-009/00; G06K-009/62
PD CN109033978-A   18 Dec 2018   G06K-009/00   201911   Pages: 16   Chinese
AD CN109033978-A    CN10684333    28 Jun 2018
PI CN10684333    28 Jun 2018
UT DIIDW:2018A5554T
ER

PT P
PN CN109035488-A
TI CNN feature extraction based aero-engine time sequence abnormality detecting method, involves obtaining aero-engine time sequence abnormal classification result by using trained convolutional neural network model and BP neural network.
AU FU X
   ZHONG S
   LIN L
   ZHANG G
AE HARBIN INST TECHNOLOGY (HAIT-C)
GA 2018A4827W
AB    NOVELTY - The method involves selecting a normal sample and an abnormal sample from aero-engine monitoring data (S2). A training set is established according to the optimized monitoring parameters. A convolutional neural network model is established (S3). The convolutional neural network model is trained by using the training set to perform feature extraction process. A BP neural network is trained (S4) by using characteristics of the convolutional neural network model. To-be-detected samples are extracted (S5) from the aero-engine monitoring data according to the optimized monitoring parameter order. An aero-engine time sequence abnormal classification result is obtained (S6) by using the trained convolutional neural network model and the trained BP neural network.
   USE - CNN feature extraction based aero-engine time sequence abnormality detecting method.
   ADVANTAGE - The method enables increasing aero-engine time sequence abnormality detection effect.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a CNN feature extraction based aero-engine time sequence abnormality detecting method. '(Drawing includes non-English language text)'
   Step for selecting normal sample and abnormal sample from aero-engine monitoring data (S2)
   Step for establishing convolutional neural network model (S3)
   Step for training BP neural network by using characteristics of convolutional neural network model (S4)
   Step for extracting multiple samples to be detected from aero-engine monitoring data (S5)
   Step for obtaining aero-engine time sequence abnormal classification result by using trained convolutional neural network model and BP neural network (S6)
DC T01 (Digital Computers); T05 (Counting, Checking, Vending, ATM and POS Systems)
MC T01-J05B2; T01-J10B2; T01-N01B3; T01-N02B2; T05-G01; T05-G03
IP G07C-005/08; G06K-009/62; G06N-003/04; G06N-003/08
PD CN109035488-A   18 Dec 2018   G07C-005/08   201911   Pages: 24   Chinese
AD CN109035488-A    CN10889863    07 Aug 2018
PI CN10889863    07 Aug 2018
UT DIIDW:2018A4827W
ER

PT P
PN CN109034020-A
TI Internet-of-things and deep learning based community risk monitoring and preventing method, involves establishing cascade sequence convolutional neural network to realize behavior analysis, and performing heterogeneous information fusion.
AU HE X
   CHEN L
   ZHAO E
AE UNIV CHONGQING POSTS & TELECOM (UCPT-C)
GA 2018A4853P
AB    NOVELTY - The method involves building a cloud-network-end integrated community risk monitoring and security network architecture. Depth coding is performed on community risk monitoring data. Operator identity authentication is monitored based on an extreme learning and face feature. A cascade sequence convolutional neural network is established to realize behavior analysis of the monitored person. Heterogeneous decision information fusion is performed based on depth coding, where the cloud end network comprises network of Third Generation (3G)/Fourth Generation (4G),Narrow Band-Internet-of-Things (NB-IoT),Wireless fidelity (WiFi) and other network.
   USE - Internet-of-things and deep learning based community risk monitoring and preventing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic illustration of an internet-of-things and deep learning based community risk monitoring and preventing method. '(Drawing includes non-English language text)'
DC T01 (Digital Computers); T04 (Computer Peripheral Equipment); W01 (Telephone and Data Transmission Systems); W05 (Alarms, Signalling, Telemetry and Telecontrol)
MC T01-D02; T01-J05B4P; T01-J10B2; T01-N01A2D; T01-N01B3; T01-N01F; T01-N02B1B; T01-N02B2; T04-D03; T04-D04; W01-A06C4E; W05-D06E1A; W05-D08E
IP G06K-009/00; G06K-009/46; G06K-009/62; G06F-017/30; G06Q-050/26; H04L-029/08
PD CN109034020-A   18 Dec 2018   G06K-009/00   201911   Pages: 8   Chinese
AD CN109034020-A    CN10765316    12 Jul 2018
PI CN10765316    12 Jul 2018
UT DIIDW:2018A4853P
ER

PT P
PN CN109035329-A
TI Camera posture based on depth characteristic estimation optimizing method, involves calculating similarity of two-dimensional-three-dimensional points, and proposing multi-feature cluster optimization algorithm.
AU JI R
   GUO F
   CHEN H
AE UNIV XIAMEN (UYXI-C)
GA 2018A4831S
AB    NOVELTY - The method involves calculating similarity of two-dimensional-three-dimensional points to map two-dimensional-three-dimensional point information by using a random forest based matching algorithm. A multi-feature fusion process is established for evaluating a camera pose by using a constraint function. An instability problem in SLAM algorithm based on deep learning is solved by using the calculating similarity of two-dimensional-three-dimensional points. A multi-feature cluster optimization algorithm is proposed.
   USE - Camera posture based on depth characteristic estimation optimizing method.
   ADVANTAGE - The method enables optimizing performance based on deep learning SLAM and experiment result for ensuring algorithm robustness.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a camera posture based on depth characteristic estimation optimizing method.
DC T01 (Digital Computers)
MC T01-J04A; T01-J04D; T01-J30A
IP G06T-007/70; G06T-007/80; G06T-007/90
PD CN109035329-A   18 Dec 2018   G06T-007/70   201911   Pages: 14   Chinese
AD CN109035329-A    CN10878967    03 Aug 2018
PI CN10878967    03 Aug 2018
UT DIIDW:2018A4831S
ER

EF